{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 何ができて何ができないのか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"rag_truth_test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "dev_data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ref': 'The FBI charged a Philadelphia woman on Thursday with trying to travel overseas to fight for ISIS. She\\'s one of three women arrested this week on terror charges. Two New York women were also taken into custody. An FBI complaint cites numerous social media messages dating back to August 2013 that were sent by Keonna Thomas, 30, also known as \"Young Lioness\" and \"Fatayat Al Khilafah.\" One Twitter message said, \"If we truly knew the realities ... we all would be rushing to join our brothers in the front lines pray ALLAH accept us as shuhada [martyrs].\" Another said, \"When you\\'re a mujahid [violent jihadi fighter] your death becomes a wedding.\" The FBI said Thomas purchased an electronic visa to Turkey on March 23. Turkey is known as the easiest place from which to enter Syria and join ISIS. An ISIS manual advises recruits to buy round-trip tickets to vacation spots such as Spain and then purchase tickets for their real destination once they arrive overseas, the FBI said. On March 26, Thomas purchased a ticket to Barcelona, with a March 29 departure and an April 15 return to the United States, the complaint said. It\\'s not clear when or where she was arrested. She was charged with knowingly attempting to provide material support and resources to a designated foreign terrorist organization. She could be sentenced to 15 years in prison. On Thursday, Noelle Velentzas, 28, and her former roommate, Asia Siddiqui, 31, were arrested in New York and accused of planning to build an explosive device for attacks in the United States, federal prosecutors said. In the past 18 months, the Justice Department\\'s National Security Division has prosecuted or is prosecuting more than 30 cases of people attempting to travel abroad to join or provide support to terrorist groups. Of those cases, 18 allegedly involve support to ISIS. \"The terrorist threat is more decentralized, more diffuse, more complicated,\" Homeland Security Secretary Jeh Johnson told reporters Thursday. \"It involves the potential lone wolf actor, it involves the effective use of social media, the Internet.\"\\n',\n",
       " 'text': 'Please judge the following statement as true or false (based on the references above): The FBI has charged a Philadelphia woman, Keonna Thomas, with trying to travel overseas to fight for ISIS. This follows the arrests of two New York women, Noelle Velentzas and Asia Siddiqui, who were accused of planning to build an explosive device for attacks in the United States. Thomas purchased an electronic visa to Turkey and a round-trip ticket to Barcelona, following the advice given in an ISIS manual for recruits. She has been charged with attempting to provide material support to a terrorist organisation and could face 15 years in prison.',\n",
       " 'labels': 0,\n",
       " 'source': 'CNN/DM',\n",
       " 'model': 'gpt-4-0613',\n",
       " 'task_type': 'Summary',\n",
       " 'source_id': '15596'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prefixをつける\n",
    "def add_prefix(data):\n",
    "    for d in data:\n",
    "        d[\"text\"] = \"Please judge the following statement as true or false (based on the references above): \" + d[\"text\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "dev_data = add_prefix(dev_data)\n",
    "dev_data[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_type: QA, Data2txt, Summary\n",
    "task_name = \"Summary\"\n",
    "dev_data = [d for d in dev_data if d[\"task_type\"] == task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    dev: Dataset({\n",
       "        features: ['ref', 'text', 'labels', 'source', 'model', 'task_type', 'source_id'],\n",
       "        num_rows: 900\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dev_df = pd.DataFrame(dev_data)\n",
    "dev_ds = Dataset.from_pandas(dev_df)\n",
    "\n",
    "raw_datasets = DatasetDict({\"dev\":dev_ds})\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 13:24:49.871431: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-23 13:24:50.268171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-23 13:24:50.448942: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-23 13:24:50.449335: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-23 13:24:50.779741: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-23 13:24:52.953728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d16f1e2bf65455a920f9b9831e92d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from models import NoRagModel\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "# モデルとトークナイザーを読み込む\n",
    "name = \"./1216_no_rag\"\n",
    "model = NoRagModel.from_pretrained(base_model,name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 使う装置\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4d114714b7403c8251141fa07d9d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "# tokenized_datasets.set_format(\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    dev: Dataset({\n",
       "        features: ['ref', 'labels', 'source', 'model', 'task_type', 'source_id', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 900\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0, 'label': 0, 'task': 'Summary'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for i, d in enumerate(tokenized_datasets[\"dev\"]):\n",
    "    results.append({\"id\": i,\"label\":d[\"labels\"],\"task\":d[\"task_type\"]})\n",
    "\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/gs/fs/tga-arase-student/yamada/check_improve.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ood.t4.gsic.titech.ac.jp/gs/fs/tga-arase-student/yamada/check_improve.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(d[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.t4.gsic.titech.ac.jp/gs/fs/tga-arase-student/yamada/check_improve.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ood.t4.gsic.titech.ac.jp/gs/fs/tga-arase-student/yamada/check_improve.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.t4.gsic.titech.ac.jp/gs/fs/tga-arase-student/yamada/check_improve.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     logits \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.t4.gsic.titech.ac.jp/gs/fs/tga-arase-student/yamada/check_improve.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     predicted_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/models.py:21\u001b[0m, in \u001b[0;36mNoRagModel.forward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 21\u001b[0m     text_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)[\u001b[39m0\u001b[39m][:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:]\n\u001b[1;32m     22\u001b[0m     \u001b[39m#text_output = text_output.mean(dim=1)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[39m#print(text_output.shape)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     text_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(text_output)\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/transformers/models/phi3/modeling_phi3.py:605\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds\n\u001b[1;32m    604\u001b[0m \u001b[39m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    607\u001b[0m \u001b[39m# decoder layers\u001b[39;00m\n\u001b[1;32m    608\u001b[0m all_hidden_states \u001b[39m=\u001b[39m () \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/transformers/models/phi3/modeling_phi3.py:368\u001b[0m, in \u001b[0;36mPhi3RotaryEmbedding.forward\u001b[0;34m(self, x, position_ids)\u001b[0m\n\u001b[1;32m    366\u001b[0m device_type \u001b[39m=\u001b[39m device_type \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device_type, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m device_type \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautocast(device_type\u001b[39m=\u001b[39mdevice_type, enabled\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 368\u001b[0m     freqs \u001b[39m=\u001b[39m (inv_freq_expanded\u001b[39m.\u001b[39;49mfloat() \u001b[39m@\u001b[39;49m position_ids_expanded\u001b[39m.\u001b[39;49mfloat())\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    369\u001b[0m     emb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((freqs, freqs), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    370\u001b[0m     cos \u001b[39m=\u001b[39m emb\u001b[39m.\u001b[39mcos()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_bmm)"
     ]
    }
   ],
   "source": [
    "# devセットで推論して、logitと結果をファイルに保存\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for i,d in tqdm(enumerate(tokenized_datasets[\"dev\"])):\n",
    "    input_ids =torch.tensor(d[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(d[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[\"logits\"]\n",
    "        predicted_index = torch.argmax(logits, dim=-1)\n",
    "    results[i][\"no_rag_logits\"] = logits.cpu().numpy()\n",
    "    results[i][\"no_rag_label\"] = predicted_index.cpu().numpy()\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7655555555555555\n",
      "F1-score: 0.6520065970313359\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# サンプルデータ\n",
    "data = results\n",
    "\n",
    "# 正解ラベルと予測ラベルをリストに抽出\n",
    "true_labels = [item['label'] for item in data]\n",
    "predicted_labels = [item['no_rag_label'][0] for item in data]\n",
    "\n",
    "# accuracy と F1-score の計算\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels, average='binary')  # binary タスク用\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1-score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb6327ca5464d08a86c934582384281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70afe8d1e124e1da287ba460fcf1af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "from models import RagModel\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "# モデルとトークナイザーを読み込む\n",
    "name = \"./1216_rag\"\n",
    "model = RagModel.from_pretrained(base_model,name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "# 使う装置\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    ref = tokenizer(examples[\"ref\"],truncation=True, max_length=512)\n",
    "    text = tokenizer(examples[\"text\"],truncation=True, max_length=512)\n",
    "    return {\n",
    "        \"ref_input_ids\":ref[\"input_ids\"],\n",
    "        \"ref_attention_mask\":ref[\"attention_mask\"],\n",
    "        \"text_input_ids\":text[\"input_ids\"],\n",
    "        \"text_attention_mask\":text[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\",\"ref\"])\n",
    "#tokenized_datasets.set_format(\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2700it [12:19,  3.65it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for i,d in tqdm(enumerate(tokenized_datasets[\"dev\"])):\n",
    "    ref_input_ids = torch.tensor(d[\"ref_input_ids\"]).unsqueeze(0).to(device)\n",
    "    text_input_ids = torch.tensor(d[\"text_input_ids\"]).unsqueeze(0).to(device)\n",
    "    input_ids = [ref_input_ids, text_input_ids]\n",
    "    ref_attention_mask = torch.tensor(d[\"ref_attention_mask\"]).unsqueeze(0).to(device)\n",
    "    text_attention_mask = torch.tensor(d[\"text_attention_mask\"]).unsqueeze(0).to(device)\n",
    "    attention_mask = [ref_attention_mask, text_attention_mask]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[\"logits\"]\n",
    "        predicted_index = torch.argmax(logits, dim=-1)\n",
    "    results[i][\"rag_logits\"] = logits.cpu().numpy()\n",
    "    results[i][\"rag_label\"] = predicted_index.cpu().numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8096296296296296\n",
      "F1-score: 0.7414486921529175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# サンプルデータ\n",
    "data = results\n",
    "\n",
    "# 正解ラベルと予測ラベルをリストに抽出\n",
    "true_labels = [item['label'] for item in data]\n",
    "predicted_labels = [item['rag_label'][0] for item in data]\n",
    "\n",
    "# accuracy と F1-score の計算\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels, average='binary')  # binary タスク用\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ数を他2つと合わせる\n",
    "def create_trip(data, id_list):\n",
    "    trip = []\n",
    "    for id in id_list:\n",
    "        num = 0\n",
    "        no_hal = []\n",
    "        has_hal = []\n",
    "        for d in data:\n",
    "            if num == 6:\n",
    "                num = 0\n",
    "                if no_hal == [] or has_hal == []:\n",
    "                    break\n",
    "                '''\n",
    "                for no in no_hal:\n",
    "                    for has in has_hal:\n",
    "                        trip.append({\"anchor\":ref,\"positive\": no, \"negative\": has, \"labels\":[0,1]})\n",
    "                '''\n",
    "                if len(no_hal)==1 or len(no_hal)==5:\n",
    "                    trip.append({\"anchor\":ref,\"positive\": no_hal[0], \"negative\": has_hal[0], \"labels\":[0,1]})\n",
    "                elif len(no_hal)==2 or len(no_hal)==4:\n",
    "                    trip.append({\"anchor\":ref,\"positive\": no_hal[0], \"negative\": has_hal[0], \"labels\":[0,1]})\n",
    "                    trip.append({\"anchor\":ref,\"positive\": no_hal[1], \"negative\": has_hal[1], \"labels\":[0,1]})\n",
    "                elif len(no_hal)==3:\n",
    "                    trip.append({\"anchor\":ref,\"positive\": no_hal[0], \"negative\": has_hal[0], \"labels\":[0,1]})\n",
    "                    trip.append({\"anchor\":ref,\"positive\": no_hal[1], \"negative\": has_hal[1], \"labels\":[0,1]})\n",
    "                    trip.append({\"anchor\":ref,\"positive\": no_hal[2], \"negative\": has_hal[2], \"labels\":[0,1]})\n",
    "                no_hal = []\n",
    "                has_hal = []\n",
    "                break\n",
    "            elif d[\"source_id\"] == id:\n",
    "                num +=1\n",
    "                ref = d[\"ref\"]\n",
    "                if d[\"labels\"] == 0:\n",
    "                    no_hal.append(d[\"text\"])\n",
    "                else:\n",
    "                    has_hal.append(d[\"text\"])\n",
    "        if num == 6:\n",
    "            if len(no_hal)==1 or len(no_hal)==5:\n",
    "                trip.append({\"anchor\":ref,\"positive\": no_hal[0], \"negative\": has_hal[0], \"labels\":[0,1]})\n",
    "            elif len(no_hal)==2 or len(no_hal)==4:\n",
    "                trip.append({\"anchor\":ref,\"positive\": no_hal[0], \"negative\": has_hal[0], \"labels\":[0,1]})\n",
    "                trip.append({\"anchor\":ref,\"positive\": no_hal[1], \"negative\": has_hal[1], \"labels\":[0,1]})\n",
    "            elif len(no_hal)==3:\n",
    "                trip.append({\"anchor\":ref,\"positive\": no_hal[0], \"negative\": has_hal[0], \"labels\":[0,1]})\n",
    "                trip.append({\"anchor\":ref,\"positive\": no_hal[1], \"negative\": has_hal[1], \"labels\":[0,1]})\n",
    "                trip.append({\"anchor\":ref,\"positive\": no_hal[2], \"negative\": has_hal[2], \"labels\":[0,1]})\n",
    "    return trip\n",
    "\n",
    "def create_trip_ver2(data, id_list):\n",
    "    trip = []\n",
    "    for d in data:\n",
    "        if d[\"labels\"] == 0: # halluってない\n",
    "            trip.append({\"anchor\":d[\"ref\"],\"positive\": d[\"text\"], \"negative\": \"hallucination\", \"labels\":[0,0]})\n",
    "        else:\n",
    "            trip.append({\"anchor\":d[\"ref\"],\"positive\": \"not hallucination\", \"negative\": d[\"text\"], \"labels\":[1,1]})\n",
    "    return trip\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    dev: Dataset({\n",
       "        features: ['anchor', 'positive', 'negative', 'labels'],\n",
       "        num_rows: 2700\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "dev_id = [d[\"source_id\"] for d in dev_data]\n",
    "dev_id = list(set(dev_id))\n",
    "dev_trip = create_trip_ver2(dev_data, dev_id)\n",
    "\n",
    "\n",
    "dev_df = pd.DataFrame(dev_trip)\n",
    "dev_ds = Dataset.from_pandas(dev_df)\n",
    "\n",
    "raw_datasets = DatasetDict({\"dev\":dev_ds})\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def triplet_loss(anchor_output, positive_output, negative_output, positive_logits, negative_logits):\n",
    "    # ラベル予測の損失計算\n",
    "    positive_targets = torch.zeros(positive_output.size(0), dtype=torch.long).to(device)  # ラベル1 (positive)\n",
    "    negative_targets = torch.ones(negative_output.size(0), dtype=torch.long).to(device)\n",
    "    positive_loss = nn.CrossEntropyLoss()(positive_logits, positive_targets)\n",
    "    negative_loss = nn.CrossEntropyLoss()(negative_logits, negative_targets)\n",
    "\n",
    "    # ラベル予測の損失を平均\n",
    "    classification_loss = (positive_loss + negative_loss) / 2.0\n",
    "\n",
    "    # トリプレット損失の計算\n",
    "    #triplet_loss = nn.TripletMarginLoss(margin=1, p=2)(anchor_output, positive_output, negative_output)\n",
    "    triplet_loss_fn = (nn.TripletMarginWithDistanceLoss(margin=1.0,distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)))\n",
    "    triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "    # 最終損失\n",
    "    total_loss = classification_loss + triplet_loss \n",
    "\n",
    "    return classification_loss, triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc882d1ca0c1429fab6194f96fb24484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8a64db48c540f8b9f62a6f86a129bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "from models import TripletModel\n",
    "\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "# モデルとトークナイザーを読み込む\n",
    "name = \"./1228_triplet\"\n",
    "model = TripletModel.from_pretrained(base_model, triplet_loss, name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "# 使う装置\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    anchor = tokenizer(examples[\"anchor\"], truncation=True,max_length=512)\n",
    "    positive = tokenizer(examples[\"positive\"], truncation=True,max_length=512)\n",
    "    negative = tokenizer(examples[\"negative\"], truncation=True,max_length=512)\n",
    "\n",
    "    return {\n",
    "        \"anchor_input_ids\": anchor[\"input_ids\"],\n",
    "        \"anchor_attention_mask\": anchor[\"attention_mask\"],\n",
    "        \"positive_input_ids\": positive[\"input_ids\"],\n",
    "        \"positive_attention_mask\": positive[\"attention_mask\"],\n",
    "        \"negative_input_ids\": negative[\"input_ids\"],\n",
    "        \"negative_attention_mask\": negative[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"anchor\", \"positive\", \"negative\"])\n",
    "#tokenized_datasets.set_format(\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2700it [15:29,  2.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for i,d in tqdm(enumerate(tokenized_datasets[\"dev\"])):\n",
    "    \n",
    "    flag = False # hallってるのはTrue\n",
    "    if d[\"labels\"][0] ==1:\n",
    "        flag = True\n",
    "    \n",
    "    anchor_input_ids = torch.tensor(d[\"anchor_input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    positive_input_ids = torch.tensor(d[\"positive_input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    negative_input_ids = torch.tensor(d[\"negative_input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    input_ids = [anchor_input_ids, positive_input_ids, negative_input_ids]\n",
    "    anchor_attention_mask = torch.tensor(d[\"anchor_attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    positive_attention_mask = torch.tensor(d[\"positive_attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    negative_attention_mask = torch.tensor(d[\"negative_attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask = [anchor_attention_mask, positive_attention_mask, negative_attention_mask]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        #print(d[\"labels\"])\n",
    "        if flag:\n",
    "            logits = outputs.logits[1]\n",
    "        else:\n",
    "            logits = outputs.logits[0]\n",
    "        predicted_index = torch.argmax(logits, dim=-1)\n",
    "    results[i][\"triplet_logits\"] = logits.cpu().numpy()\n",
    "    results[i][\"triplet_label\"] = predicted_index.cpu().numpy()\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7077777777777777\n",
      "F1-score: 0.6265972550875533\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# サンプルデータ\n",
    "data = results\n",
    "\n",
    "# 正解ラベルと予測ラベルをリストに抽出\n",
    "true_labels = [item['label'] for item in data]\n",
    "predicted_labels = [item['triplet_label'][0] for item in data]\n",
    "\n",
    "# accuracy と F1-score の計算\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels, average='binary')  # binary タスク用\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2699,\n",
       " 'label': 0,\n",
       " 'task': 'QA',\n",
       " 'no_rag_logits': [[5.56593656539917, -5.5176544189453125]],\n",
       " 'no_rag_label': [0],\n",
       " 'rag_logits': [[2.7037570476531982, -1.0352840423583984]],\n",
       " 'rag_label': [0],\n",
       " 'triplet_logits': [[1.4126927852630615, -0.585303544998169]],\n",
       " 'triplet_label': [0]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まず保存\n",
    "import numpy as np\n",
    "import json\n",
    "for result in results:\n",
    "    if isinstance(result[\"no_rag_logits\"], np.ndarray):\n",
    "        result[\"no_rag_logits\"] = result[\"no_rag_logits\"].tolist()\n",
    "    if isinstance(result[\"rag_logits\"], np.ndarray):\n",
    "        result[\"rag_logits\"] = result[\"rag_logits\"].tolist()\n",
    "    if isinstance(result[\"triplet_logits\"], np.ndarray):\n",
    "        result[\"triplet_logits\"] = result[\"triplet_logits\"].tolist()\n",
    "    if isinstance(result[\"no_rag_label\"], np.ndarray):\n",
    "        result[\"no_rag_label\"] = result[\"no_rag_label\"].tolist()\n",
    "    if isinstance(result[\"rag_label\"], np.ndarray):\n",
    "        result[\"rag_label\"] = result[\"rag_label\"].tolist()\n",
    "    if isinstance(result[\"triplet_label\"], np.ndarray):\n",
    "        result[\"triplet_label\"] = result[\"triplet_label\"].tolist()\n",
    "\n",
    "\n",
    "\n",
    "with open(\"check_improve.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ここからでok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('check_improve.json',\"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "results = data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972 181 502 169 71 80 212 513\n"
     ]
    }
   ],
   "source": [
    "# no_rag→tripletでhallを検出できるようになった例\n",
    "num=0\n",
    "id_000 = [] # どっちでもhalじゃないと言えた\n",
    "id_001 = [] # どっちでも検出できなかった\n",
    "id_010 = [] # tripletの時に過検出\n",
    "id_011 = [] # tripletで検出できるようになった\n",
    "id_100 = [] # tripletでnoと言えるようになった\n",
    "id_101 = [] # tripletで検出できなくなった\n",
    "id_110 = [] # どっちでも過検出\n",
    "id_111 = [] # どっちでも検出できた\n",
    "for result in results:\n",
    "    if result[\"no_rag_label\"][0]==0 and result[\"triplet_label\"][0]==0 and result[\"label\"]==0:\n",
    "        id_000.append(result[\"id\"])\n",
    "    elif result[\"no_rag_label\"][0]==0 and result[\"triplet_label\"][0]==0 and result[\"label\"]==1:\n",
    "        id_001.append(result[\"id\"])\n",
    "    elif result[\"no_rag_label\"][0]==0 and result[\"triplet_label\"][0]==1 and result[\"label\"]==0:\n",
    "        id_010.append(result[\"id\"])\n",
    "    elif result[\"no_rag_label\"][0]==0 and result[\"triplet_label\"][0]==1 and result[\"label\"]==1:\n",
    "        id_011.append(result[\"id\"])\n",
    "    elif result[\"no_rag_label\"][0]==1 and result[\"triplet_label\"][0]==0 and result[\"label\"]==0:\n",
    "        id_100.append(result[\"id\"])\n",
    "    elif result[\"no_rag_label\"][0]==1 and result[\"triplet_label\"][0]==0 and result[\"label\"]==1:\n",
    "        id_101.append(result[\"id\"])\n",
    "    elif result[\"no_rag_label\"][0]==1 and result[\"triplet_label\"][0]==1 and result[\"label\"]==0:\n",
    "        id_110.append(result[\"id\"])\n",
    "    elif result[\"no_rag_label\"][0]==1 and result[\"triplet_label\"][0]==1 and result[\"label\"]==1:\n",
    "        id_111.append(result[\"id\"])\n",
    "    \n",
    "    \n",
    "# 000:972, 001:181, 010:502, 011:169, 100:71, 101:80, 110:212, 111:513\n",
    "print(len(id_000),len(id_001),len(id_010),len(id_011),len(id_100),len(id_101),len(id_110),len(id_111))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 長さ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llama-2-13b-chat': 925.9, 'mistral-7B-instruct': 738.5, 'llama-2-7b-chat': 1027.0, 'llama-2-70b-chat': 796.7, 'gpt-3.5-turbo-0613': 613.8, 'gpt-4-0613': 294.0}\n",
      "{'gpt-4-0613': 810.2, 'mistral-7B-instruct': 879.8, 'llama-2-7b-chat': 885.4, 'llama-2-13b-chat': 1128.9, 'llama-2-70b-chat': 986.3, 'gpt-3.5-turbo-0613': 1331.9}\n",
      "{'llama-2-13b-chat': 613.4, 'llama-2-70b-chat': 719.3, 'mistral-7B-instruct': 683.8, 'gpt-3.5-turbo-0613': 1209.5, 'llama-2-7b-chat': 646.0, 'gpt-4-0613': 861.0}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "hal_model_len_qa = defaultdict(lambda:[])\n",
    "hal_model_len_d2t = defaultdict(lambda:[])\n",
    "hal_model_len_sum = defaultdict(lambda:[])\n",
    "\n",
    "for d in test_data_span:\n",
    "    if d[\"hallucination_id\"] == []:\n",
    "        continue\n",
    "        \n",
    "    if d[\"task_type\"] == \"QA\":\n",
    "        hal_model_len_qa[d[\"model\"]].append(len(d[\"text\"]))\n",
    "    elif d[\"task_type\"] == \"Data2txt\":\n",
    "        hal_model_len_d2t[d[\"model\"]].append(len(d[\"text\"]))\n",
    "    else:\n",
    "        hal_model_len_sum[d[\"model\"]].append(len(d[\"text\"]))\n",
    "        \n",
    "hal_model_len_qa = dict(hal_model_len_qa)\n",
    "hal_model_len_d2t = dict(hal_model_len_d2t)\n",
    "hal_model_len_sum = dict(hal_model_len_sum)\n",
    "\n",
    "hal_model_len_qa = {k: round(np.mean(v), 1) for k, v in hal_model_len_qa.items()}\n",
    "hal_model_len_d2t = {k: round(np.mean(v), 1) for k, v in hal_model_len_d2t.items()}\n",
    "hal_model_len_sum = {k: round(np.mean(v), 1) for k, v in hal_model_len_sum.items()}\n",
    "\n",
    "\n",
    "print(hal_model_len_qa)\n",
    "print(hal_model_len_d2t)\n",
    "print(hal_model_len_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llama-2-70b-chat': 486.0, 'mistral-7B-instruct': 739.9, 'gpt-3.5-turbo-0613': 229.0, 'llama-2-7b-chat': 450.0, 'gpt-4-0613': 294.0, 'llama-2-13b-chat': 672.0}\n",
      "{'gpt-4-0613': 811.2, 'gpt-3.5-turbo-0613': 1331.9, 'llama-2-7b-chat': 722.8, 'llama-2-70b-chat': 794.2, 'mistral-7B-instruct': 927.7, 'llama-2-13b-chat': 1057.3}\n",
      "{'llama-2-70b-chat': 654.2, 'llama-2-13b-chat': 514.8, 'gpt-3.5-turbo-0613': 1209.5, 'llama-2-7b-chat': 672.8, 'gpt-4-0613': 861.0, 'mistral-7B-instruct': 646.5}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "hal_model_len_qa_imp = defaultdict(lambda:[])\n",
    "hal_model_len_d2t_imp = defaultdict(lambda:[])\n",
    "hal_model_len_sum_imp = defaultdict(lambda:[])\n",
    "\n",
    "for i in id_001 + id_101:\n",
    "    d = test_data_span[i]\n",
    "    if d[\"hallucination_id\"] == []:\n",
    "        continue\n",
    "        \n",
    "    if d[\"task_type\"] == \"QA\":\n",
    "        hal_model_len_qa_imp[d[\"model\"]].append(len(d[\"text\"]))\n",
    "    elif d[\"task_type\"] == \"Data2txt\":\n",
    "        hal_model_len_d2t_imp[d[\"model\"]].append(len(d[\"text\"]))\n",
    "    else:\n",
    "        hal_model_len_sum_imp[d[\"model\"]].append(len(d[\"text\"]))\n",
    "        \n",
    "hal_model_len_qa_imp = dict(hal_model_len_qa_imp)\n",
    "hal_model_len_d2t_imp = dict(hal_model_len_d2t_imp)\n",
    "hal_model_len_sum_imp = dict(hal_model_len_sum_imp)\n",
    "\n",
    "hal_model_len_qa_imp = {k: round(np.mean(v), 1) for k, v in hal_model_len_qa_imp.items()}\n",
    "hal_model_len_d2t_imp = {k: round(np.mean(v), 1) for k, v in hal_model_len_d2t_imp.items()}\n",
    "hal_model_len_sum_imp = {k: round(np.mean(v), 1) for k, v in hal_model_len_sum_imp.items()}\n",
    "\n",
    "\n",
    "print(hal_model_len_qa_imp)\n",
    "print(hal_model_len_d2t_imp)\n",
    "print(hal_model_len_sum_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 長さの分布\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "d2t_posi_len=[]\n",
    "d2t_nega_len =[]\n",
    "\n",
    "for d in train_data_span:\n",
    "    if d[\"task_type\"] != \"Data2txt\":\n",
    "        continue\n",
    "    \n",
    "    if d[\"labels\"] == 1:\n",
    "        d2t_nega_len.append(len(d[\"text\"]))\n",
    "    else:\n",
    "        d2t_posi_len.append(len(d[\"text\"]))\n",
    "        \n",
    "plt.hist(d2t_posi_len, alpha=0.5, label=\"Positive Length\", bins=100)\n",
    "plt.hist(d2t_nega_len, alpha=0.5, label=\"Negative Length\", bins=100)\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d2t_nega_len), len(d2t_posi_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hallucinationのタイプで見てみる？\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"rag_truth_span_test.json\", \"r\") as f:\n",
    "    test_data_span = json.load(f)\n",
    "with open(\"rag_truth_span_train.json\", \"r\") as f:\n",
    "    train_data_span = json.load(f)\n",
    "with open(\"rag_truth_span_dev.json\", \"r\") as f:\n",
    "    dev_data_span = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for d in dev_data_span:\n",
    "    if d[\"hallucination_id\"] == []:\n",
    "        continue\n",
    "    hal_type = []\n",
    "    for hal_id in d[\"hallucination_id\"]:\n",
    "        hal_type.append(hal_id[\"label_type\"])\n",
    "        \n",
    "    if d[\"task_type\"] == \"QA\":\n",
    "        for hal in hal_type:\n",
    "            if hal == \"Subtle Conflict\":\n",
    "                #print(d)\n",
    "                a.append(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ref': \"{'question': 'how do you remove programs and processes from windows xp', 'passages': 'passage 1:When the programs and features window opens select the program your want to uninstall from the list and click on the Uninstall button. 5. Confirm that you want to uninstall a program by clicking on the Yes button.\\\\n\\\\npassage 2:In the list of programs, click Windows XP, and then click Change/Remove. 3. Click Uninstall Windows XP, and then click Continue. 4. If any programs have been modified since the upgrade to Windows XP, you will receive a warning message.\\\\n\\\\npassage 3:When the Start Menu opens select Control Panel. 3. Depending on your view options either click on uninstall a program or program and features. 4. When the programs and features window opens select the program your want to uninstall from the list and click on the Uninstall button.\\\\n\\\\n'}\",\n",
       " 'text': 'Based on the provided passages, here is how you can remove programs and processes from Windows XP:\\n\\nPassage 1: To uninstall a program, follow these steps:\\n\\n1. Open the Programs and Features window by typing \"Add or Remove Programs\" in the Start menu search bar.\\n2. Select the program you want to uninstall from the list.\\n3. Click the Uninstall button to start the uninstallation process.\\n4. Confirm that you want to uninstall the program by clicking the Yes button.\\n\\nPassage 2: To uninstall Windows XP, follow these steps:\\n\\n1. In the Start menu, type \"Control Panel\" and open it.\\n2. Click on \"System and Security\" or \"Security and Maintenance\".\\n3. Click on \"Change/Remove\" or \"Uninstall Windows XP\".\\n4. Follow the prompts to complete the uninstallation process.\\n\\nPassage 3: To uninstall a program using the Start menu, follow these steps:\\n\\n1. Open the Start menu and select \"Control Panel\".\\n2. Click on \"Programs and Features\" or \"Uninstall a Program\".\\n3. Select the program you want to uninstall from the list.\\n4. Click the Uninstall button to start the uninstallation process.\\n\\nBased on the provided passages, there is no way to directly remove processes from Windows XP. The passages only provide instructions for removing programs and the Windows XP operating system itself. Therefore, I am unable to answer your question based on the given passages.',\n",
       " 'labels': 1,\n",
       " 'hallucination_id': [{'start': 156,\n",
       "   'end': 261,\n",
       "   'text': '1. Open the Programs and Features window by typing \"Add or Remove Programs\" in the Start menu search bar.',\n",
       "   'meta': None,\n",
       "   'label_type': 'Evident Baseless Info',\n",
       "   'implicit_true': False,\n",
       "   'due_to_null': False},\n",
       "  {'start': 527,\n",
       "   'end': 579,\n",
       "   'text': 'In the Start menu, type \"Control Panel\" and open it.',\n",
       "   'meta': 'SUBTLE CONFLICT\\nOriginal: When the Start Menu opens select Control Panel\\nGenerated: In the Start menu, type \"Control Panel\" and open it.',\n",
       "   'label_type': 'Subtle Conflict',\n",
       "   'implicit_true': False,\n",
       "   'due_to_null': False},\n",
       "  {'start': 580,\n",
       "   'end': 644,\n",
       "   'text': '2. Click on \"System and Security\" or \"Security and Maintenance\".',\n",
       "   'meta': 'HIGH INTRODUCTION OF NEW INFORMATION\\n\"2. Click on \"System and Security\" or \"Security and Maintenance\".\" was not mentioned in the passages',\n",
       "   'label_type': 'Evident Baseless Info',\n",
       "   'implicit_true': False,\n",
       "   'due_to_null': False}],\n",
       " 'hallucination': {'hallucination_list': ['1. Open the Programs and Features window by typing \"Add or Remove Programs\" in the Start menu search bar.',\n",
       "   'In the Start menu, type \"Control Panel\" and open it.',\n",
       "   '2. Click on \"System and Security\" or \"Security and Maintenance\".']},\n",
       " 'source': 'MARCO',\n",
       " 'model': 'llama-2-7b-chat',\n",
       " 'task_type': 'QA',\n",
       " 'source_id': '12259'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ref': 'The FBI charged a Philadelphia woman on Thursday with trying to travel overseas to fight for ISIS. She\\'s one of three women arrested this week on terror charges. Two New York women were also taken into custody. An FBI complaint cites numerous social media messages dating back to August 2013 that were sent by Keonna Thomas, 30, also known as \"Young Lioness\" and \"Fatayat Al Khilafah.\" One Twitter message said, \"If we truly knew the realities ... we all would be rushing to join our brothers in the front lines pray ALLAH accept us as shuhada [martyrs].\" Another said, \"When you\\'re a mujahid [violent jihadi fighter] your death becomes a wedding.\" The FBI said Thomas purchased an electronic visa to Turkey on March 23. Turkey is known as the easiest place from which to enter Syria and join ISIS. An ISIS manual advises recruits to buy round-trip tickets to vacation spots such as Spain and then purchase tickets for their real destination once they arrive overseas, the FBI said. On March 26, Thomas purchased a ticket to Barcelona, with a March 29 departure and an April 15 return to the United States, the complaint said. It\\'s not clear when or where she was arrested. She was charged with knowingly attempting to provide material support and resources to a designated foreign terrorist organization. She could be sentenced to 15 years in prison. On Thursday, Noelle Velentzas, 28, and her former roommate, Asia Siddiqui, 31, were arrested in New York and accused of planning to build an explosive device for attacks in the United States, federal prosecutors said. In the past 18 months, the Justice Department\\'s National Security Division has prosecuted or is prosecuting more than 30 cases of people attempting to travel abroad to join or provide support to terrorist groups. Of those cases, 18 allegedly involve support to ISIS. \"The terrorist threat is more decentralized, more diffuse, more complicated,\" Homeland Security Secretary Jeh Johnson told reporters Thursday. \"It involves the potential lone wolf actor, it involves the effective use of social media, the Internet.\"\\n',\n",
       " 'text': 'The FBI has charged a Philadelphia woman, Keonna Thomas, with trying to travel overseas to fight for ISIS. This follows the arrests of two New York women, Noelle Velentzas and Asia Siddiqui, who were accused of planning to build an explosive device for attacks in the United States. Thomas purchased an electronic visa to Turkey and a round-trip ticket to Barcelona, following the advice given in an ISIS manual for recruits. She has been charged with attempting to provide material support to a terrorist organisation and could face 15 years in prison.',\n",
       " 'labels': 0,\n",
       " 'hallucination_id': [],\n",
       " 'hallucination': {'hallucination_list': []},\n",
       " 'source': 'CNN/DM',\n",
       " 'model': 'gpt-4-0613',\n",
       " 'task_type': 'Summary',\n",
       " 'source_id': '15596'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_span[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Evident Baseless Info': 156, 'Evident Conflict': 30, 'Subtle Baseless Info': 49}\n",
      "{'Evident Conflict': 489, 'Evident Baseless Info': 465, 'Subtle Baseless Info': 95, 'Subtle Conflict': 5}\n",
      "{'Evident Conflict': 100, 'Evident Baseless Info': 117, 'Subtle Conflict': 11, 'Subtle Baseless Info': 16}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "hal_type_qa = defaultdict(lambda:0)\n",
    "hal_type_d2t = defaultdict(lambda:0)\n",
    "hal_type_sum = defaultdict(lambda:0)\n",
    "\n",
    "for d in test_data_span:\n",
    "    if d[\"hallucination_id\"] == []:\n",
    "        continue\n",
    "    hal_type = []\n",
    "    for hal_id in d[\"hallucination_id\"]:\n",
    "        hal_type.append(hal_id[\"label_type\"])\n",
    "        \n",
    "    if d[\"task_type\"] == \"QA\":\n",
    "        for hal in hal_type:\n",
    "            hal_type_qa[hal] += 1\n",
    "    elif d[\"task_type\"] == \"Data2txt\":\n",
    "        for hal in hal_type:\n",
    "            hal_type_d2t[hal] += 1\n",
    "    else:\n",
    "        for hal in hal_type:\n",
    "            hal_type_sum[hal] += 1\n",
    "print(dict(hal_type_qa))\n",
    "print(dict(hal_type_d2t))\n",
    "print(dict(hal_type_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Evident Conflict': 23, 'Evident Baseless Info': 63, 'Subtle Baseless Info': 18}\n",
      "{'Evident Conflict': 61, 'Evident Baseless Info': 73, 'Subtle Baseless Info': 19, 'Subtle Conflict': 2}\n",
      "{'Evident Conflict': 82, 'Subtle Conflict': 9, 'Evident Baseless Info': 86, 'Subtle Baseless Info': 6}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "hal_type_qa_imp = defaultdict(lambda:0)\n",
    "hal_type_d2t_imp = defaultdict(lambda:0)\n",
    "hal_type_sum_imp = defaultdict(lambda:0)\n",
    "\n",
    "for i in id_001 + id_011:\n",
    "    if test_data_span[i][\"hallucination_id\"]==[]:\n",
    "        continue\n",
    "    hal_type_imp = []\n",
    "    for hal_id in test_data_span[i][\"hallucination_id\"]:\n",
    "        hal_type_imp.append(hal_id[\"label_type\"])\n",
    "        \n",
    "    if test_data_span[i][\"task_type\"] == \"QA\":\n",
    "        for hal in hal_type_imp:\n",
    "            hal_type_qa_imp[hal] += 1\n",
    "    elif test_data_span[i][\"task_type\"] == \"Data2txt\":\n",
    "        for hal in hal_type_imp:\n",
    "            hal_type_d2t_imp[hal] += 1\n",
    "    else:\n",
    "        for hal in hal_type_imp:\n",
    "            hal_type_sum_imp[hal] += 1\n",
    "print(dict(hal_type_qa_imp))\n",
    "print(dict(hal_type_d2t_imp))\n",
    "print(dict(hal_type_sum_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = []\n",
    "for i in id_011:\n",
    "    if test_data_span[i][\"task_type\"]!=\"QA\":\n",
    "        continue\n",
    "    \n",
    "    hal_type2 = []\n",
    "    for hal_id in test_data_span[i][\"hallucination_id\"]:\n",
    "        hal_type2.append(hal_id[\"label_type\"])\n",
    "    \n",
    "    if \"Evident Conflict\" in hal_type2:\n",
    "        sample.append(test_data_span[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ref': '{\\'question\\': \\'history of the word gemini\\', \\'passages\\': \"passage 1:The word “Gemini” is a Latin word for twins and it is one of the constellations that actually look like its name would suggest. The first known reference of the Gemini constellation was in Aristotle’s Meteorologica, over 300 years BC. He mentions that he observed an occulting a star in Gemini and speaks of observing Jupiter in conjunction with it.\\\\n\\\\npassage 2:Gemini History. The History of Gemini. As other signs in the zodiac, Gemini is not in the same position as the constellation of Gemini. In the zodiac, it follows Taurus and takes the third 30 degrees of the zodiacal circle. Gemini is a mutable sign that is preceding the summer, and as such, it announces change while ruling the time of year when Taurus spring has ended, and life on Earth is about to change.\\\\n\\\\npassage 3:Gemini Mythology The Story Behind the Gemini Zodiac Sign. The origins of some constellation myths are heavily debated. Gemini mythology, luckily, is pretty straightforward. By all accounts, the twins represented in Gemini mythology are Castor and Pollux of Greek mythology. They share the same mother, Leda (Greek mythology), but have different fathers. Castor\\'s father is Tyndarus, the King of Sparta, and Leda\\'s husband.\\\\n\\\\n\"}',\n",
       " 'text': 'Based on the given passages, we can gather that the word \"Gemini\" comes from the Latin language and refers to twins. It is also one of the constellations that resembles its name. The first known reference to the Gemini constellation was in Aristotle\\'s Meteorologica, around 300 years ago. The constellation represents the twins Castor and Pollux in Greek mythology.',\n",
       " 'labels': 1,\n",
       " 'hallucination_id': [{'start': 267,\n",
       "   'end': 287,\n",
       "   'text': 'around 300 years ago',\n",
       "   'meta': 'EVIDENT CONFLICT\\nOriginal: 300 years BC\\nGenerative: around 300 years ago',\n",
       "   'label_type': 'Evident Conflict',\n",
       "   'implicit_true': False,\n",
       "   'due_to_null': False}],\n",
       " 'hallucination': {'hallucination_list': ['around 300 years ago']},\n",
       " 'source': 'MARCO',\n",
       " 'model': 'mistral-7B-instruct',\n",
       " 'task_type': 'QA',\n",
       " 'source_id': '15540'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = []\n",
    "for i in id_101:\n",
    "    if test_data_span[i][\"task_type\"]!=\"Data2txt\":\n",
    "        continue\n",
    "    \n",
    "    hal_type2 = []\n",
    "    for hal_id in test_data_span[i][\"hallucination_id\"]:\n",
    "        hal_type2.append(hal_id[\"label_type\"])\n",
    "    \n",
    "    if \"Subtle Baseless Info\" in hal_type2 and \"Evident Baseless Info\" in hal_type2:\n",
    "        sample2.append(test_data_span[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ref': '{\\'name\\': \\'Finch & Fork\\', \\'address\\': \\'31 W Carrillo St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Breakfast & Brunch, American (New), Restaurants, American (Traditional), Nightlife, Bars\\', \\'hours\\': {\\'Monday\\': \\'17:30-23:0\\', \\'Tuesday\\': \\'17:0-21:0\\', \\'Wednesday\\': \\'17:0-21:0\\', \\'Thursday\\': \\'17:0-21:0\\', \\'Friday\\': \\'17:0-21:0\\', \\'Saturday\\': \\'17:0-21:0\\', \\'Sunday\\': \\'9:0-14:0\\'}, \\'attributes\\': {\\'BusinessParking\\': {\\'garage\\': True, \\'street\\': True, \\'validated\\': True, \\'lot\\': False, \\'valet\\': True}, \\'RestaurantsReservations\\': True, \\'OutdoorSeating\\': False, \\'WiFi\\': \\'free\\', \\'RestaurantsTakeOut\\': True, \\'RestaurantsGoodForGroups\\': True, \\'Music\\': False, \\'Ambience\\': {\\'romantic\\': False, \\'intimate\\': False, \\'classy\\': True, \\'hipster\\': False, \\'divey\\': False, \\'touristy\\': False, \\'trendy\\': False, \\'upscale\\': False, \\'casual\\': False}}, \\'business_stars\\': 4.0, \\'review_info\\': [{\\'review_stars\\': 5.0, \\'review_date\\': \\'2022-01-09 19:18:59\\', \\'review_text\\': \\'What a great brunch. A couple days ago my wife randomly decided she\\\\\\'d book a hotel in Santa Barbara for the weekend. She said she had already found a good spot for brunch Sunday morning. Usually I plan all the trips so she had a lot of pressure not to disappoint. And... she did not. Moral of the story: here we are now, just enjoyed a delicious breakfast burrito (her) and steak/eggs/frites (me). Most importantly the \"mimosa madness\" was served just the way I like it, just a splash of juice. This may have influenced this review :)\\\\nOur server Jamie was awesome and very hospitable. 5 stars for food, ambiance, and service. We\\\\\\'ll be back some day.\\'}, {\\'review_stars\\': 5.0, \\'review_date\\': \\'2021-12-24 05:45:27\\', \\'review_text\\': \\'main downside is that the food was overpriced. taste and quality of the food was great and service was also good. ambiance is nice, the restaurant is located in a hotel lobby.\\'}, {\\'review_stars\\': 5.0, \\'review_date\\': \\'2021-12-21 03:21:40\\', \\'review_text\\': \"The best bottomless brunch! $14 for the first mimosa, and $1 for every mimosa after that. \\\\n\\\\nWe came on a Sunday for brunch and we didn\\'t have to wait which was nice. The restaurant is inside a VERY nice hotel, and I was shocked at how cheap the menu is, given how nice the ambiance is. \\\\n\\\\nOur server Matthew was great. We were in SB for the day and he was giving us tips on what to do & where to go. He was very outgoing & kind, and the service was fast and on point. \\\\n\\\\nThe food was good as well, I got the eggs benedict and it was filling & delicious. The mimosas were BOMB and so cheap... we paid only $16 each for bottomless! They also have different flavors: orange, grapefruit, and poinsettia/cranberry which were all v tasty. \\\\n\\\\nDEFINITELY recommend this place to anyone looking for a good bottomless brunch!\"}]}',\n",
       " 'text': 'Finch & Fork is a breakfast and brunch restaurant located in the heart of Santa Barbara, within a luxury hotel. The restaurant offers a variety of dishes including breakfast burritos, steak, eggs, and frites, along with a selection of bottomless mimosas. According to customer reviews, the food is delicious, with generous portions, and the service is friendly and attentive. The ambiance is described as nice, with a classy and upscale atmosphere, despite being located in a hotel lobby. Some customers have noted that the prices are a bit high, but the quality of the food and service makes up for it. Finch & Fork is a popular spot for both locals and visitors, and is especially known for its bottomless brunch deals. Overall, this restaurant is a great choice for those looking for a tasty and satisfying breakfast or brunch experience in Santa Barbara.',\n",
       " 'labels': 1,\n",
       " 'hallucination_id': [{'start': 429,\n",
       "   'end': 448,\n",
       "   'text': 'upscale atmosphere,',\n",
       "   'meta': 'EVIDENT CONFLICT\\nOriginal: false for upscale\\nAIGC: with a[n] upscale atmosphere',\n",
       "   'label_type': 'Evident Conflict',\n",
       "   'implicit_true': False,\n",
       "   'due_to_null': False},\n",
       "  {'start': 622,\n",
       "   'end': 663,\n",
       "   'text': 'popular spot for both locals and visitors',\n",
       "   'meta': 'Original: No mention that this restaurant is a popular spot for locals and visitors. ',\n",
       "   'label_type': 'Evident Baseless Info',\n",
       "   'implicit_true': False,\n",
       "   'due_to_null': False},\n",
       "  {'start': 50,\n",
       "   'end': 88,\n",
       "   'text': 'located in the heart of Santa Barbara,',\n",
       "   'meta': 'LOW INTRO OF NEW INFO\\nNo mention of this information but may stand to be accurate. ',\n",
       "   'label_type': 'Subtle Baseless Info',\n",
       "   'implicit_true': True,\n",
       "   'due_to_null': False}],\n",
       " 'hallucination': {'hallucination_list': ['upscale atmosphere,',\n",
       "   'popular spot for both locals and visitors',\n",
       "   'located in the heart of Santa Barbara,']},\n",
       " 'source': 'Yelp',\n",
       " 'model': 'llama-2-70b-chat',\n",
       " 'task_type': 'Data2txt',\n",
       " 'source_id': '13601'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample3 = []\n",
    "for i in id_011:\n",
    "    if test_data_span[i][\"task_type\"]!=\"Summary\":\n",
    "        continue\n",
    "    \n",
    "    hal_type2 = []\n",
    "    for hal_id in test_data_span[i][\"hallucination_id\"]:\n",
    "        hal_type2.append(hal_id[\"label_type\"])\n",
    "    \n",
    "    if \"Subtle Baseless Info\" in hal_type2 or \"Evident Baseless Info\" in hal_type2:\n",
    "        sample3.append(test_data_span[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ref': 'A nuclear submarine being repaired at a Russian shipyard has caught on fire, according to a law enforcement source speaking to Russia\\'s state-run news agency ITAR-Tass. \"The submarine is in a dry dock,\" Tass reports, citing the source, and there is no ammunition on board. \"The rubber insulation between the submarine\\'s light and pressure hull is on fire,\" Tass reported. Russia\\'s RIA Novosti news agency says insulation caught on fire as welding work was being done on the submarine. Tass reported that the fire began on a sub in the Zvyozdochka shipyard in northwestern Russia. Zvyozdochka spokesman Yevgeny Gladyshev told the news agency that the sub had been undergoing repairs since November 2013. \"Nuclear fuel from the sub\\'s reactor has been unloaded,\" he reportedly said. \"There are no armaments or chemically active, dangerous substances, fissionable materials on it,\" Gladyshev said to Tass. \"The enterprise\\'s personnel left the premises when the submarine caught fire, no one has been injured. The fire presents no threat to people and the shipyard.\"\\n',\n",
       " 'text': 'A nuclear submarine being repaired in Russia caught fire due to accidental welding damage, causing an explosion that released radiation into the air. Four workers were killed in the incident and over 170 others were hospitalized. The cause of the explosion remains unknown but experts have suggested that a spark may have ignited a hydrogen leak. The damaged sub is currently being dismantled and decommissioned.',\n",
       " 'labels': 1,\n",
       " 'hallucination_id': [{'start': 91,\n",
       "   'end': 149,\n",
       "   'text': 'causing an explosion that released radiation into the air.',\n",
       "   'meta': 'HIGH INTRO OF NEW INFO\\nNo mention of an explosion. ',\n",
       "   'label_type': 'Evident Baseless Info',\n",
       "   'implicit_true': False,\n",
       "   'due_to_null': False},\n",
       "  {'start': 150,\n",
       "   'end': 346,\n",
       "   'text': 'Four workers were killed in the incident and over 170 others were hospitalized. The cause of the explosion remains unknown but experts have suggested that a spark may have ignited a hydrogen leak.',\n",
       "   'meta': 'HIGH INTRO OF NEW INFO\\nNo mention of these details. ',\n",
       "   'label_type': 'Evident Baseless Info',\n",
       "   'implicit_true': False,\n",
       "   'due_to_null': False},\n",
       "  {'start': 347,\n",
       "   'end': 412,\n",
       "   'text': 'The damaged sub is currently being dismantled and decommissioned.',\n",
       "   'meta': 'HIGH INTRO OF NEW INFO\\nNo mention of these details. ',\n",
       "   'label_type': 'Evident Baseless Info',\n",
       "   'implicit_true': False,\n",
       "   'due_to_null': False}],\n",
       " 'hallucination': {'hallucination_list': ['causing an explosion that released radiation into the air.',\n",
       "   'Four workers were killed in the incident and over 170 others were hospitalized. The cause of the explosion remains unknown but experts have suggested that a spark may have ignited a hydrogen leak.',\n",
       "   'The damaged sub is currently being dismantled and decommissioned.']},\n",
       " 'source': 'CNN/DM',\n",
       " 'model': 'mistral-7B-instruct',\n",
       " 'task_type': 'Summary',\n",
       " 'source_id': '15714'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample3[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample3_len = [len(x[\"ref\"])+len(x[\"text\"]) for x in sample3]\n",
    "\n",
    "# 6 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./nlp/lib/python3.9/site-packages (4.48.0)\n",
      "Requirement already satisfied: datasets in ./nlp/lib/python3.9/site-packages (2.19.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001b[K     |████████████████████████████████| 480 kB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: huggingface_hub in ./nlp/lib/python3.9/site-packages (0.27.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./nlp/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./nlp/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./nlp/lib/python3.9/site-packages (from transformers) (1.24.0)\n",
      "Requirement already satisfied: requests in ./nlp/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./nlp/lib/python3.9/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./nlp/lib/python3.9/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: filelock in ./nlp/lib/python3.9/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./nlp/lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./nlp/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./nlp/lib/python3.9/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: aiohttp in ./nlp/lib/python3.9/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: xxhash in ./nlp/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pandas in ./nlp/lib/python3.9/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in ./nlp/lib/python3.9/site-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./nlp/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./nlp/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./nlp/lib/python3.9/site-packages (from huggingface_hub) (4.12.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./nlp/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./nlp/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./nlp/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./nlp/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./nlp/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./nlp/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./nlp/lib/python3.9/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./nlp/lib/python3.9/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./nlp/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./nlp/lib/python3.9/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./nlp/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./nlp/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./nlp/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./nlp/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.19.1\n",
      "    Uninstalling datasets-2.19.1:\n",
      "      Successfully uninstalled datasets-2.19.1\n",
      "Successfully installed datasets-3.2.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/home/3/uw03923/nlp/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers datasets huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"rag_truth_test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "def add_prefix(data):\n",
    "    for d in data:\n",
    "        d[\"text\"] = \"Please judge the following statement as true or false based on the references above: \" + d[\"text\"]\n",
    "    return data\n",
    "\n",
    "test_data = add_prefix(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"rag_truth_dev.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "def add_prefix(data):\n",
    "    for d in data:\n",
    "        d[\"text\"] = \"Please judge the following statement as true or false based on the references above: \" + d[\"text\"]\n",
    "    return data\n",
    "\n",
    "test_data = add_prefix(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_type: QA, Data2txt, Summary\n",
    "task_name = \"QA\"\n",
    "\n",
    "test_data = [d for d in test_data if d[\"task_type\"] == task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "test_data = random.sample(test_data, 100)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 15:53:07.995293: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-17 15:53:08.008292: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-17 15:53:08.025242: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-17 15:53:08.025273: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-17 15:53:08.037857: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-17 15:53:09.208306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d84e1626b804508a61d2cf9a5d9b8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# RoBERTaで埋め込みを取得（[CLS]トークンの出力を使用）\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 35\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:] \u001b[38;5;66;03m#baseの時\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#embeddings =outputs.output #訓練済みの時\u001b[39;00m\n",
      "File \u001b[0;32m~/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/nlp/lib64/python3.9/site-packages/transformers/models/phi3/modeling_phi3.py:605\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# decoder layers\u001b[39;00m\n\u001b[1;32m    608\u001b[0m all_hidden_states \u001b[38;5;241m=\u001b[39m () \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/nlp/lib64/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nlp/lib64/python3.9/site-packages/transformers/models/phi3/modeling_phi3.py:368\u001b[0m, in \u001b[0;36mPhi3RotaryEmbedding.forward\u001b[0;34m(self, x, position_ids)\u001b[0m\n\u001b[1;32m    366\u001b[0m device_type \u001b[38;5;241m=\u001b[39m device_type \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device_type, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m device_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39mdevice_type, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 368\u001b[0m     freqs \u001b[38;5;241m=\u001b[39m (\u001b[43minv_freq_expanded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids_expanded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    369\u001b[0m     emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((freqs, freqs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    370\u001b[0m     cos \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39mcos()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_bmm)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "import transformers\n",
    "from models_rob import NoRagModel\n",
    "\n",
    "# ===== 1. RoBERTaで埋め込みを生成 =====\n",
    "# トークナイザーとモデルを読み込む\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name,device_map=\"auto\")\n",
    "\n",
    "\n",
    "#model = NoRagModel.from_pretrained(base_model, \"0102_no_rag_rob\")\n",
    "\n",
    "\n",
    "# デバイスの設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# サンプルデータ\n",
    "texts = [x[\"text\"] for x in test_data]\n",
    "#texts = [\"これはテストです。\", \"これはテストですか。\", \"これはテストです。\"]\n",
    "\n",
    "# トークン化\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "\n",
    "# RoBERTaで埋め込みを取得（[CLS]トークンの出力を使用）\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs[0][:,-1,:] #baseの時\n",
    "    #embeddings =outputs.output #訓練済みの時\n",
    "    embeddings = embeddings.cpu().numpy()  # numpy形式に変換\n",
    "\n",
    "# ===== 2. UMAPで次元削減 =====\n",
    "# 次元削減\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "embeddings_2d = umap_reducer.fit_transform(embeddings)\n",
    "\n",
    "#print(f\"Number of texts: {len(texts)}\")\n",
    "#print(f\"Shape of embeddings_2d: {embeddings_2d.shape}\")\n",
    "#print(\"Shape of embeddings:\", embeddings.shape)\n",
    "\n",
    "\n",
    "# ===== 3. 可視化 =====\n",
    "# プロット\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, text in enumerate(texts):\n",
    "    \"\"\"\n",
    "    if test_data[i][\"task_type\"] == \"QA\":\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"red\")\n",
    "    elif test_data[i][\"task_type\"] == \"Data2txt\":\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"blue\")\n",
    "    else:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"green\")\n",
    "    \"\"\"\n",
    "    if test_data[i][\"task_type\"] == \"QA\" and test_data[i][\"labels\"] == 1:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"red\",s = 10,marker=\"^\")\n",
    "    elif test_data[i][\"task_type\"] == \"QA\" and test_data[i][\"labels\"] == 0:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#ff1493\",s = 10)\n",
    "    elif test_data[i][\"task_type\"] == \"Data2txt\" and test_data[i][\"labels\"] == 1:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#00008b\",s = 10,marker=\"^\")\n",
    "    elif test_data[i][\"task_type\"] == \"Data2txt\" and test_data[i][\"labels\"] == 0:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"blue\",s = 10)\n",
    "    elif test_data[i][\"task_type\"] == \"Summary\" and test_data[i][\"labels\"] == 1:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"green\",s = 10,marker=\"^\")\n",
    "    elif test_data[i][\"task_type\"] == \"Summary\" and test_data[i][\"labels\"] == 0:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#32cd32\",s = 10)\n",
    "\n",
    "\"\"\"\n",
    "plt.scatter([], [],color = \"red\", label = \"QA\")\n",
    "plt.scatter([], [],color = \"blue\", label = \"Data2txt\")\n",
    "plt.scatter([], [],color = \"green\", label = \"Summary\")\n",
    "\"\"\"\n",
    "plt.scatter([], [],color = \"red\", label = \"QA (hal)\",marker=\"^\")\n",
    "plt.scatter([], [],color = \"#ff1493\", label = \"QA (not hal)\")\n",
    "#plt.scatter([], [],color = \"#00008b\", label = \"Data2txt (hal)\",marker=\"^\")\n",
    "#plt.scatter([], [],color = \"blue\", label = \"Data2txt (not hal)\")\n",
    "#plt.scatter([], [],color = \"green\", label = \"Summary (hal)\",marker=\"^\")\n",
    "#plt.scatter([], [],color = \"#32cd32\", label = \"Summary (not hal)\")\n",
    "\n",
    "plt.title(\"UMAP Visualization of RoBERTa Embeddings\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"rag_truth_span_test_sim.json\", \"r\") as f:\n",
    "    test_data_span = json.load(f)\n",
    "with open(\"rag_truth_span_train.json\", \"r\") as f:\n",
    "    train_data_span = json.load(f)\n",
    "with open(\"rag_truth_span_dev.json\", \"r\") as f:\n",
    "    dev_data_span = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ref': 'The FBI charged a Philadelphia woman on Thursday with trying to travel overseas to fight for ISIS. She\\'s one of three women arrested this week on terror charges. Two New York women were also taken into custody. An FBI complaint cites numerous social media messages dating back to August 2013 that were sent by Keonna Thomas, 30, also known as \"Young Lioness\" and \"Fatayat Al Khilafah.\" One Twitter message said, \"If we truly knew the realities ... we all would be rushing to join our brothers in the front lines pray ALLAH accept us as shuhada [martyrs].\" Another said, \"When you\\'re a mujahid [violent jihadi fighter] your death becomes a wedding.\" The FBI said Thomas purchased an electronic visa to Turkey on March 23. Turkey is known as the easiest place from which to enter Syria and join ISIS. An ISIS manual advises recruits to buy round-trip tickets to vacation spots such as Spain and then purchase tickets for their real destination once they arrive overseas, the FBI said. On March 26, Thomas purchased a ticket to Barcelona, with a March 29 departure and an April 15 return to the United States, the complaint said. It\\'s not clear when or where she was arrested. She was charged with knowingly attempting to provide material support and resources to a designated foreign terrorist organization. She could be sentenced to 15 years in prison. On Thursday, Noelle Velentzas, 28, and her former roommate, Asia Siddiqui, 31, were arrested in New York and accused of planning to build an explosive device for attacks in the United States, federal prosecutors said. In the past 18 months, the Justice Department\\'s National Security Division has prosecuted or is prosecuting more than 30 cases of people attempting to travel abroad to join or provide support to terrorist groups. Of those cases, 18 allegedly involve support to ISIS. \"The terrorist threat is more decentralized, more diffuse, more complicated,\" Homeland Security Secretary Jeh Johnson told reporters Thursday. \"It involves the potential lone wolf actor, it involves the effective use of social media, the Internet.\"\\n',\n",
       " 'text': 'The FBI has charged a Philadelphia woman, Keonna Thomas, with trying to travel overseas to fight for ISIS. This follows the arrests of two New York women, Noelle Velentzas and Asia Siddiqui, who were accused of planning to build an explosive device for attacks in the United States. Thomas purchased an electronic visa to Turkey and a round-trip ticket to Barcelona, following the advice given in an ISIS manual for recruits. She has been charged with attempting to provide material support to a terrorist organisation and could face 15 years in prison.',\n",
       " 'labels': 0,\n",
       " 'hallucination_id': [],\n",
       " 'hallucination': {'hallucination_list': []},\n",
       " 'source': 'CNN/DM',\n",
       " 'model': 'gpt-4-0613',\n",
       " 'task_type': 'Summary',\n",
       " 'source_id': '15596',\n",
       " 'sim_before': 0.9261488318443298}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_span[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llama-2-13b-chat': 36, 'mistral-7B-instruct': 31, 'llama-2-7b-chat': 52, 'llama-2-70b-chat': 35, 'gpt-3.5-turbo-0613': 5, 'gpt-4-0613': 1}\n",
      "{'gpt-4-0613': 35, 'mistral-7B-instruct': 134, 'llama-2-7b-chat': 123, 'llama-2-13b-chat': 138, 'llama-2-70b-chat': 112, 'gpt-3.5-turbo-0613': 37}\n",
      "{'llama-2-13b-chat': 33, 'llama-2-70b-chat': 24, 'mistral-7B-instruct': 86, 'gpt-3.5-turbo-0613': 4, 'llama-2-7b-chat': 51, 'gpt-4-0613': 6}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "hal_model_qa = defaultdict(lambda:0)\n",
    "hal_model_d2t = defaultdict(lambda:0)\n",
    "hal_model_sum = defaultdict(lambda:0)\n",
    "\n",
    "for d in test_data_span:\n",
    "    if d[\"hallucination_id\"] == []:\n",
    "        continue\n",
    "        \n",
    "    if d[\"task_type\"] == \"QA\":\n",
    "        hal_model_qa[d[\"model\"]] += 1\n",
    "    elif d[\"task_type\"] == \"Data2txt\":\n",
    "        hal_model_d2t[d[\"model\"]] += 1\n",
    "    else:\n",
    "        hal_model_sum[d[\"model\"]] += 1\n",
    "print(dict(hal_model_qa))\n",
    "print(dict(hal_model_d2t))\n",
    "print(dict(hal_model_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llama-2-13b-chat': 35, 'llama-2-7b-chat': 51, 'llama-2-70b-chat': 29, 'mistral-7B-instruct': 24, 'gpt-3.5-turbo-0613': 4}\n",
      "{'llama-2-70b-chat': 80, 'llama-2-13b-chat': 132, 'mistral-7B-instruct': 121, 'llama-2-7b-chat': 101, 'gpt-4-0613': 1}\n",
      "{'llama-2-13b-chat': 20, 'mistral-7B-instruct': 39, 'llama-2-7b-chat': 34, 'llama-2-70b-chat': 11}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "hal_model_qa_imp = defaultdict(lambda:0)\n",
    "hal_model_d2t_imp = defaultdict(lambda:0)\n",
    "hal_model_sum_imp = defaultdict(lambda:0)\n",
    "\n",
    "for i in id_011 + id_111:\n",
    "    d = test_data_span[i]\n",
    "    if d[\"hallucination_id\"] == []:\n",
    "        continue\n",
    "        \n",
    "    if d[\"task_type\"] == \"QA\":\n",
    "        hal_model_qa_imp[d[\"model\"]] += 1\n",
    "    elif d[\"task_type\"] == \"Data2txt\":\n",
    "        hal_model_d2t_imp[d[\"model\"]] += 1\n",
    "    else:\n",
    "        hal_model_sum_imp[d[\"model\"]] += 1\n",
    "print(dict(hal_model_qa_imp))\n",
    "print(dict(hal_model_d2t_imp))\n",
    "print(dict(hal_model_sum_imp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 類似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers==4.44.0\n",
    "%pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/RoBERTa-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from models_rob import NoRagModel, RagModel, TripletModel\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "\n",
    "name = \"./0102_no_rag_rob\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/RoBERTa-base\")\n",
    "base_model = AutoModel.from_pretrained(\"FacebookAI/RoBERTa-base\")\n",
    "model = NoRagModel.from_pretrained(base_model,name)\n",
    "\n",
    "def calc_sim(ref, text):\n",
    "    # テキストをトークン化\n",
    "    ref_tokens = tokenizer(ref, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    text_tokens = tokenizer(text, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # モデルの出力を取得（最終隠れ層）\n",
    "    with torch.no_grad():  # 推論時には勾配計算を行わない\n",
    "        ref_output = model(**ref_tokens)\n",
    "        text_output = model(**text_tokens)\n",
    "    #print(ref_output)\n",
    "    # 全トークンを平均化して文のベクトル表現を得る\n",
    "    #ref_embedding = ref_output.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    #text_embedding = text_output.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    ref_embedding = ref_output.output.cpu().numpy()\n",
    "    text_embedding = text_output.output.cpu().numpy()\n",
    "    #print(ref_embedding)\n",
    "\n",
    "    # コサイン類似度を計算\n",
    "    return cosine_similarity(ref_embedding, text_embedding)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/RoBERTa-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from models_rob import NoRagModel, RagModel, TripletModel\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "\n",
    "name = \"./0102_rag_rob\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/RoBERTa-base\")\n",
    "base_model = AutoModel.from_pretrained(\"FacebookAI/RoBERTa-base\")\n",
    "model = RagModel.from_pretrained(base_model,name)\n",
    "\n",
    "def calc_sim(ref, text):\n",
    "    # テキストをトークン化\n",
    "    ref_tok = tokenizer(ref, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    text_tok = tokenizer(text, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    inputs = {\n",
    "    \"input_ids\":[ref_tok[\"input_ids\"],text_tok[\"input_ids\"]],\n",
    "    \"attention_mask\":[ref_tok[\"attention_mask\"],text_tok[\"attention_mask\"]]\n",
    "    }   \n",
    "    \n",
    "    # モデルの出力を取得（最終隠れ層）\n",
    "    with torch.no_grad():  # 推論時には勾配計算を行わない\n",
    "        output = model(**inputs)\n",
    "    #print(ref_output)\n",
    "    ref_output = output.output[0].cpu().numpy()\n",
    "    text_output = output.output[1].cpu().numpy()\n",
    "    #print(ref_embedding)\n",
    "\n",
    "    # コサイン類似度を計算\n",
    "    return cosine_similarity(ref_output, text_output)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def triplet_loss(anchor_output, positive_output, negative_output, positive_logits, negative_logits):\n",
    "    # ラベル予測の損失計算\n",
    "    positive_targets = torch.zeros(positive_output.size(0), dtype=torch.long) # ラベル1 (positive)\n",
    "    negative_targets = torch.ones(negative_output.size(0), dtype=torch.long)\n",
    "    positive_loss = nn.CrossEntropyLoss()(positive_logits, positive_targets)\n",
    "    negative_loss = nn.CrossEntropyLoss()(negative_logits, negative_targets)\n",
    "\n",
    "    # ラベル予測の損失を平均\n",
    "    classification_loss = (positive_loss + negative_loss) / 2.0\n",
    "\n",
    "    # トリプレット損失の計算\n",
    "    #triplet_loss = nn.TripletMarginLoss(margin=1, p=2)(anchor_output, positive_output, negative_output)\n",
    "    triplet_loss_fn = (nn.TripletMarginWithDistanceLoss(margin=1.0,distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)))\n",
    "    triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "    # 最終損失\n",
    "    total_loss = classification_loss + triplet_loss \n",
    "\n",
    "    return classification_loss, triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/RoBERTa-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from models_rob import NoRagModel, RagModel, TripletModel\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "\n",
    "name = \"./0102_triplet_rob\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/RoBERTa-base\")\n",
    "base_model = AutoModel.from_pretrained(\"FacebookAI/RoBERTa-base\")\n",
    "model = TripletModel.from_pretrained(base_model,triplet_loss, name)\n",
    "\n",
    "def calc_sim(ref, text,label):\n",
    "    # テキストをトークン化\n",
    "    ref_tok = tokenizer(ref, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    text_tok = tokenizer(text, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    sample_tok = tokenizer(\"sample\", padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    if label[0] == 1:\n",
    "        inputs = {\n",
    "        \"input_ids\":[ref_tok[\"input_ids\"],sample_tok[\"input_ids\"],text_tok[\"input_ids\"]],\n",
    "        \"attention_mask\":[ref_tok[\"attention_mask\"],sample_tok[\"attention_mask\"],text_tok[\"attention_mask\"]],\n",
    "        \"labels\": label\n",
    "        }   \n",
    "    elif label[0] == 0:\n",
    "        inputs = {\n",
    "        \"input_ids\":[ref_tok[\"input_ids\"],text_tok[\"input_ids\"], sample_tok[\"input_ids\"]],\n",
    "        \"attention_mask\":[ref_tok[\"attention_mask\"],text_tok[\"attention_mask\"], sample_tok[\"attention_mask\"]],\n",
    "        \"labels\": label\n",
    "        }\n",
    "    \n",
    "    # モデルの出力を取得（最終隠れ層）\n",
    "    with torch.no_grad():  # 推論時には勾配計算を行わない\n",
    "        output = model(**inputs)\n",
    "    #print(ref_output)\n",
    "    ref_output = output.output[0].cpu().numpy()\n",
    "    text_output = output.output[1].cpu().numpy()\n",
    "    #print(ref_embedding)\n",
    "\n",
    "    # コサイン類似度を計算\n",
    "    return cosine_similarity(ref_output, text_output)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8732698"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_sim(test_data_span[0][\"ref\"], test_data_span[0][\"text\"], [test_data_span[0][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2700/2700 [14:59<00:00,  3.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA -0.026896995\n",
      "Data2txt -0.08624694\n",
      "Sum 0.55979466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "qa_cos = []\n",
    "d2t_cos = []\n",
    "sum_cos = []\n",
    "\n",
    "for d in tqdm(test_data_span):\n",
    "    #if d[\"labels\"] ==0:\n",
    "    #    continue\n",
    "    sim = calc_sim(d[\"ref\"], d[\"text\"], [d[\"labels\"]])\n",
    "    if d[\"task_type\"] == \"QA\":\n",
    "        qa_cos.append(sim)\n",
    "        #d[\"sim_before\"] = sim\n",
    "        #d[\"sim_no_doc\"] = sim\n",
    "        #d[\"sim_doc\"] = sim\n",
    "        d[\"triplet\"] = sim\n",
    "    elif d[\"task_type\"] == \"Data2txt\":\n",
    "        d2t_cos.append(sim)\n",
    "        #d[\"sim_before\"] = sim\n",
    "        #d[\"sim_no_doc\"] = sim\n",
    "        #d[\"sim_doc\"] = sim\n",
    "        d[\"triplet\"] = sim\n",
    "    else:\n",
    "        sum_cos.append(sim)\n",
    "        #d[\"sim_before\"] = sim\n",
    "        #d[\"sim_no_doc\"] = sim\n",
    "        #d[\"sim_doc\"] = sim\n",
    "        d[\"triplet\"] = sim\n",
    "\n",
    "print(\"QA\", np.mean(qa_cos))\n",
    "print(\"Data2txt\", np.mean(d2t_cos))\n",
    "print(\"Sum\", np.mean(sum_cos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ref': 'Blue Bell ice cream has temporarily shut down one of its manufacturing plants over the discovery of listeria contamination in a serving of ice cream originating from that plant. Public health officials warned consumers Friday not to eat any Blue Bell-branded products made at the company\\'s Broken Arrow, Oklahoma, plant. That includes 3-ounce servings of Blue Bell ice cream from this plant that went to institutions in containers marked with the letters O, P, Q, R, S or T behind the coding date. The warning by the Centers for Disease Control and Prevention does not affect other Blue Bell ice cream, including other 3-ounce servings, not made at the plant. But Blue Bell has recalled other products. The company is shutting down the Broken Arrow facility \"out of an abundance of caution\" to search for a possible cause of contamination. It is the third time Blue Bell has taken action in light of a listeria outbreak at a Kansas hospital that served the company\\'s ice cream. Listeria monocytogenes was recently found in a cup of ice cream recovered from the hospital. The cup contaminated with the bacteria was produced at the Broken Arrow plant in April 2014, Blue Bell said. And, according to the CDC, listeria bacteria was found in additional samples of the same product that were recovered from the plant. The bacteria in the hospital sample and the factory sample appeared to match each other genetically, the CDC said. But they did not appear identical to listeria samples taken from patients infected in the Kansas outbreak. In a separate outbreak in Texas, the CDC did find that listeria samples taken from patients who came down with listeriosis between 2010 and 2014 in a hospital that served 3-ounce Blue Bell cups matched the listeria in recovered samples. None of this means the ice cream is the source of either spate of the infections. \"Investigation to determine whether these illnesses are related to exposure to Blue Bell products is ongoing,\" the CDC said. In early March, in light of the Kansas listeria outbreak, Blue Bell recalled a group of products made at a plant in Texas. It later added 3-ounce cup servings to the recall. Five people were infected and three died in the past year in Kansas from listeria that might be linked to Blue Bell Creameries products, according to the CDC. All five of them were hospitalized at the same hospital before developing listeriosis, the CDC said. At least four of them had consumed milkshakes made with Blue Bell ice cream before developing the infection. \"We are devastated and know that Blue Bell has to be and can be better than this,\" Paul Kruse, Blue Bell CEO and president, said in a statement. \"Quality and safety have always been our top priorities. We are deeply saddened and concerned for all those who have been affected.\" The CDC advises that individuals and institutions should check their freezers for the recalled products and throw them away. In a statement on its website, Blue Bell said \"this recall in no way includes Blue Bell ice cream half gallons, pints, quarts, 3 gallons or other 3 oz. cups.\" This has been the first product recall in the 108-year history of Blue Bell Creameries, the company said. Listeriosis is a serious infection caused by eating food contaminated with listeria, and primarily affects the elderly, pregnant women, newborns and people with weakened immune systems, according to the CDC. Symptoms of a listeria infection are fever and muscle aches, sometimes associated with diarrhea or other gastrointestinal symptoms. In the United States, an estimated 1,600 people become seriously ill each year, and approximately 16% of these illnesses result in death. Cervical infections caused by listeriosis in pregnant women may result in stillbirth or spontaneous abortion during the second or third trimesters. CNN\\'s Debra Goldschmidt, Amanda Watts and Jacque Wilson contributed to this report.\\n',\n",
       " 'text': 'Blue Bell ice cream has temporarily shut down one of its plants after listeria contamination was discovered in a serving of ice cream. Public health officials advised consumers not to eat any Blue Bell products made at the plant in Oklahoma. The recall includes 3-ounce servings of ice cream with certain coding dates. This is the third time Blue Bell has taken action due to listeria contamination, and the company is cooperating with investigations. No illnesses have been reported directly linked to the contaminated ice cream, but five people in Kansas have died from listeriosis in the past year after consuming Blue Bell products.',\n",
       " 'labels': 1,\n",
       " 'hallucination_id': [{'start': 535,\n",
       "   'end': 566,\n",
       "   'text': 'five people in Kansas have died',\n",
       "   'meta': 'EVIDENT CONFLICT:\\nOriginal:  Five people were infected and \"three\" died in the past year in Kansas from listeria\\nAIGC: \"five\" people in Kansas have died\\n',\n",
       "   'label_type': 'Evident Conflict',\n",
       "   'implicit_true': False,\n",
       "   'due_to_null': False}],\n",
       " 'hallucination': {'hallucination_list': ['five people in Kansas have died']},\n",
       " 'source': 'CNN/DM',\n",
       " 'model': 'llama-2-13b-chat',\n",
       " 'task_type': 'Summary',\n",
       " 'source_id': '15599',\n",
       " 'sim_before': 0.936119556427002,\n",
       " 'sim_no_doc': 0.75806874,\n",
       " 'sim_doc': 0.69217885,\n",
       " 'triplet': 0.75832367}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_span[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# JSON 書き込み\n",
    "#with open(\"rag_truth_span_test_sim.json\", \"w\") as f:\n",
    "#    json.dump(test_data_span, f, cls=NumpyEncoder, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rag_truth_span_test_sim.json\", \"r\") as f:\n",
    "    test_data_span = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "before_posi = []\n",
    "before_nega = []\n",
    "\n",
    "id_all = id_000 + id_001 + id_010 + id_011 + id_100 + id_101 + id_110 + id_111\n",
    "id_tri_fail = id_001 + id_010 + id_101 + id_110\n",
    "id_tri_suc = id_000 + id_011 + id_100 + id_111\n",
    "for i in id_all :\n",
    "    d = test_data_span[i]\n",
    "    if d[\"task_type\"] !=\"QA\":\n",
    "        continue\n",
    "    mode = \"triplet\"\n",
    "    if d[\"labels\"] == 1:\n",
    "        before_nega.append(d[mode])\n",
    "    else:\n",
    "        before_posi.append(d[mode])\n",
    "        \n",
    "plt.hist(before_posi, bins=100, alpha=0.5, label=\"Positive\")\n",
    "plt.hist(before_nega, bins=100, alpha=0.5, label=\"Negative\")\n",
    "plt.xlabel(\"Cosine sim\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプリング\n",
    "\n",
    "import json\n",
    "\n",
    "for i in id_001 + id_101:\n",
    "    d = test_data_span[i]\n",
    "    if d[\"triplet\"] > 0.90 and d[\"task_type\"] == \"Summary\":\n",
    "        print(json.dumps(d, indent=4, ensure_ascii=False,cls=NumpyEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_all = id_000 + id_001 + id_010 + id_011 + id_100 + id_101 + id_110 + id_111\n",
    "id_tri_fail = id_001 + id_010 + id_101 + id_110\n",
    "id_tri_suc = id_000 + id_011 + id_100 + id_111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ref\": \"'Bold and the Beautiful' Spoilers: Luna Leaks Eric's Secret To Ridge, RJ Betrayed By His New Love?\\nThe Bold and the Beautiful (B&B) spoilers tease that Luna, the intern (Lisa Yamada), will reveal Eric Forrester's (John McCook) diagnosis to Ridge Forrester (Thorsten Kaye). She Knows Soaps reports that Luna wants to help RJ Forrester (Joshua Hoffman) and take the burden of this heavy secret from his shoulders.\\nOn Friday's episode, B&B revealed that Luna is Li Flannigan's (Naomi Matsuda) niece. Her mother warned her to stay away from the Forresters. She appeared to want to protect her daughter from heartbreak. She could be a long-lost Forrester somehow. However, we have yet to learn how.\\nNow, back to RJ. Lune worries that the pressure of keeping Eric's secret is too heavy for her new friend. She urges him to tell his father because Eric's health condition seems more severe than he initially said.\\nThe Bold and the Beautiful spoilers say that RJ knows he should tell his dad but worries about Eric feeling betrayed. He wants to wait until after the fashion show. All his granddad wants is one more line. He plans on giving that to him.\\nLuna decides to go behind RJ's back and tells Ridge about Eric's devastating diagnosis. She reveals that RJ knew about Eric's condition and hesitated to tell him about it.\\nRidge will blow up at RJ about keeping something so important from him. RJ will feel betrayed by Luna, but his dad urges him to forgive his friend for doing the right thing.\\nB&B fans, will RJ forgive Luna for telling Ridge? What is Luna's motive for telling Eric's secret? Keep watching The Bold and the Beautiful, airing weekdays on CBS and streaming on Paramount Plus. Please share your thoughts with me in the comments section below. Remember to follow me to read more of my content.\",\n",
      "    \"text\": \"Here is a summary of the article in 78 words or less:\\n\\nLuna reveals Eric's secret to Ridge, causing tension between RJ and Luna. Luna wants to help RJ but goes behind his back, leading to betrayal and drama. Fans wonder if RJ will forgive Luna and what Luna's true motives are.\",\n",
      "    \"labels\": 0,\n",
      "    \"hallucination_id\": [],\n",
      "    \"hallucination\": {\n",
      "        \"hallucination_list\": []\n",
      "    },\n",
      "    \"source\": \"Recent News\",\n",
      "    \"model\": \"llama-2-7b-chat\",\n",
      "    \"task_type\": \"Summary\",\n",
      "    \"source_id\": \"13535\",\n",
      "    \"sim_before\": 0.8932280540466309,\n",
      "    \"sim_no_doc\": 0.787675678730011,\n",
      "    \"sim_doc\": 0.7203843593597412,\n",
      "    \"triplet\": 0.17335960268974304\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for i in id_all:\n",
    "    d = test_data_span[i]\n",
    "    if d[\"task_type\"] ==\"Summary\" and d[\"source\"]!=\"CNN/DM\" and len(d[\"text\"])<300:\n",
    "        print(json.dumps(d, indent=4, ensure_ascii=False,cls=NumpyEncoder))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample3 = []\n",
    "for i in id_all:\n",
    "    if test_data_span[i][\"task_type\"]!=\"Summary\":\n",
    "        continue\n",
    "    \n",
    "    hal_type2 = []\n",
    "    for hal_id in test_data_span[i][\"hallucination_id\"]:\n",
    "        hal_type2.append(hal_id[\"label_type\"])\n",
    "    \n",
    "    if \"Subtle Conflict\" in hal_type2:\n",
    "        sample3.append(test_data_span[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ref': 'Their relationship led to jail time for her, but Mary Kay Letourneau Fualaau wants the world to know that she and her husband, Vili Fualaau, are still happily together. She was a married 34-year-old teacher and mother of four in Seattle in 1996 when she began an affair with Fualaau, her 13-year-old student. Letourneau gave birth to her young lover\\'s child and went on to serve more than seven years in prison on charges related to their sexual relationship. On Friday, ABC\\'s \"20/20\" will air an exclusive interview with the couple, who will celebrate their 10th wedding anniversary on May 20. The pair wed soon after she was released from prison in 2005 and are now the parents of two teen girls. According to ABC, during the interview with Barbara Walters, \"Mary Kay tells Walters what makes their marriage work in spite of their huge difference in their age.\" \"She also discusses her surprising plans to teach again, as well as her intentions to have her status as a registered sex offender lifted,\" a posting on the network\\'s site says. \"Vili Fualaau, meanwhile, discusses his bouts with alcoholism, depression and why he believes the system failed him while he was still a minor.\" Letourneau Fualaau is now 53, and her husband is 31.\\n',\n",
       "  'text': 'Mary Kay Letourneau Fualaau, who gained infamy for her relationship with her 13-year-old student, is still happily married to Vili Fualaau. After serving over seven years in prison, they have been married for 10 years and have two teenage daughters. In an upcoming interview with Barbara Walters, Letourneau Fualaau discusses their marriage, her plans to teach again, and her desire to have her status as a registered sex offender lifted. Fualaau talks about his struggles with alcoholism, depression, and feeling failed by the system.',\n",
       "  'labels': 1,\n",
       "  'hallucination_id': [{'start': 372,\n",
       "    'end': 382,\n",
       "    'text': 'her desire',\n",
       "    'meta': 'SUBTLE CONFLICT\\nOriginal: her intentions\\nGenerated: her desire',\n",
       "    'label_type': 'Subtle Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False}],\n",
       "  'hallucination': {'hallucination_list': ['her desire']},\n",
       "  'source': 'CNN/DM',\n",
       "  'model': 'gpt-3.5-turbo-0613',\n",
       "  'task_type': 'Summary',\n",
       "  'source_id': '15613',\n",
       "  'sim_before': 0.9606919884681702,\n",
       "  'sim_no_doc': 0.8066741228103638,\n",
       "  'sim_doc': 0.865234911441803,\n",
       "  'triplet': 0.7991065382957458},\n",
       " {'ref': 'Tejano star Selena, who died 20 years ago, is coming back in a big way: with a hologram-like figure. Billboard reports that the singer\\'s family is creating a version of the singer that will be \"walking, talking, singing and dancing digital embodiment\" of her persona. \"By no means is this something that\\'s creepy or weird,\" her sister, Suzette Quintanilla, told Billboard. \"We think it\\'s something amazing. A lot of the new fans that did not get to experience what Selena was about hopefully will be able to get a sense of her with this new technology that\\'s going to be coming out.\" Selena: 20 years after her death. The technology is being handled by Acrovirt LLC, a Nevada-based tech company. \"Using detailed individual personalized functions spanning the mind, brain and body, the individual\\'s Digitized Human Essence will autonomously learn and react on behalf of its human counterpart\\'s,\" the company explained. The project is being called \"Selena the One.\" Twenty years after she was killed by her fan club president, Selena remains incredibly popular, with her Facebook page recording 2 million likes and fans continuing to post videos and tributes. Selena will be the first figure to use the Acrovirt technology, Quintanilla said. \"I\\'m excited at the fact that she will be the first ever, and the fact that she\\'s a Latina makes it even more awesome,\" she said. \"It\\'s not about replacing Selena in any shape, way or form; it\\'s just something to help her legacy continue growing.\" The family intends to expand her legacy in another way: with some new music. Selena the One \"will release new songs and videos, will collaborate with current hit artists, and aims to go on tour in 2018,\" said a statement on Selena\\'s Facebook page. Selena isn\\'t the first performer to try the virtual route. A Michael Jackson hologram appeared at the Billboard Music Awards in 2014, and a hologram of Tupac Shakur performed at Coachella in 2012. But the new technology is a step forward, Quintanilla said. \"People don\\'t realize how fast technology is moving,\" she told Billboard. \"This is something that we\\'re building for another two to three years, so when 2018 comes around they\\'ll be like, \\'Oh, OK, we get it.\\' \" Fans can join an Indiegogo campaign, www.selenatheone.com, to support the launch. The campaign, which hopes to raise $500,000, begins April 16. The commemorative Fiesta de la Flor in Corpus Christi, Texas -- which celebrates her life -- is scheduled for April 17 and 18. CNN\\'s Katia Hetter contributed to this story.\\n',\n",
       "  'text': 'Selena, the late Tejano star, will be resurrected as a hologram-like figure using advanced technology. Her family plans to release new music and potentially go on tour in 2018. The technology, developed by Acrovirt LLC, allows for a digital embodiment of the singer that will walk, talk, sing, and dance. This is not the first time a deceased artist has been brought back in a virtual format, but the new technology is considered a step forward. Fans can support the launch through an Indiegogo campaign starting on April 16, and a commemorative festival will take place in Corpus Christi, Texas on April 17 and 18.',\n",
       "  'labels': 1,\n",
       "  'hallucination_id': [{'start': 377,\n",
       "    'end': 391,\n",
       "    'text': 'virtual format',\n",
       "    'meta': 'SUBTLE CONFLICT\\nOriginal: virtual route\\nGenerated: virtual format',\n",
       "    'label_type': 'Subtle Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False}],\n",
       "  'hallucination': {'hallucination_list': ['virtual format']},\n",
       "  'source': 'CNN/DM',\n",
       "  'model': 'llama-2-13b-chat',\n",
       "  'task_type': 'Summary',\n",
       "  'source_id': '15627',\n",
       "  'sim_before': 0.9530389308929443,\n",
       "  'sim_no_doc': 0.7736385464668274,\n",
       "  'sim_doc': 0.803662896156311,\n",
       "  'triplet': 0.8440068960189819},\n",
       " {'ref': 'Duhok, Iraqi Kurdistan (CNN)The children laugh and shriek, as some of them seem to always have the capacity to do no matter how depressing the circumstances. Their bright clothes provide splashes of color against the otherwise drab monotone white of the endless rows of tents. A small group plays with rocks, replacements for the toys they left behind when they fled, while others clamber through a jagged tear in the wire fence surrounding the refugee camp. The Shariya refugee camp opened around six months ago, made up of some 4,000 tents and counting. Thousands of Yazidis now call this corner of Iraqi Kurdistan home, about 18 miles (30 kilometers) from one of the front lines with ISIS, where one can hear the occasional reverberation in the distance of what we are told are airstrikes. The vast majority of the camp\\'s occupants are from the town of Sinjar, which is near the border with Syrian Kurdistan, and fled the ISIS assault there back in August. But not everyone escaped. ISIS took thousands of Yazidis captive. The fighters separated the young women and girls, some as young as 8 years old, to be sold as slaves, for their \"masters\" to use as concubines. Men faced a choice: Convert to Islam or be shot. Mahmoud was out running errands when ISIS fighters arrived, taking his wife, Ahlam, their three children -- the youngest of which was just a month old -- and his elderly parents. \"They took our phones, jewelry, money,\" Ahlam recalls. \"They had guns. They forced us at gunpoint into big trailer trucks.\" They were taken to a school turned prison in Tal Afar. From there, the family was moved from village to village -- and at one stage taken to Mosul. \"They wrote everyone\\'s name down and they asked where we want to work, in the fields, as cleaners or as herders,\" she says. Ahlam and her family chose to herd goats. They were then taken to a Shia village whose residents had fled, where they were part of a group of around 40 living in one house. In the home, Ahlam found a cell phone left behind by its former occupants and called her husband. \"I said we are alive but we are prisoners.\" Ahlam\\'s husband, who up until that moment had lost the will to live, thinking his family was dead, says he cried out of happiness despite his pain. Ahlam would call when she could, briefly, after midnight, hiding under her bedcovers. If she was caught with a phone, she would be killed. The village itself was a massive prison, its entrances guarded by ISIS fighters. She recalls that two men, in their late 40s or 50s, tried to escape. When they were caught, their bones were broken, their bodies tied to the back of a truck and then driven through the streets. The Yazidi captives were forced to watch the gruesome spectacle. The men\\'s corpses were then tossed into a ditch and an order given not to bury them. One night, some of the Yazidi men risked their lives to toss dirt onto the bodies, to give those slain what dignity they could. Ahlam tells us that about a week before we met, ISIS fighters came by and took away her in-laws and the other elderly people living in the house. \"We didn\\'t know where they were taking them, we thought we would be next,\" she remembers. So she and the rest of the group realized that they had to try to flee. \"We decided that either we survive or we don\\'t.\" They left at midnight. Ahlam cradled the baby, as her two other children, ages 3 and 4 years old, clutched at her clothes. She prayed the baby wouldn\\'t cry, that the children could keep walking. They knew the general direction to take, but not the exact route, and they could only hope it was toward freedom. \"When the sun started to come up, I thought that\\'s it, we are going to get caught,\" Ahlam says. \"And what am I going to do with the kids?  I can\\'t carry all three of them and run.\" Luckily, Ahlam never had to answer that impossible question. The group made it into Iraqi Kurdistan. The couple can\\'t put into words their emotions when they were reunited. Mahmoud, gently caressing his daughter\\'s palm, says he could hardly believe that the woman whose stunning eyes and gentle words he had fallen in love with, and their three children, were by his side again. It had been eight agonizing months. But their joy was tainted by fear for Mahmoud\\'s parents. A few days after Mahmoud and Ahlam were reunited, ISIS released 217 captives. No one is disclosing exactly why. Among them were 60 children, a handful of men and women, and the rest were elderly -- including Mahmoud\\'s parents. \"We didn\\'t know if they were going to slaughter us or what they were going to do with us,\" Mahmoud\\'s father says. \"They moved us around a lot, and at one stage we stayed in in one place for three days.\" But Ahlam\\'s parents are still with ISIS. Vian Dakhil, a Yazidi member of Iraq\\'s parliament, speculates that the Yazidis ISIS released are individuals they were struggling to care for. Dakhil is part of Iraqi Prime Minister Haider al-Abadi\\'s delegation to the United States this week. She will be addressing the United Nations -- pleading for more international support for Iraq and more airstrikes to help defeat ISIS -- and she will address the plight of the Yazidis. \"We sometimes say that we wish we had been massacred. This would be better than being kidnapped and raped. We prefer death now over the fate of what is happening to these girls and women,\" Dakhil says. \"It is now on the government and on the international community to focus on this. How to get these captives back. It\\'s inconceivable that in the 21st century, something like this is happening as if we were living in the Stone Age.\" Ahlam says she was spared because she was breastfeeding and she had young children, which, we\\'re told, makes her impure and therefore unable to be used as a sex slave. For Ahlam, what she went through was not the hardest part of her ordeal. It\\'s the moment when the ISIS fighters began taking away the girls and young women. She\\'s haunted by their screams, the image of them being dragged away sobbing and screaming.\\n',\n",
       "  'text': \"The Shariya refugee camp in Iraqi Kurdistan is home to thousands of Yazidis who fled ISIS attacks. Ahlam, a Yazidi woman, and her family were captured by ISIS and held captive for eight months. They were separated, with Ahlam and her children being taken to one location and her husband being taken to another. Ahlam was forced to work as a cleaner and was sexually harassed by her captors. She managed to escape with her children by pretending to be a Muslim and hiding her Yazidi identity. They eventually made it to the Shariya refugee camp, where they were reunited with Mahmoud, Ahlam's husband. However, Ahlam's parents are still being held captive by ISIS. Vian Dakhil, a Yazidi member of Iraq's parliament, is calling for more international support to defeat ISIS and free the remaining captives.\",\n",
       "  'labels': 1,\n",
       "  'hallucination_id': [{'start': 275,\n",
       "    'end': 309,\n",
       "    'text': 'her husband being taken to another',\n",
       "    'meta': 'SUBTLE CONFLICT\\nAIGC: her husband being taken to another\\nOriginal: Mahmoud was out running errands when ISIS fighters arrived.. (not captured)',\n",
       "    'label_type': 'Subtle Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False},\n",
       "   {'start': 311,\n",
       "    'end': 348,\n",
       "    'text': 'Ahlam was forced to work as a cleaner',\n",
       "    'meta': 'EVIDENT CONFLICT\\nAIGC: Ahlam was forced to work as a cleaner\\nOriginal: Ahlam and her family chose to herd goats\\n',\n",
       "    'label_type': 'Evident Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False},\n",
       "   {'start': 353,\n",
       "    'end': 389,\n",
       "    'text': 'was sexually harassed by her captors',\n",
       "    'meta': 'EVIDENT CONFLICT\\nAIGC: was sexually harassed by her captors\\nOriginal: Ahlam says she was spared…unable to be used as a sex slave',\n",
       "    'label_type': 'Evident Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False},\n",
       "   {'start': 391,\n",
       "    'end': 491,\n",
       "    'text': 'She managed to escape with her children by pretending to be a Muslim and hiding her Yazidi identity.',\n",
       "    'meta': 'HIGH INTRO OF NEW INFO\\nThis information that says she managed to escape with her children by pretending to be a Muslim and hiding her Yazidi identity was not mentioned in the passage',\n",
       "    'label_type': 'Evident Baseless Info',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False},\n",
       "   {'start': 492,\n",
       "    'end': 569,\n",
       "    'text': 'They eventually made it to the Shariya refugee camp, where they were reunited',\n",
       "    'meta': '\\nEVIDENT CONFLICT\\nAIGC: they made it to the “Shariya refugee camp”, where they were reunited\\nOriginal: . The group made it into “Iraqi Kurdistan”',\n",
       "    'label_type': 'Evident Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False}],\n",
       "  'hallucination': {'hallucination_list': ['her husband being taken to another',\n",
       "    'Ahlam was forced to work as a cleaner',\n",
       "    'was sexually harassed by her captors',\n",
       "    'She managed to escape with her children by pretending to be a Muslim and hiding her Yazidi identity.',\n",
       "    'They eventually made it to the Shariya refugee camp, where they were reunited']},\n",
       "  'source': 'CNN/DM',\n",
       "  'model': 'llama-2-70b-chat',\n",
       "  'task_type': 'Summary',\n",
       "  'source_id': '15778',\n",
       "  'sim_before': 0.9495251178741455,\n",
       "  'sim_no_doc': 0.7378888130187988,\n",
       "  'sim_doc': 0.798792839050293,\n",
       "  'triplet': 0.8966684937477112},\n",
       " {'ref': 'Mountaineers have returned to Mount Everest for this year\\'s climbing season, resuming the quest to summit the world\\'s highest peak after a deadly season last year. In 2014, the Nepal climbing season ended after a piece of glacial ice fell, unleashing an avalanche that killed 16 Nepalis who had just finished their morning prayers. The April 18 accident was the single deadliest incident to ever occur on Mount Everest. The deaths launched fierce debates about the enormous risks faced by the Sherpas and the dangers of climbing Everest. In order to reduce risks, the route through Khumbu Icefall, the notoriously treacherous path where the 16 were killed, has been changed to one that takes longer but is expected to be safer. \"They\\'re going in the icefall and, as we found out on April 18, it\\'s the most dangerous place,\" said Conrad Anker, a veteran climber who has been to Everest three times. \"They\\'re exposed to the tumbling ice, hanging seracs above it. It\\'s very, very dangerous. It\\'s the most dangerous place I\\'ve been in the mountains.\" At this point in the season, climbing teams have not yet entered Khumbu Icefall, which is essentially a frozen river rapid with jagged pieces breaking off and moving. Nepal has issued 347 permits this year to climb Mount Everest, with 125 of them from the previously shortened season, according to the Nepal Ministry of Tourism.  It\\'s a slight increase from the 334 who were given permission last year. The local Nepalese committee that determines the path up Everest announced in February that a different route had been selected. The climbers will now take a central route through the Khumbu Icefall, avoiding the area where the deaths occurred. The committee comprised of Sherpas voted to return to the central route for safety reasons. \"There will be little risk of avalanche than in the right or left,\" said Yangji Doma Sherpa, the spokeswoman for the Sagarmatha Pollution Control Committee. The central route had been used in the 1990s, but was abandoned in favor of a quicker route, she said. The new path means climbers will have to cross more crevasses, and use more vertical and horizontal ladders. The committee issued a recommendation that the weight of workers\\' gear be limited to avoid overloading the ladders. \"I think it will be an hour longer on the icefall,\" said Alan Arnette, who is blogging from Everest base camp this season. \"I don\\'t think it will be game changer.\" But one company, Alpenglow Expeditions, said it would stop climbing from the Nepal side, where the climbers have to go through the icefall, in favor of the northern route from China. \"We\\'ve seen it get progressively more dangerous over the last few years,\" said  Adrian Ballinger, the company\\'s founder and CEO. \"We believe the risk is too great for our workers.\" According to the China Tibet Mountaineering Association, 320 people have been registered to climb the northern route to Everest this year. That\\'s 136 more than last year. The Chinese side of Everest has typically been less popular than its Nepal counterpart, because of concerns of government closures. Some Everest observers say the northern route has harsher weather and more rocky terrain, but it also doesn\\'t have an icefall. The increasing popularity of the northern route has caused concern amongst Nepali companies that climbers will divert to the Chinese side. \"I can already see the shift with mountaineers I speak to,\" said Dawa Steven Sherpa, who is based in Nepal. \"More people are going to go to Tibet than Nepal. Nepal needs the tourism far more than China does. China has incredible wealth of resources and Nepal does not.\" Leading expeditions is how Sherpas feed their families and send their children to school. Nepal depends heavily on tourism dollars. Many of the guides had to bury their friends after the accident last year, and while they may be ready to return to the summit, their families are not. Many of them are \"leaving behind nervous, stressed-out wives and children,\" whose memories of what happened last year are fresh, said Dawa Sherpa, managing director of Asian Trekking. \"They do say they don\\'t want to put them through that again,\" he said. \"They\\'re not fearful for their own lives, it\\'s what they\\'re putting their family through.\" Several mountaineers are also returning this year. One of them is Jon Reiter, who spoke to CNN last year after the tragedy. When the icy avalanche thundered down, Reiter was shoved behind an ice block by his Sherpa guide. Reiter, who is making his way to base camp this year, could not be reached directly. But he explained why he\\'s heading back to Everest this year on his blog. \"I can\\'t quite find the words to tell you why, or what really pulls me back to the mountains,\" he wrote. \"When we were in the midst of last year\\'s events it was hard to see the big picture. It was hard to remember that people die in the mountains but that it\\'s more rare than not. \"It was hard for me to remember that I\\'m not choosing between my life at home and dying in the mountains. I like to think it\\'s similar to surviving a plane crash or a major pile up on the freeway.\" CNN\\'s Sugam Pokharel contributed to this report.\\n',\n",
       "  'text': \"Climbers have returned to Mount Everest for this year's climbing season, after a deadly avalanche in 2014, that killed 16 Sherpas, resulted in the closure of the climbing season. Following the disaster, there were debates about the risks faced by Sherpas and the danger of climbing Everest. The route through Khumbu Icefall, where the avalanche occurred, has been changed to a safer, but longer, route than the one used in the 1990s. However, Alpenglow Expeditions has announced it will no longer use the Nepal route, due to increasing danger. It will instead use the northern route from China, where there is no icefall. This has resulted in a rise in numbers registered to climb the northern route to Everest, which has led to concerns amongst Nepali companies that climbers will divert to the Chinese side. Nepal relies heavily on tourism and the Sherpas rely on leading expeditions for their income. Also returning to Everest are several climbers who survived the avalanche.\",\n",
       "  'labels': 1,\n",
       "  'hallucination_id': [{'start': 112,\n",
       "    'end': 129,\n",
       "    'text': 'killed 16 Sherpas',\n",
       "    'meta': 'SUBTLE CONFLICT\\nOriginal:  killed 16 Nepalis\\nGenerated:  killed 16 Sherpas',\n",
       "    'label_type': 'Subtle Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False}],\n",
       "  'hallucination': {'hallucination_list': ['killed 16 Sherpas']},\n",
       "  'source': 'CNN/DM',\n",
       "  'model': 'gpt-4-0613',\n",
       "  'task_type': 'Summary',\n",
       "  'source_id': '11395',\n",
       "  'sim_before': 0.9666372537612915,\n",
       "  'sim_no_doc': 0.8068988919258118,\n",
       "  'sim_doc': 0.8072909712791443,\n",
       "  'triplet': 0.9136581420898438},\n",
       " {'ref': 'A mom furious at her son for apparently taking part in the Baltimore riots has become a sensation online. In video captured by CNN affiliate WMAR, the woman is seen pulling her masked son away from a crowd, smacking him in the head repeatedly, and screaming at him. As he tries to walk away, she follows him, screaming, \"Get the f--- over here!\" Eventually, he turns toward her, his face no longer covered.  The boy is dressed in dark pants and a black hoodie, with a dark backpack on. WMAR reports that the woman saw her son on television throwing rocks at police. The name of the woman dressed in light blue jeans, a yellow lace tunic and a cropped yellow jacket was not immediately known. But Police Commissioner Anthony Batts thanked her in remarks to the media. \"And if you saw in one scene you had one mother who grabbed their child who had a hood on his head and she started smacking him on the head because she was so embarrassed,\" he said Monday.  \"I wish I had more parents that took charge of their kids out there tonight.\"\\n',\n",
       "  'text': 'A video showing a woman furiously beating her son for participating in the Baltimore riots has gone viral. The woman, who was seen dragging her son away from the crowd and hitting him, was praised by the Police Commissioner, Anthony Batts, for her actions.',\n",
       "  'labels': 1,\n",
       "  'hallucination_id': [{'start': 34,\n",
       "    'end': 49,\n",
       "    'text': 'beating her son',\n",
       "    'meta': 'SUBTLE CONFLICT\\nOriginal: smacking him in the head repeatedly\\nGenerated: beating her son',\n",
       "    'label_type': 'Subtle Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False}],\n",
       "  'hallucination': {'hallucination_list': ['beating her son']},\n",
       "  'source': 'CNN/DM',\n",
       "  'model': 'gpt-4-0613',\n",
       "  'task_type': 'Summary',\n",
       "  'source_id': '11795',\n",
       "  'sim_before': 0.9541915059089661,\n",
       "  'sim_no_doc': 0.8195021152496338,\n",
       "  'sim_doc': 0.8391664624214172,\n",
       "  'triplet': 0.8873164653778076},\n",
       " {'ref': 'A longtime friend of Joni Mitchell has filed a legal petition seeking to be named the singer-songwriter\\'s conservator. Mitchell, who was inducted into the Rock and Roll Hall of Fame in 1997, was taken to UCLA Medical Center a month ago after being found unconscious in her home. She remains at the hospital, according to Leslie Morris\\' court petition filed April 28. Morris is described as Mitchell\\'s friend of more than 44 years in the legal documents. Whether the folk and jazz singer is conscious remains a matter of some confusion. In her April 28 petition, Morris says Mitchell is unconscious: \"At this time she (Mitchell) remains unconscious and unable to make any responses, and is therefore unable to provide for any of her personal needs.\" But on Mitchell\\'s website, a statement posted the same day that says it was approved by Morris says Mitchell is alert. \"Contrary to rumors circulating on the Internet today, Joni is not in a coma. Joni is still in the hospital -- but she comprehends, she\\'s alert, and she has her full senses. A full recovery is expected. The document obtained by a certain media outlet simply gives her longtime friend Leslie Morris the authority -- in the absence of 24-hour doctor care -- to make care decisions for Joni once she leaves the hospital.\" When asked about the discrepancy between statements about Mitchell\\'s health on her website and in the legal filing, her publicist, Alisse Kingsley, responded that \"the website\" was accurate. A doctor\\'s capacity declaration stated that Mitchell will likely be unable, due to her medical condition, to attend any court hearings for the next four to six months. Adoring fans are posting their tributes to Mitchell at WeLoveYouJoni.com. Sonya Hamasaki and Cheri Mossburg contributed to this report.\\n',\n",
       "  'text': \"Sure! Here's the summary within 74 words:\\n\\nJoni Mitchell's friend of 44 years has filed a legal petition to be appointed the singer-songwriter's conservator, as Mitchell remains unconscious in the hospital after being found at home a month ago. The petition states that Mitchell is unable to provide for her own needs, but her website claims she is alert and will make a full recovery. A doctor's declaration states that Mitchell will likely be unable to attend court hearings for the next four to six months.\",\n",
       "  'labels': 1,\n",
       "  'hallucination_id': [{'start': 59,\n",
       "    'end': 77,\n",
       "    'text': 'friend of 44 years',\n",
       "    'meta': 'EVIDENT CONFLICT\\nAIGC: friend of 44 years \\nOriginal: friend of more than 44 years',\n",
       "    'label_type': 'Subtle Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False},\n",
       "   {'start': 161,\n",
       "    'end': 205,\n",
       "    'text': 'Mitchell remains unconscious in the hospital',\n",
       "    'meta': '\\nSUBTLE CONFLICT:\\nAIGC: Mitchell remains unconscious in the hospital \\nOriginal: Mitchell remains in the hospital but doesn’t necessarily mean she’s still unconscious. It was confirmed by her friend that she’s not in a coma.',\n",
       "    'label_type': 'Subtle Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False}],\n",
       "  'hallucination': {'hallucination_list': ['friend of 44 years',\n",
       "    'Mitchell remains unconscious in the hospital']},\n",
       "  'source': 'CNN/DM',\n",
       "  'model': 'llama-2-13b-chat',\n",
       "  'task_type': 'Summary',\n",
       "  'source_id': '11860',\n",
       "  'sim_before': 0.9263888597488403,\n",
       "  'sim_no_doc': 0.8240620493888855,\n",
       "  'sim_doc': 0.8194522857666016,\n",
       "  'triplet': -0.15562522411346436},\n",
       " {'ref': '38 and home. The Kentucky Wildcats\\' bid for perfection ended Saturday night at the hands of the Wisconsin Badgers, who rallied for a tough 71-64 victory in the NCAA men\\'s basketball Final Four at Indianapolis. They will meet four-time national champ Duke in Monday night\\'s title game. The Blue Devils dominated Michigan State 81-61 in the first contest of the night. National player of the year finalist Frank Kaminsky led Wisconsin with 20 points and 11 rebounds. Junior forward Sam Dekker added 16 points. \"These guys just gutted it out.,\" Wisconsin Head Coach Bo Ryan said. \"We just kept battling on every possession.\" Freshman Karl-Anthony Towns led Kentucky with 16 points. Sophomore Andrew Harrison chipped in 13 points while Aaron Harrison had 12 points. Kentucky came into the game as the first men\\'s team to have a 38-0 record. But the Badgers fought back from a 60-56 deficit with 6:39 left in the game. Wisconsin went on an 8-0 run to regain a lead it never relinquished. It will be looking to win its first title since 1941. As the final minutes ticked off, the Wildcats failed to score. They were stuck on 60 for five minutes. The Harrison twins, who shot well in the first half, couldn\\'t get much done offensively. \"I didn\\'t execute. I mean, we didn\\'t execute as a team, but me being the point guard, I didn\\'t do what coach told me to on a couple of occasions,\" Andrew Harrison said at the postgame news conference. His coach sternly interjected. \"He did fine. He did fine,\" John Calipari said. Meanwhile, Dekker stepped up, making a layup, a three-pointer and a free throw as the Badgers scooted ahead. Calipari said the Badgers did a good job defending the Kentucky big men. \"They crowded us a little bit, and (our) guys got a little tentative,\" he said. Calipari looked at the stat sheet: Kentucky had only six turnovers, hit 90% of its free throws, made 48% of its field goals, and lost. He said his team struggled to guard Wisconsin\\'s players, and the rebound battle -- which Wisconsin won by 12 -- was crucial. Kamisky, who turned 22 on Saturday, was asked how the Badgers outrebounded a team that is the tallest in basketball. \"We stayed into them, attacking them, trying to do whatever we can,\" he said. \"Just trying to keep them off the glass was one of our main priorities.\" Freshman Justise Winslow led Duke with 19 points while national freshman of the year Jahlil Okafor had 18 points, 10 of which came in the first half. Senior guard Quinn Cook had 17. \"The defense was terrific,\" Duke Head Coach Mike Krzyzewski said. \"We drove the ball with such strength. I actually think our offense gave our defense a push.\" Junior guard Denzel Valentine sparked the Spartans with three early three-pointers and finished with 22 points and a game-high 11 rebounds. Senior guard Travis Trice had 16 points. The Spartans, of the Big Ten, began the game in a higher gear than Duke, and quickly took an eight-point lead. But after Valentine hit his trio of long-range shots and Michigan State led 14-6, the Spartans struggled to get baskets the rest of the first half. \"They did a good job of denying us and forcing us out (farther from the basket). We took some bad shots,\" Valentine said. Duke occasionally used a 2-3 zone defense that protected some of its players with foul concerns and slowed the Spartans down. But it was when Duke played man-to-man that the Spartans really had a hard time scoring. State made five of its first seven shots, then only made three more in the final 16 minutes of the opening half. The Blue Devils, of the Atlantic Coast Conference, found baskets inside as the 6-foot-11, 270-pound Okafor powered for points around the basket. Winslow used his quickness to score seven points in the first half, but he only played 12 minutes because he committed two fouls. Duke led at halftime 36-25, a turnaround of 19 points from its early deficit. \"Coach always stresses that great defense leads to great offense,\" Cook said. The confidence gained in getting defensive stops led to the desire for Duke players to drive the lane instead of shooting three-pointers. \"We saw seams that we could take advantage of,\" Cook said. In the second half, Duke increased its lead to 20 points, then cruised home. The Spartans were never able to cut the deficit to single digits and Duke sank its free throws in the final minutes. \"We got beat and ...  Duke played awfully well, but I thought it wasn\\'t one of our better games,\" Michigan State head coach Tom Izzo said. The Blue Devils apparently like playing in Final Fours in Indianapolis, home of the NCAA. It won titles there in 1991 and 2010. The other two crowns came in 1992 and 2001, each in Minneapolis. Duke has beaten Wisconsin this season, 80-70 in Wisconsin in December. If Duke wins Monday night, it would tie Indiana and North Carolina for third-most NCAA championships with five.\\n',\n",
       "  'text': \"The Wisconsin Badgers upset the top-ranked Kentucky Wildcats in the NCAA men's basketball Final Four with a 71-64 victory on Saturday night. The Badgers, led by national player of the year finalist Frank Kaminsky, overcame a 60-56 deficit with 6:39 remaining in the game and went on to win their first championship since 1941. Kentucky came into the game as the first men's team to have a 38-0 record, but the Badgers fought back from a 60-56 deficit with an 8-0 run. The Harrison twins, who shot well in the first half, struggled offensively in the second half, and the Badgers' rebounding advantage proved to be the difference in the game. Duke dominated the second half against Michigan State, leading by as many as 20 points en route to an 81-61 victory. Freshman Justise Winslow led Duke with 19 points, while national freshman of the year Jahlil Okafor had 18 points and 10 rebounds. Duke will face Wisconsin in Monday night's title game, seeking its fifth NCAA championship.\",\n",
       "  'labels': 1,\n",
       "  'hallucination_id': [{'start': 275,\n",
       "    'end': 325,\n",
       "    'text': 'went on to win their first championship since 1941',\n",
       "    'meta': 'SUBTLE CONFLICT\\nOriginal: It will be looking to win its first title since 1941\\nGenerated: went on to win their first championship since 1941',\n",
       "    'label_type': 'Subtle Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False},\n",
       "   {'start': 863,\n",
       "    'end': 888,\n",
       "    'text': '18 points and 10 rebounds',\n",
       "    'meta': 'EVIDENT CONFLICT\\nOriginal: 18 points, 10 of which came in the first half\\nGenerated: 18 points and 10 rebounds',\n",
       "    'label_type': 'Evident Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False}],\n",
       "  'hallucination': {'hallucination_list': ['went on to win their first championship since 1941',\n",
       "    '18 points and 10 rebounds']},\n",
       "  'source': 'CNN/DM',\n",
       "  'model': 'llama-2-7b-chat',\n",
       "  'task_type': 'Summary',\n",
       "  'source_id': '11910',\n",
       "  'sim_before': 0.9625529050827026,\n",
       "  'sim_no_doc': 0.7398980855941772,\n",
       "  'sim_doc': 0.806201696395874,\n",
       "  'triplet': 0.7513225078582764},\n",
       " {'ref': 'Duhok, Iraqi Kurdistan (CNN)The children laugh and shriek, as some of them seem to always have the capacity to do no matter how depressing the circumstances. Their bright clothes provide splashes of color against the otherwise drab monotone white of the endless rows of tents. A small group plays with rocks, replacements for the toys they left behind when they fled, while others clamber through a jagged tear in the wire fence surrounding the refugee camp. The Shariya refugee camp opened around six months ago, made up of some 4,000 tents and counting. Thousands of Yazidis now call this corner of Iraqi Kurdistan home, about 18 miles (30 kilometers) from one of the front lines with ISIS, where one can hear the occasional reverberation in the distance of what we are told are airstrikes. The vast majority of the camp\\'s occupants are from the town of Sinjar, which is near the border with Syrian Kurdistan, and fled the ISIS assault there back in August. But not everyone escaped. ISIS took thousands of Yazidis captive. The fighters separated the young women and girls, some as young as 8 years old, to be sold as slaves, for their \"masters\" to use as concubines. Men faced a choice: Convert to Islam or be shot. Mahmoud was out running errands when ISIS fighters arrived, taking his wife, Ahlam, their three children -- the youngest of which was just a month old -- and his elderly parents. \"They took our phones, jewelry, money,\" Ahlam recalls. \"They had guns. They forced us at gunpoint into big trailer trucks.\" They were taken to a school turned prison in Tal Afar. From there, the family was moved from village to village -- and at one stage taken to Mosul. \"They wrote everyone\\'s name down and they asked where we want to work, in the fields, as cleaners or as herders,\" she says. Ahlam and her family chose to herd goats. They were then taken to a Shia village whose residents had fled, where they were part of a group of around 40 living in one house. In the home, Ahlam found a cell phone left behind by its former occupants and called her husband. \"I said we are alive but we are prisoners.\" Ahlam\\'s husband, who up until that moment had lost the will to live, thinking his family was dead, says he cried out of happiness despite his pain. Ahlam would call when she could, briefly, after midnight, hiding under her bedcovers. If she was caught with a phone, she would be killed. The village itself was a massive prison, its entrances guarded by ISIS fighters. She recalls that two men, in their late 40s or 50s, tried to escape. When they were caught, their bones were broken, their bodies tied to the back of a truck and then driven through the streets. The Yazidi captives were forced to watch the gruesome spectacle. The men\\'s corpses were then tossed into a ditch and an order given not to bury them. One night, some of the Yazidi men risked their lives to toss dirt onto the bodies, to give those slain what dignity they could. Ahlam tells us that about a week before we met, ISIS fighters came by and took away her in-laws and the other elderly people living in the house. \"We didn\\'t know where they were taking them, we thought we would be next,\" she remembers. So she and the rest of the group realized that they had to try to flee. \"We decided that either we survive or we don\\'t.\" They left at midnight. Ahlam cradled the baby, as her two other children, ages 3 and 4 years old, clutched at her clothes. She prayed the baby wouldn\\'t cry, that the children could keep walking. They knew the general direction to take, but not the exact route, and they could only hope it was toward freedom. \"When the sun started to come up, I thought that\\'s it, we are going to get caught,\" Ahlam says. \"And what am I going to do with the kids?  I can\\'t carry all three of them and run.\" Luckily, Ahlam never had to answer that impossible question. The group made it into Iraqi Kurdistan. The couple can\\'t put into words their emotions when they were reunited. Mahmoud, gently caressing his daughter\\'s palm, says he could hardly believe that the woman whose stunning eyes and gentle words he had fallen in love with, and their three children, were by his side again. It had been eight agonizing months. But their joy was tainted by fear for Mahmoud\\'s parents. A few days after Mahmoud and Ahlam were reunited, ISIS released 217 captives. No one is disclosing exactly why. Among them were 60 children, a handful of men and women, and the rest were elderly -- including Mahmoud\\'s parents. \"We didn\\'t know if they were going to slaughter us or what they were going to do with us,\" Mahmoud\\'s father says. \"They moved us around a lot, and at one stage we stayed in in one place for three days.\" But Ahlam\\'s parents are still with ISIS. Vian Dakhil, a Yazidi member of Iraq\\'s parliament, speculates that the Yazidis ISIS released are individuals they were struggling to care for. Dakhil is part of Iraqi Prime Minister Haider al-Abadi\\'s delegation to the United States this week. She will be addressing the United Nations -- pleading for more international support for Iraq and more airstrikes to help defeat ISIS -- and she will address the plight of the Yazidis. \"We sometimes say that we wish we had been massacred. This would be better than being kidnapped and raped. We prefer death now over the fate of what is happening to these girls and women,\" Dakhil says. \"It is now on the government and on the international community to focus on this. How to get these captives back. It\\'s inconceivable that in the 21st century, something like this is happening as if we were living in the Stone Age.\" Ahlam says she was spared because she was breastfeeding and she had young children, which, we\\'re told, makes her impure and therefore unable to be used as a sex slave. For Ahlam, what she went through was not the hardest part of her ordeal. It\\'s the moment when the ISIS fighters began taking away the girls and young women. She\\'s haunted by their screams, the image of them being dragged away sobbing and screaming.\\n',\n",
       "  'text': \"A family, including a pregnant woman and three young children, managed to escape from a Yazidi refugee camp in Duhok, Iraqi Kurdistan, after being held captive by ISIS for eight months. The family, along with hundreds of others, were taken from their homes in Sinjar and brought to the camp, located near the front lines with ISIS. Many of the camp's inhabitants are Yazidis who were captured and enslaved by ISIS, with reports of widespread rape and abuse. The family's story is one of horror and resilience, as they recount the brutality they faced at the hands of their captors, including the separation of women and children, and the sale of young girls as sex slaves. Despite the hardship, the family is grateful to be alive and are hoping for international support to help free the remaining captives and rebuild their community.\",\n",
       "  'labels': 1,\n",
       "  'hallucination_id': [{'start': 10,\n",
       "    'end': 36,\n",
       "    'text': 'including a pregnant woman',\n",
       "    'meta': 'SUBTLE CONFLICT:\\nAIGC:including a pregnant woman\\nOriginal: …their three children…\\n..breast feeding… (implies she just gave birth, there was no mention that she was pregnant) \\n',\n",
       "    'label_type': 'Subtle Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False}],\n",
       "  'hallucination': {'hallucination_list': ['including a pregnant woman']},\n",
       "  'source': 'CNN/DM',\n",
       "  'model': 'llama-2-7b-chat',\n",
       "  'task_type': 'Summary',\n",
       "  'source_id': '15778',\n",
       "  'sim_before': 0.9511702656745911,\n",
       "  'sim_no_doc': 0.7486119270324707,\n",
       "  'sim_doc': 0.7978987693786621,\n",
       "  'triplet': 0.41077110171318054},\n",
       " {'ref': 'Two Delaware boys are in a coma and their father still is unable to talk or move two weeks after they became sick -- perhaps from pesticide exposure, federal officials say -- during a trip to the U.S. Virgin Islands, their lawyer said Saturday. Steve Esmond, his teenage sons and the teens\\' mother fell ill more than two weeks ago in St. John, where they were renting a villa at the Sirenusa resort. The family was airlifted to hospitals in the United States. The boys, 16 and 14, were in critical condition at a Philadelphia hospital on Saturday, the family\\'s lawyer, James Maron of Delaware, said. \"The boys are in rough shape,\" Maron said. \"The family are all fighters,\" he added. \"They\\'re fighting for everything right now. I understand it\\'s a long recovery.\" Esmond, also being treated at a hospital, is conscious but cannot move, Maron said. The teens\\' mother, Theresa Devine, was treated at a hospital but released, and is now in occupational therapy, Maron said. The U.S. Environmental Protection Agency said Friday that the presence of a pesticide at the rented villa in St. John may have caused the illnesses, which were reported to the EPA on March 20. Paramedics were called to the villa, which the family began had been renting since March 14. Esmond was found unconscious; the boys and their mother were having seizures, Maron said. The lawyer did not say who called the paramedics. Elias Rodriguez, an EPA spokesman, said the agency\\'s preliminary test results \"do show that there was a presence of methyl bromide in the unit where the family was staying.\" Exposure to methyl bromide can result in serious health effects, including central nervous system and respiratory system damage, according to the EPA. The use of the pesticide is restricted in the United States because of its acute toxicity. It\\'s not allowed to be used indoors. Only certified professionals are permitted to use it in certain agricultural settings. \"It\\'s an ongoing investigation; we\\'re still on the island doing our assessment,\" Rodriguez said. \"We have been doing different types of air sampling and wipe sampling.\" Final test results were expected next week. The EPA said it is working with local government agencies to investigate whether the family was made ill after a fumigation at the resort on March 18 and whether any environmental regulations or laws were violated. Maron, the family\\'s attorney, declined to comment on the investigation. Depending on the season, the luxury villa where the family stayed rents between $550 and $1,200 per night. Sea Glass Vacations, which acts as a rental agent for several units at Sirenusa, said the unit directly below the one where the family stayed was recently treated for pests, but their unit was not treated. The company said it licensed an outside company, Terminix, for the pest control services. \"Sea Glass Vacations does not treat the units it manages for pests but instead relies on licensed professionals for pest control services,\" the company said in a statement. The U.S. Department of Justice has initiated a criminal investigation into the matter, according to a U.S. Securities and Exchange Commission filing made Monday by ServiceMaster Global Holdings, the parent company of Terminix. In an email to CNN, a spokesman for Terminix wrote that the company is \"committed to performing all work ... in a manner that is safe for our customers, employees, the public and the environment\" and is \"looking into this matter internally, and cooperating with authorities.\" \"We\\'re thinking about the family, and we join the community in wishing them a speedy recovery,\" Terminix wrote. The SEC filing described the injuries to the family members as \"serious.\"\\n',\n",
       "  'text': \"Two Delaware boys are in a coma and their father remains unable to talk or move two weeks after falling ill during a trip to the US Virgin Islands. The family was renting a villa at the Sirenusa resort when they became ill, and initial tests reveal that a pesticide may have caused their illnesses. The US Environmental Protection Agency (EPA) is investigating whether the family was exposed to the pesticide during a fumigation at the resort on March 18. The family's lawyer declined to comment on the investigation, but the EPA confirmed that it is working with local government agencies to determine if any environmental regulations or laws were violated. The boys, aged 16 and 14, and their mother were transported to hospitals in the US for treatment, while the father remains hospitalized.\",\n",
       "  'labels': 1,\n",
       "  'hallucination_id': [{'start': 659,\n",
       "    'end': 756,\n",
       "    'text': 'The boys, aged 16 and 14, and their mother were transported to hospitals in the US for treatment,',\n",
       "    'meta': 'SUBTLE CONFLICT\\nAll of them are transported to hospitals in US.\\noriginal:  The family was airlifted to hospitals in the United States',\n",
       "    'label_type': 'Subtle Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False}],\n",
       "  'hallucination': {'hallucination_list': ['The boys, aged 16 and 14, and their mother were transported to hospitals in the US for treatment,']},\n",
       "  'source': 'CNN/DM',\n",
       "  'model': 'llama-2-7b-chat',\n",
       "  'task_type': 'Summary',\n",
       "  'source_id': '15786',\n",
       "  'sim_before': 0.9483201503753662,\n",
       "  'sim_no_doc': 0.7511695623397827,\n",
       "  'sim_doc': 0.7829123735427856,\n",
       "  'triplet': -0.025741098448634148},\n",
       " {'ref': 'It\\'s been a busy few weeks for multiples. The first set of female quintuplets in the world since 1969 was born in Houston on April 8, and the parents are blogging about their unique experience. Danielle Busby delivered all five girls at the Woman\\'s Hospital of Texas via C-section at 28 weeks and two days, according to CNN affiliate KPRC. Parents Danielle and Adam and big sister Blayke are now a family of eight. The babies are named Ava Lane, Hazel Grace, Olivia Marie, Parker Kate and Riley Paige. \"We are so thankful and blessed,\" said Danielle Busby, who had intrauterine insemination to get pregnant. \"I honestly give all the credit to my God. I am so thankful for this wonderful hospital and team of people here. They truly all are amazing.\" You can learn all about their journey at their blog, \"It\\'s a Buzz World.\" Early news reports said the Busby girls were the first all-female quintuplets born in the U.S. But a user alerted CNN to news clippings that show quintuplet girls were born in 1959 to Charles and Cecilia Hannan in San Antonio. All of the girls died within 24 hours. Like the Busby family, Sharon and Korey Rademacher were hoping for a second child. When they found out what they were having, they decided to keep it a secret from family and friends. That\\'s why they didn\\'t tell their family the gender of baby No. 2 -- or that Sharon was actually expecting not one but two girls, according to CNN affiliate WEAR. And when everyone arrived at West Florida Hospital in Pensacola, Florida, after Sharon gave birth March 11, they recorded everyone\\'s reactions to meeting twins Mary Ann Grace and Brianna Faith. The video was uploaded to YouTube on Saturday and has been viewed more than 700,000 times. Could you keep it a secret?\\n',\n",
       "  'text': 'Sure! Here is a summary of the news in 77 words:\\n\\nThe Busby family welcomed five female quintuplets in Houston after IVF treatment, while the Rademacher family surprised everyone with the arrival of twin girls in Florida. The Busby girls are Ava, Hazel, Olivia, Parker, and Riley, while the Rademacher twins are Mary Ann and Brianna. Both families kept their pregnancies a secret until the babies were born.',\n",
       "  'labels': 1,\n",
       "  'hallucination_id': [{'start': 334,\n",
       "    'end': 406,\n",
       "    'text': 'Both families kept their pregnancies a secret until the babies were born',\n",
       "    'meta': 'EVIDENT CONFLICT\\nOriginal: \"The first set of female quintuplets... parents are blogging about their unique experience.\"\\nGenerative: \"Both families kept their pregnancies a secret until the babies were born.\"\\n(Only Sharon and Korey Rademacher decided to keep it a secret from family and friends.)',\n",
       "    'label_type': 'Evident Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False},\n",
       "   {'start': 117,\n",
       "    'end': 131,\n",
       "    'text': 'IVF treatment,',\n",
       "    'meta': 'SUBTLE CONFLICT\\nOriginal: \"intrauterine insemination\"\\nGenerative: \"IVF treatment\"\\n\\nDifferent procedure but same goal. Does not change any factual details.',\n",
       "    'label_type': 'Subtle Conflict',\n",
       "    'implicit_true': False,\n",
       "    'due_to_null': False}],\n",
       "  'hallucination': {'hallucination_list': ['Both families kept their pregnancies a secret until the babies were born',\n",
       "    'IVF treatment,']},\n",
       "  'source': 'CNN/DM',\n",
       "  'model': 'llama-2-7b-chat',\n",
       "  'task_type': 'Summary',\n",
       "  'source_id': '11768',\n",
       "  'sim_before': 0.9255634546279907,\n",
       "  'sim_no_doc': 0.7106889486312866,\n",
       "  'sim_doc': 0.7979223728179932,\n",
       "  'triplet': 0.06573312729597092}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
