{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"dataset/rag_truth_test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ref': 'The FBI charged a Philadelphia woman on Thursday with trying to travel overseas to fight for ISIS. She\\'s one of three women arrested this week on terror charges. Two New York women were also taken into custody. An FBI complaint cites numerous social media messages dating back to August 2013 that were sent by Keonna Thomas, 30, also known as \"Young Lioness\" and \"Fatayat Al Khilafah.\" One Twitter message said, \"If we truly knew the realities ... we all would be rushing to join our brothers in the front lines pray ALLAH accept us as shuhada [martyrs].\" Another said, \"When you\\'re a mujahid [violent jihadi fighter] your death becomes a wedding.\" The FBI said Thomas purchased an electronic visa to Turkey on March 23. Turkey is known as the easiest place from which to enter Syria and join ISIS. An ISIS manual advises recruits to buy round-trip tickets to vacation spots such as Spain and then purchase tickets for their real destination once they arrive overseas, the FBI said. On March 26, Thomas purchased a ticket to Barcelona, with a March 29 departure and an April 15 return to the United States, the complaint said. It\\'s not clear when or where she was arrested. She was charged with knowingly attempting to provide material support and resources to a designated foreign terrorist organization. She could be sentenced to 15 years in prison. On Thursday, Noelle Velentzas, 28, and her former roommate, Asia Siddiqui, 31, were arrested in New York and accused of planning to build an explosive device for attacks in the United States, federal prosecutors said. In the past 18 months, the Justice Department\\'s National Security Division has prosecuted or is prosecuting more than 30 cases of people attempting to travel abroad to join or provide support to terrorist groups. Of those cases, 18 allegedly involve support to ISIS. \"The terrorist threat is more decentralized, more diffuse, more complicated,\" Homeland Security Secretary Jeh Johnson told reporters Thursday. \"It involves the potential lone wolf actor, it involves the effective use of social media, the Internet.\"\\n',\n",
       " 'text': 'Please judge the following statement whether it includes hallucination or not (based on the references above): The FBI has charged a Philadelphia woman, Keonna Thomas, with trying to travel overseas to fight for ISIS. This follows the arrests of two New York women, Noelle Velentzas and Asia Siddiqui, who were accused of planning to build an explosive device for attacks in the United States. Thomas purchased an electronic visa to Turkey and a round-trip ticket to Barcelona, following the advice given in an ISIS manual for recruits. She has been charged with attempting to provide material support to a terrorist organisation and could face 15 years in prison.',\n",
       " 'labels': 0,\n",
       " 'source': 'CNN/DM',\n",
       " 'model': 'gpt-4-0613',\n",
       " 'task_type': 'Summary',\n",
       " 'source_id': '15596'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_prefix(data):\n",
    "    for d in data:\n",
    "        d[\"text\"] = \"Please judge the following statement whether it includes hallucination or not (based on the references above): \" + d[\"text\"]\n",
    "    return data\n",
    "\n",
    "\n",
    "dev_data = add_prefix(test_data)\n",
    "dev_data[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_type: QA, Data2txt, Summary\n",
    "task_name = \"Summary\"\n",
    "test_data = [d for d in test_data if d[\"task_type\"] == task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['ref', 'text', 'labels', 'source', 'model', 'task_type', 'source_id'],\n",
       "        num_rows: 900\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "raw_datasets = DatasetDict({\"test\":test_ds})\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from models_rob import NoDocModel\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"FacebookAI/RoBERTa-base\")\n",
    "# load trained model\n",
    "name = \"./trained/no_doc_rob\"\n",
    "model = NoDocModel.from_pretrained(base_model,name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc216da133640b7870c3d368eb887ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['ref', 'labels', 'source', 'model', 'task_type', 'source_id', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 900\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0, 'label': 0, 'task': 'Summary'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for i, d in enumerate(tokenized_datasets[\"test\"]):\n",
    "    results.append({\"id\": i,\"label\":d[\"labels\"],\"task\":d[\"task_type\"]})\n",
    "\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2700it [00:13, 199.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i,d in tqdm(enumerate(tokenized_datasets[\"test\"])):\n",
    "    input_ids =torch.tensor(d[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(d[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[\"logits\"]\n",
    "        predicted_index = torch.argmax(logits, dim=-1) # predicted label\n",
    "    results[i][\"no_rag_logits\"] = logits.cpu().numpy()\n",
    "    results[i][\"no_rag_label\"] = predicted_index.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### with_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "from models_rob import WithDocModel\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"FacebookAI/RoBERTa-base\")\n",
    "\n",
    "name = \"./trained/with_doc_rob\"\n",
    "model = WithDocModel.from_pretrained(base_model,name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    ref = tokenizer(examples[\"ref\"],truncation=True, max_length=512)\n",
    "    text = tokenizer(examples[\"text\"],truncation=True, max_length=512)\n",
    "    return {\n",
    "        \"ref_input_ids\":ref[\"input_ids\"],\n",
    "        \"ref_attention_mask\":ref[\"attention_mask\"],\n",
    "        \"text_input_ids\":text[\"input_ids\"],\n",
    "        \"text_attention_mask\":text[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\",\"ref\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2700it [00:32, 84.11it/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i,d in tqdm(enumerate(tokenized_datasets[\"test\"])):\n",
    "    ref_input_ids = torch.tensor(d[\"ref_input_ids\"]).unsqueeze(0).to(device)\n",
    "    text_input_ids = torch.tensor(d[\"text_input_ids\"]).unsqueeze(0).to(device)\n",
    "    input_ids = [ref_input_ids, text_input_ids]\n",
    "    \n",
    "    ref_attention_mask = torch.tensor(d[\"ref_attention_mask\"]).unsqueeze(0).to(device)\n",
    "    text_attention_mask = torch.tensor(d[\"text_attention_mask\"]).unsqueeze(0).to(device)\n",
    "    attention_mask = [ref_attention_mask, text_attention_mask]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[\"logits\"]\n",
    "        predicted_index = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "    results[i][\"rag_logits\"] = logits.cpu().numpy()\n",
    "    results[i][\"rag_label\"] = predicted_index.cpu().numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet loss is not calculated during inference, so the other data is dummy (\"hallucination\"/\"not hallucination\")\n",
    "\n",
    "def create_trip_ver2(data):\n",
    "    trip = []\n",
    "    for d in data:\n",
    "        if d[\"labels\"] == 0: # not hallucination\n",
    "            trip.append({\"anchor\":d[\"ref\"],\"positive\": d[\"text\"], \"negative\": \"hallucination\", \"labels\": 0})\n",
    "        else:\n",
    "            trip.append({\"anchor\":d[\"ref\"],\"positive\": \"not hallucination\", \"negative\": d[\"text\"], \"labels\": 1})\n",
    "    return trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['anchor', 'positive', 'negative', 'labels'],\n",
       "        num_rows: 900\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "test_trip = create_trip_ver2(test_data)\n",
    "\n",
    "test_df = pd.DataFrame(test_trip)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "raw_datasets = DatasetDict({\"test\":test_ds})\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def triplet_loss(anchor_output, positive_output, negative_output, positive_logits, negative_logits):\n",
    "    positive_targets = torch.zeros(positive_output.size(0), dtype=torch.long).to(device)  # not hallucination\n",
    "    negative_targets = torch.ones(negative_output.size(0), dtype=torch.long).to(device)\n",
    "    positive_loss = nn.CrossEntropyLoss()(positive_logits, positive_targets)\n",
    "    negative_loss = nn.CrossEntropyLoss()(negative_logits, negative_targets)\n",
    "\n",
    "    classification_loss = (positive_loss + negative_loss) / 2.0\n",
    "\n",
    "    triplet_loss_fn = (nn.TripletMarginWithDistanceLoss(margin=1.0,distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)))\n",
    "    triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "\n",
    "    return classification_loss, triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "from models_rob import TripletModel\n",
    "\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"FacebookAI/RoBERTa-base\")\n",
    "\n",
    "name = \"./trained/triplet_rob\"\n",
    "model = TripletModel.from_pretrained(base_model, triplet_loss, name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    anchor = tokenizer(examples[\"anchor\"], truncation=True,max_length=512)\n",
    "    positive = tokenizer(examples[\"positive\"], truncation=True,max_length=512)\n",
    "    negative = tokenizer(examples[\"negative\"], truncation=True,max_length=512)\n",
    "\n",
    "    return {\n",
    "        \"anchor_input_ids\": anchor[\"input_ids\"],\n",
    "        \"anchor_attention_mask\": anchor[\"attention_mask\"],\n",
    "        \"positive_input_ids\": positive[\"input_ids\"],\n",
    "        \"positive_attention_mask\": positive[\"attention_mask\"],\n",
    "        \"negative_input_ids\": negative[\"input_ids\"],\n",
    "        \"negative_attention_mask\": negative[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"anchor\", \"positive\", \"negative\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "900it [00:13, 65.32it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i,d in tqdm(enumerate(tokenized_datasets[\"test\"])):\n",
    "    \n",
    "    flag = False # hallusination → True\n",
    "    if d[\"labels\"] == 1:\n",
    "        flag = True\n",
    "    \n",
    "    anchor_input_ids = torch.tensor(d[\"anchor_input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    positive_input_ids = torch.tensor(d[\"positive_input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    negative_input_ids = torch.tensor(d[\"negative_input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    input_ids = [anchor_input_ids, positive_input_ids, negative_input_ids]\n",
    "    anchor_attention_mask = torch.tensor(d[\"anchor_attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    positive_attention_mask = torch.tensor(d[\"positive_attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    negative_attention_mask = torch.tensor(d[\"negative_attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask = [anchor_attention_mask, positive_attention_mask, negative_attention_mask]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        if flag:\n",
    "            logits = outputs.logits[1] # negative_logit\n",
    "        else:\n",
    "            logits = outputs.logits[0] # positive_logit\n",
    "        predicted_index = torch.argmax(logits, dim=-1)\n",
    "    results[i][\"triplet_logits\"] = logits.cpu().numpy()\n",
    "    results[i][\"triplet_label\"] = predicted_index.cpu().numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2699,\n",
       " 'label': 0,\n",
       " 'task': 'QA',\n",
       " 'no_rag_logits': array([[-0.47302994,  0.9884404 ]], dtype=float32),\n",
       " 'no_rag_label': array([1]),\n",
       " 'rag_logits': array([[0.34309223, 0.17094655]], dtype=float32),\n",
       " 'rag_label': array([0]),\n",
       " 'triplet_logits': array([[-1.6563241,  1.5209625]], dtype=float32),\n",
       " 'triplet_label': array([1])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "import numpy as np\n",
    "import json\n",
    "for result in results:\n",
    "    if isinstance(result[\"no_rag_logits\"], np.ndarray):\n",
    "        result[\"no_rag_logits\"] = result[\"no_rag_logits\"].tolist()\n",
    "    if isinstance(result[\"rag_logits\"], np.ndarray):\n",
    "        result[\"rag_logits\"] = result[\"rag_logits\"].tolist()\n",
    "    if isinstance(result[\"triplet_logits\"], np.ndarray):\n",
    "        result[\"triplet_logits\"] = result[\"triplet_logits\"].tolist()\n",
    "    if isinstance(result[\"no_rag_label\"], np.ndarray):\n",
    "        result[\"no_rag_label\"] = result[\"no_rag_label\"].tolist()\n",
    "    if isinstance(result[\"rag_label\"], np.ndarray):\n",
    "        result[\"rag_label\"] = result[\"rag_label\"].tolist()\n",
    "    if isinstance(result[\"triplet_label\"], np.ndarray):\n",
    "        result[\"triplet_label\"] = result[\"triplet_label\"].tolist()\n",
    "\n",
    "\n",
    "\n",
    "with open(\"check_improve_rob.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('check_improve_rob.json',\"r\") as f:\n",
    "    results = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516 53 455 88 148 49 638 753\n"
     ]
    }
   ],
   "source": [
    "# compare no_doc and triplet\n",
    "\n",
    "id_000 = [] # both success not to detect\n",
    "id_001 = [] # both fail to detect\n",
    "id_010 = [] # over-detect in triplet\n",
    "id_011 = [] # success to detect only in triplet\n",
    "id_100 = [] # success not to detect only i triplet\n",
    "id_101 = [] # fail to detect only in triplet\n",
    "id_110 = [] # both over-detect\n",
    "id_111 = [] # both success to detect\n",
    "\n",
    "for result in results:\n",
    "    if result[\"no_rag_label\"][0]==0 and result[\"triplet_label\"][0]==0 and result[\"label\"]==0:\n",
    "        id_000.append(result[\"id\"])\n",
    "    elif result[\"no_rag_label\"][0]==0 and result[\"triplet_label\"][0]==0 and result[\"label\"]==1:\n",
    "        id_001.append(result[\"id\"])\n",
    "    elif result[\"no_rag_label\"][0]==0 and result[\"triplet_label\"][0]==1 and result[\"label\"]==0:\n",
    "        id_010.append(result[\"id\"])\n",
    "    elif result[\"no_rag_label\"][0]==0 and result[\"triplet_label\"][0]==1 and result[\"label\"]==1:\n",
    "        id_011.append(result[\"id\"])\n",
    "    elif result[\"no_rag_label\"][0]==1 and result[\"triplet_label\"][0]==0 and result[\"label\"]==0:\n",
    "        id_100.append(result[\"id\"])\n",
    "    elif result[\"no_rag_label\"][0]==1 and result[\"triplet_label\"][0]==0 and result[\"label\"]==1:\n",
    "        id_101.append(result[\"id\"])\n",
    "    elif result[\"no_rag_label\"][0]==1 and result[\"triplet_label\"][0]==1 and result[\"label\"]==0:\n",
    "        id_110.append(result[\"id\"])\n",
    "    elif result[\"no_rag_label\"][0]==1 and result[\"triplet_label\"][0]==1 and result[\"label\"]==1:\n",
    "        id_111.append(result[\"id\"])    \n",
    "    \n",
    "\n",
    "print(len(id_000),len(id_001),len(id_010),len(id_011),len(id_100),len(id_101),len(id_110),len(id_111))\n",
    "# 000:516, 001:53, 010:455, 011:88, 100:148, 101:49, 110:638, 111:753 ---rob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"dataset/rag_truth_test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "def add_prefix(data):\n",
    "    for d in data:\n",
    "        d[\"text\"] = \"Please judge the following statement whether it includes hallucination or not based on the references above: \" + d[\"text\"]\n",
    "    return data\n",
    "\n",
    "test_data = add_prefix(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_type: QA, Data2txt, Summary\n",
    "task_name = \"Data2txt\"\n",
    "task = \"d2t\" # used for file name\n",
    "test_data = [d for d in test_data if d[\"task_type\"] == task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_folder = \"embedding\"\n",
    "os.makedirs(save_folder, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "from models_rob import NoDocModel\n",
    "\n",
    "model_name = \"FacebookAI/RoBERTa-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name) # before train\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "texts = [x[\"text\"] for x in test_data]\n",
    "\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs[1] \n",
    "    embeddings = embeddings.cpu().numpy()  \n",
    "\n",
    "# dimensional reduction\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "embeddings_2d = umap_reducer.fit_transform(embeddings)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, text in enumerate(texts):\n",
    "    if test_data[i][\"task_type\"] == \"QA\" and test_data[i][\"labels\"] == 1:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#000000\",s = 30,marker=\"^\")\n",
    "    elif test_data[i][\"task_type\"] == \"QA\" and test_data[i][\"labels\"] == 0:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"red\",s = 30)\n",
    "    elif test_data[i][\"task_type\"] == \"Data2txt\" and test_data[i][\"labels\"] == 1:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#000000\",s = 30,marker=\"^\")\n",
    "    elif test_data[i][\"task_type\"] == \"Data2txt\" and test_data[i][\"labels\"] == 0:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#00DDFF\",s = 30)\n",
    "    elif test_data[i][\"task_type\"] == \"Summary\" and test_data[i][\"labels\"] == 1:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#000000\",s = 30,marker=\"^\")\n",
    "    elif test_data[i][\"task_type\"] == \"Summary\" and test_data[i][\"labels\"] == 0:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"green\",s = 30)\n",
    "\n",
    "# change according to task\n",
    "\n",
    "#plt.scatter([], [],color = \"#000000\", label = \"QA (hal)\",marker=\"^\")\n",
    "#plt.scatter([], [],color = \"red\", label = \"QA (not hal)\")\n",
    "plt.scatter([], [],color = \"#000000\", label = \"Data2txt (hal)\",marker=\"^\")\n",
    "plt.scatter([], [],color = \"#00DDFF\", label = \"Data2txt (not hal)\")\n",
    "#plt.scatter([], [],color = \"#002100\", label = \"Summary (hal)\",marker=\"^\")\n",
    "#plt.scatter([], [],color = \"green\", label = \"Summary (not hal)\")\n",
    "\n",
    "\n",
    "plt.legend(fontsize=\"x-large\")\n",
    "\n",
    "file_path = os.path.join(save_folder, f\"before_{task}.png\")\n",
    "plt.savefig(file_path, format=\"png\", dpi=300) \n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "from models_rob import NoDocModel\n",
    "\n",
    "model_name = \"FacebookAI/RoBERTa-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "model = NoDocModel.from_pretrained(base_model, \"trained/no_doc_rob\") \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "texts = [x[\"text\"] for x in test_data]\n",
    "\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings =outputs.output \n",
    "    embeddings = embeddings.cpu().numpy() \n",
    "\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "embeddings_2d = umap_reducer.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, text in enumerate(texts):\n",
    "    if test_data[i][\"task_type\"] == \"QA\" and test_data[i][\"labels\"] == 1:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#000000\",s = 30,marker=\"^\")\n",
    "    elif test_data[i][\"task_type\"] == \"QA\" and test_data[i][\"labels\"] == 0:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"red\",s = 30)\n",
    "    elif test_data[i][\"task_type\"] == \"Data2txt\" and test_data[i][\"labels\"] == 1:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#000000\",s = 30,marker=\"^\")\n",
    "    elif test_data[i][\"task_type\"] == \"Data2txt\" and test_data[i][\"labels\"] == 0:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#00DDFF\",s = 30)\n",
    "    elif test_data[i][\"task_type\"] == \"Summary\" and test_data[i][\"labels\"] == 1:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#000000\",s = 30,marker=\"^\")\n",
    "    elif test_data[i][\"task_type\"] == \"Summary\" and test_data[i][\"labels\"] == 0:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"green\",s = 30)\n",
    "\n",
    "#plt.scatter([], [],color = \"#000000\", label = \"QA (hal)\",marker=\"^\")\n",
    "#plt.scatter([], [],color = \"red\", label = \"QA (not hal)\")\n",
    "plt.scatter([], [],color = \"#000000\", label = \"Data2txt (hal)\",marker=\"^\")\n",
    "plt.scatter([], [],color = \"#00DDFF\", label = \"Data2txt (not hal)\")\n",
    "#plt.scatter([], [],color = \"#002100\", label = \"Summary (hal)\",marker=\"^\")\n",
    "#plt.scatter([], [],color = \"green\", label = \"Summary (not hal)\")\n",
    "\n",
    "plt.legend(fontsize=\"x-large\")\n",
    "\n",
    "file_path = os.path.join(save_folder, f\"no_doc_{task}.png\")\n",
    "plt.savefig(file_path, format=\"png\", dpi=300) \n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "from models_rob import WithDocModel\n",
    "\n",
    "\n",
    "model_name = \"FacebookAI/RoBERTa-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "model = WithDocModel.from_pretrained(base_model, \"trained/with_doc_rob\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# data\n",
    "refs = [x[\"ref\"] for x in test_data]\n",
    "texts = [x[\"text\"] for x in test_data]\n",
    "refs_tok = tokenizer(refs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "texts_tok = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "inputs = {\n",
    "    \"input_ids\":[refs_tok[\"input_ids\"].to(device),texts_tok[\"input_ids\"].to(device)],\n",
    "    \"attention_mask\":[refs_tok[\"attention_mask\"].to(device),texts_tok[\"attention_mask\"].to(device)]\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings =outputs.output[1] \n",
    "    embeddings = embeddings.cpu().numpy()  \n",
    "\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "embeddings_2d = umap_reducer.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, text in enumerate(texts):\n",
    "    if test_data[i][\"task_type\"] == \"QA\" and test_data[i][\"labels\"] == 1:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#000000\",s = 30,marker=\"^\")\n",
    "    elif test_data[i][\"task_type\"] == \"QA\" and test_data[i][\"labels\"] == 0:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"red\",s = 30)\n",
    "    elif test_data[i][\"task_type\"] == \"Data2txt\" and test_data[i][\"labels\"] == 1:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#000000\",s = 30,marker=\"^\")\n",
    "    elif test_data[i][\"task_type\"] == \"Data2txt\" and test_data[i][\"labels\"] == 0:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#00DDFF\",s = 30)\n",
    "    elif test_data[i][\"task_type\"] == \"Summary\" and test_data[i][\"labels\"] == 1:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#000000\",s = 30,marker=\"^\")\n",
    "    elif test_data[i][\"task_type\"] == \"Summary\" and test_data[i][\"labels\"] == 0:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"green\",s = 30)\n",
    "\n",
    "#plt.scatter([], [],color = \"#000000\", label = \"QA (hal)\",marker=\"^\")\n",
    "#plt.scatter([], [],color = \"red\", label = \"QA (not hal)\")\n",
    "plt.scatter([], [],color = \"#000000\", label = \"Data2txt (hal)\",marker=\"^\")\n",
    "plt.scatter([], [],color = \"#00DDFF\", label = \"Data2txt (not hal)\")\n",
    "#plt.scatter([], [],color = \"#002100\", label = \"Summary (hal)\",marker=\"^\")\n",
    "#plt.scatter([], [],color = \"green\", label = \"Summary (not hal)\")\n",
    "\n",
    "plt.legend(fontsize=\"x-large\")\n",
    "\n",
    "file_path = os.path.join(save_folder, f\"with_doc_{task}.png\")\n",
    "plt.savefig(file_path, format=\"png\", dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def triplet_loss(anchor_output, positive_output, negative_output, positive_logits, negative_logits):\n",
    "    positive_targets = torch.zeros(positive_output.size(0), dtype=torch.long).to(device)  \n",
    "    negative_targets = torch.ones(negative_output.size(0), dtype=torch.long).to(device)\n",
    "    positive_loss = nn.CrossEntropyLoss()(positive_logits, positive_targets)\n",
    "    negative_loss = nn.CrossEntropyLoss()(negative_logits, negative_targets)\n",
    "\n",
    "    classification_loss = (positive_loss + negative_loss) / 2.0\n",
    "\n",
    "    triplet_loss_fn = (nn.TripletMarginWithDistanceLoss(margin=1.0,distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)))\n",
    "    triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "\n",
    "    return classification_loss, triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "from models_rob import TripletModel\n",
    "\n",
    "model_name = \"FacebookAI/RoBERTa-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "model = TripletModel.from_pretrained(base_model, triplet_loss,\"trained/triplet_rob\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "refs = [x[\"ref\"] for x in test_data]\n",
    "posis = [x[\"text\"] if x[\"labels\"] == 0 else \"sample\" for x in test_data]\n",
    "negas = [x[\"text\"] if x[\"labels\"] == 1 else \"sample\" for x in test_data]\n",
    "labels = [x[\"labels\"] for x in test_data]\n",
    "\n",
    "refs_tok = tokenizer(refs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "posis_tok = tokenizer(posis, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "negas_tok = tokenizer(negas, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "inputs = {\n",
    "    \"input_ids\":[refs_tok[\"input_ids\"].to(device),posis_tok[\"input_ids\"].to(device),negas_tok[\"input_ids\"].to(device)],\n",
    "    \"attention_mask\":[refs_tok[\"attention_mask\"].to(device),posis_tok[\"attention_mask\"].to(device),negas_tok[\"attention_mask\"].to(device)],\n",
    "    \"labels\":labels\n",
    "}\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings =outputs.output[1] \n",
    "    embeddings = embeddings.cpu().numpy() \n",
    "\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "embeddings_2d = umap_reducer.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, text in enumerate(texts):\n",
    "    if test_data[i][\"task_type\"] == \"QA\" and test_data[i][\"labels\"] == 1:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#000000\",s = 30,marker=\"^\")\n",
    "    elif test_data[i][\"task_type\"] == \"QA\" and test_data[i][\"labels\"] == 0:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"red\",s = 30)\n",
    "    elif test_data[i][\"task_type\"] == \"Data2txt\" and test_data[i][\"labels\"] == 1:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#000000\",s = 30,marker=\"^\")\n",
    "    elif test_data[i][\"task_type\"] == \"Data2txt\" and test_data[i][\"labels\"] == 0:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#00DDFF\",s = 30)\n",
    "    elif test_data[i][\"task_type\"] == \"Summary\" and test_data[i][\"labels\"] == 1:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"#000000\",s = 30,marker=\"^\")\n",
    "    elif test_data[i][\"task_type\"] == \"Summary\" and test_data[i][\"labels\"] == 0:\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],color = \"green\",s = 30)\n",
    "\n",
    "#plt.scatter([], [],color = \"#000000\", label = \"QA (hal)\",marker=\"^\")\n",
    "#plt.scatter([], [],color = \"red\", label = \"QA (not hal)\")\n",
    "plt.scatter([], [],color = \"#000000\", label = \"Data2txt (hal)\",marker=\"^\")\n",
    "plt.scatter([], [],color = \"#00DDFF\", label = \"Data2txt (not hal)\")\n",
    "#plt.scatter([], [],color = \"#002100\", label = \"Summary (hal)\",marker=\"^\")\n",
    "#plt.scatter([], [],color = \"green\", label = \"Summary (not hal)\")\n",
    "\n",
    "plt.legend(fontsize=\"x-large\")\n",
    "\n",
    "file_path = os.path.join(save_folder, f\"triplet_{task}.png\")\n",
    "plt.savefig(file_path, format=\"png\", dpi=300) \n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity (between document and text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers==4.44.0\n",
    "%pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"dataset/rag_truth_span_test.json\", \"r\") as f:\n",
    "    test_data_span = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def triplet_loss(anchor_output, positive_output, negative_output, positive_logits, negative_logits):\n",
    "    positive_targets = torch.zeros(positive_output.size(0), dtype=torch.long).to(device)  \n",
    "    negative_targets = torch.ones(negative_output.size(0), dtype=torch.long).to(device)\n",
    "    positive_loss = nn.CrossEntropyLoss()(positive_logits, positive_targets)\n",
    "    negative_loss = nn.CrossEntropyLoss()(negative_logits, negative_targets)\n",
    "\n",
    "    classification_loss = (positive_loss + negative_loss) / 2.0\n",
    "\n",
    "    triplet_loss_fn = (nn.TripletMarginWithDistanceLoss(margin=1.0,distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)))\n",
    "    triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "\n",
    "    return classification_loss, triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from models_rob import NoDocModel, WithDocModel, TripletModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/RoBERTa-base\")\n",
    "base_model = AutoModel.from_pretrained(\"FacebookAI/RoBERTa-base\")\n",
    "\n",
    "no_doc_model = NoDocModel.from_pretrained(base_model,\"trained/no_doc_rob\")\n",
    "with_doc_model = WithDocModel.from_pretrained(base_model,\"trained/with_doc_rob\")\n",
    "triplet_model = TripletModel.from_pretrained(base_model,triplet_loss, \"trained/triplet_rob\")\n",
    "\n",
    "def calc_sim_before(ref, text):\n",
    "    ref_tokens = tokenizer(ref, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    text_tokens = tokenizer(text, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        ref_output = base_model(**ref_tokens)\n",
    "        text_output = base_model(**text_tokens)\n",
    "\n",
    "    ref_embedding = ref_output.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    text_embedding = text_output.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    \n",
    "    return cosine_similarity(ref_embedding, text_embedding)[0][0]\n",
    "\n",
    "def calc_sim_no_doc(ref, text):\n",
    "    ref_tokens = tokenizer(ref, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    text_tokens = tokenizer(text, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        ref_output = no_doc_model(**ref_tokens)\n",
    "        text_output = no_doc_model(**text_tokens)\n",
    "\n",
    "    ref_embedding = ref_output.output.cpu().numpy()\n",
    "    text_embedding = text_output.output.cpu().numpy()\n",
    "    \n",
    "    return cosine_similarity(ref_embedding, text_embedding)[0][0]\n",
    "\n",
    "def calc_sim_with_doc(ref, text):\n",
    "    ref_tok = tokenizer(ref, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    text_tok = tokenizer(text, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    inputs = {\n",
    "    \"input_ids\":[ref_tok[\"input_ids\"],text_tok[\"input_ids\"]],\n",
    "    \"attention_mask\":[ref_tok[\"attention_mask\"],text_tok[\"attention_mask\"]]\n",
    "    }   \n",
    " \n",
    "    with torch.no_grad(): \n",
    "        output = with_doc_model(**inputs)\n",
    "\n",
    "    ref_output = output.output[0].cpu().numpy()\n",
    "    text_output = output.output[1].cpu().numpy()\n",
    "\n",
    "    return cosine_similarity(ref_output, text_output)[0][0]\n",
    "\n",
    "def calc_sim_triplet(ref, text, label):\n",
    "    ref_tok = tokenizer(ref, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    text_tok = tokenizer(text, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    sample_tok = tokenizer(\"sample\", padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    if label[0] == 1: # hallucination / negative\n",
    "        inputs = {\n",
    "        \"input_ids\":[ref_tok[\"input_ids\"],sample_tok[\"input_ids\"],text_tok[\"input_ids\"]],\n",
    "        \"attention_mask\":[ref_tok[\"attention_mask\"],sample_tok[\"attention_mask\"],text_tok[\"attention_mask\"]],\n",
    "        \"labels\": label\n",
    "        }   \n",
    "    elif label[0] == 0: # not hallucination / positive\n",
    "        inputs = {\n",
    "        \"input_ids\":[ref_tok[\"input_ids\"],text_tok[\"input_ids\"], sample_tok[\"input_ids\"]],\n",
    "        \"attention_mask\":[ref_tok[\"attention_mask\"],text_tok[\"attention_mask\"], sample_tok[\"attention_mask\"]],\n",
    "        \"labels\": label\n",
    "        }\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        output = triplet_model(**inputs)\n",
    "    ref_output = output.output[0].cpu().numpy()\n",
    "    text_output = output.output[1].cpu().numpy()\n",
    "\n",
    "    return cosine_similarity(ref_output, text_output)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for d in tqdm(test_data_span):\n",
    "    sim_before = calc_sim_before(d[\"ref\"], d[\"text\"])\n",
    "    sim_no_doc = calc_sim_no_doc(d[\"ref\"], d[\"text\"])\n",
    "    sim_with_doc = calc_sim_with_doc(d[\"ref\"], d[\"text\"])\n",
    "    sim_triplet = calc_sim_triplet(d[\"ref\"], d[\"text\"], [d[\"labels\"]])\n",
    "    \n",
    "    d[\"sim_before\"] = sim_before\n",
    "    d[\"sim_no_doc\"] = sim_no_doc\n",
    "    d[\"sim_doc\"] = sim_with_doc\n",
    "    d[\"sim_triplet\"] = sim_triplet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_span[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# write json\n",
    "with open(\"dataset/rag_truth_span_test_sim.json\", \"w\") as f:\n",
    "    json.dump(test_data_span, f, cls=NumpyEncoder, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/rag_truth_span_test_sim.json\", \"r\") as f:\n",
    "    test_data_span = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "before_posi = []\n",
    "before_nega = []\n",
    "\n",
    "id_all = id_000 + id_001 + id_010 + id_011 + id_100 + id_101 + id_110 + id_111\n",
    "id_tri_fail = id_001 + id_010 + id_101 + id_110\n",
    "id_tri_suc = id_000 + id_011 + id_100 + id_111\n",
    "\n",
    "# mode: sim_before, sim_no_doc, sim_doc, sim_triplet\n",
    "mode = \"triplet\" # if you use the file you made, change \"triplet\" to \"sim_triplet\"\n",
    "\n",
    "for i in id_all :\n",
    "    d = test_data_span[i]\n",
    "    \n",
    "    if d[\"labels\"] == 1:\n",
    "        before_nega.append(d[mode])\n",
    "    else:\n",
    "        before_posi.append(d[mode])\n",
    "        \n",
    "plt.hist(before_posi, bins=100, alpha=0.5, label=\"Positive\")\n",
    "plt.hist(before_nega, bins=100, alpha=0.5, label=\"Negative\")\n",
    "plt.xlabel(\"Cosine sim\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
