{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"rag_truth_train2.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"rag_truth_dev.json\", \"r\") as f:\n",
    "    dev_data = json.load(f)\n",
    "with open(\"rag_truth_test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix„Çí„Å§„Åë„Çã\n",
    "def add_prefix(data):\n",
    "    for d in data:\n",
    "        d[\"text\"] = \"Please judge the following statement as true or false based on the references above: \" + d[\"text\"]\n",
    "    return data\n",
    "\n",
    "train_data = add_prefix(train_data)\n",
    "dev_data = add_prefix(dev_data)\n",
    "test_data = add_prefix(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_type: QA, Data2txt, Summary\n",
    "task_name = \"Summary\"\n",
    "train_data = [d for d in train_data if d[\"task_type\"] == task_name]\n",
    "dev_data = [d for d in dev_data if d[\"task_type\"] == task_name]\n",
    "test_data = [d for d in test_data if d[\"task_type\"] == task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_trip(data, id_list):\n",
    "    trip = []\n",
    "    for id in id_list:\n",
    "        num = 0\n",
    "        no_hal = []\n",
    "        has_hal = []\n",
    "        for d in data:\n",
    "            if num == 6:\n",
    "                num = 0\n",
    "                if no_hal == [] or has_hal == []:\n",
    "                    break\n",
    "                '''\n",
    "                for no in no_hal:\n",
    "                    for has in has_hal:\n",
    "                        trip.append({\"anchor\":ref,\"positive\": no, \"negative\": has, \"labels\":[0,1]})\n",
    "                '''\n",
    "                # „Ç∑„É£„ÉÉ„Éï„É´\n",
    "                random.seed(id)\n",
    "                no_hal = random.sample(no_hal, len(no_hal))\n",
    "                has_hal = random.sample(has_hal, len(has_hal))\n",
    "                \n",
    "                if len(no_hal)==1 or len(no_hal)==5:\n",
    "                    trip.append({\"anchor\":ref,\"positive\": no_hal[0], \"negative\": has_hal[0], \"labels\":[0,1]})\n",
    "                elif len(no_hal)==2 or len(no_hal)==4:\n",
    "                    trip.append({\"anchor\":ref,\"positive\": no_hal[0], \"negative\": has_hal[0], \"labels\":[0,1]})\n",
    "                    trip.append({\"anchor\":ref,\"positive\": no_hal[1], \"negative\": has_hal[1], \"labels\":[0,1]})\n",
    "                elif len(no_hal)==3:\n",
    "                    trip.append({\"anchor\":ref,\"positive\": no_hal[0], \"negative\": has_hal[0], \"labels\":[0,1]})\n",
    "                    trip.append({\"anchor\":ref,\"positive\": no_hal[1], \"negative\": has_hal[1], \"labels\":[0,1]})\n",
    "                    trip.append({\"anchor\":ref,\"positive\": no_hal[2], \"negative\": has_hal[2], \"labels\":[0,1]})\n",
    "                no_hal = []\n",
    "                has_hal = []\n",
    "                break\n",
    "            elif d[\"source_id\"] == id:\n",
    "                num +=1\n",
    "                ref = d[\"ref\"]\n",
    "                if d[\"labels\"] == 0:\n",
    "                    no_hal.append(d[\"text\"])\n",
    "                else:\n",
    "                    has_hal.append(d[\"text\"])\n",
    "        if num == 6:\n",
    "            if len(no_hal)==1 or len(no_hal)==5:\n",
    "                trip.append({\"anchor\":ref,\"positive\": no_hal[0], \"negative\": has_hal[0], \"labels\":[0,1]})\n",
    "            elif len(no_hal)==2 or len(no_hal)==4:\n",
    "                trip.append({\"anchor\":ref,\"positive\": no_hal[0], \"negative\": has_hal[0], \"labels\":[0,1]})\n",
    "                trip.append({\"anchor\":ref,\"positive\": no_hal[1], \"negative\": has_hal[1], \"labels\":[0,1]})\n",
    "            elif len(no_hal)==3:\n",
    "                trip.append({\"anchor\":ref,\"positive\": no_hal[0], \"negative\": has_hal[0], \"labels\":[0,1]})\n",
    "                trip.append({\"anchor\":ref,\"positive\": no_hal[1], \"negative\": has_hal[1], \"labels\":[0,1]})\n",
    "                trip.append({\"anchor\":ref,\"positive\": no_hal[2], \"negative\": has_hal[2], \"labels\":[0,1]})\n",
    "    return trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2305 210 450\n"
     ]
    }
   ],
   "source": [
    "train_id = [d[\"source_id\"] for d in train_data]\n",
    "train_id = list(set(train_id))\n",
    "dev_id = [d[\"source_id\"] for d in dev_data]\n",
    "dev_id = list(set(dev_id))\n",
    "test_id = [d[\"source_id\"] for d in test_data]\n",
    "test_id = list(set(test_id))\n",
    "print(len(train_id), len(dev_id), len(test_id))\n",
    "train_trip = create_trip(train_data, train_id)\n",
    "dev_trip = create_trip(dev_data, dev_id)\n",
    "test_trip = create_trip(test_data, test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3760, 337, 633)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_trip), len(dev_trip), len(test_trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['anchor', 'positive', 'negative', 'labels'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['anchor', 'positive', 'negative', 'labels'],\n",
       "        num_rows: 337\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['anchor', 'positive', 'negative', 'labels'],\n",
       "        num_rows: 633\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(train_trip)\n",
    "dev_df = pd.DataFrame(dev_trip)\n",
    "test_df = pd.DataFrame(test_trip)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "dev_ds = Dataset.from_pandas(dev_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "tri_raw_datasets = DatasetDict({\"train\": train_ds, \"dev\": dev_ds, \"test\": test_ds})\n",
    "tri_raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "#tri_tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/RoBERTa-base\")\n",
    "tri_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "#tri_tokenizer.pad_token_id = tri_tokenizer.eos_token_id\n",
    "#tri_tokenizer.padding_side = \"left\"\n",
    "#tri_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\",use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchor': '{\\'name\\': \\'Santa Barbara Chicken Ranch\\', \\'address\\': \\'2618 De La Vina St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Health & Medical, Restaurants, Mexican, Cannabis Clinics, Barbeque\\', \\'hours\\': {\\'Monday\\': \\'11:0-22:0\\', \\'Tuesday\\': \\'11:0-22:0\\', \\'Wednesday\\': \\'11:0-22:0\\', \\'Thursday\\': \\'11:0-22:0\\', \\'Friday\\': \\'11:0-22:0\\', \\'Saturday\\': \\'11:0-22:0\\', \\'Sunday\\': \\'11:0-22:0\\'}, \\'attributes\\': {\\'BusinessParking\\': {\\'garage\\': False, \\'street\\': False, \\'validated\\': False, \\'lot\\': True, \\'valet\\': False}, \\'RestaurantsReservations\\': False, \\'OutdoorSeating\\': True, \\'WiFi\\': \\'no\\', \\'RestaurantsTakeOut\\': True, \\'RestaurantsGoodForGroups\\': True, \\'Music\\': None, \\'Ambience\\': {\\'romantic\\': False, \\'intimate\\': False, \\'touristy\\': False, \\'hipster\\': False, \\'divey\\': False, \\'classy\\': False, \\'trendy\\': False, \\'upscale\\': False, \\'casual\\': True}}, \\'business_stars\\': 4.0, \\'review_info\\': [{\\'review_stars\\': 5.0, \\'review_date\\': \\'2021-12-08 19:15:29\\', \\'review_text\\': \"If you\\'re not from town you might want to be sure to check this place out. Otherwise you likely are familiar already. Wood fire grill and homemade salsa and priced how I recall things 15 years ago. They deliver and open at 11 am sharp. You will probably wait a bit if you don\\'t call ahead.\"}, {\\'review_stars\\': 5.0, \\'review_date\\': \\'2021-11-23 11:06:15\\', \\'review_text\\': \\'This place easily gets five stars for delicious food, low cost, good portions, and quick service. \\\\xa0Our family had the chicken plate a tri-tip burrito, and two drinks. The kids can be a bit picky about chicken, but they gobbled this Mesquite BBQ. They both put the chicken on the included flour tortillas with some Spanish rice, pinto beans, and pico. The tri-tip burrito was great; well stacked and very flavorful. \\\\xa0After a couple days of vacationing in SB with expensive but mediocre food, this place was awesome. \\\\xa0Would definitely come again.\\'}, {\\'review_stars\\': 5.0, \\'review_date\\': \\'2021-10-15 00:27:53\\', \\'review_text\\': \\'Love this place! Just found this week and have gone twice. Their burritos are delish, shredded chicken is fresh and amazing! Well priced and well worth the trip!!!\\'}]}',\n",
       " 'positive': \"Please judge the following statement as true or false based on the references above: Santa Barbara Chicken Ranch is a local business located at 2618 De La Vina St in Santa Barbara, CA. The establishment offers a variety of services and cuisines, including Health & Medical, Restaurants, Mexican, Cannabis Clinics, and Barbeque. Operating hours are from 11:00 am to 10:00 pm every day of the week. The business has a parking lot available for customers. Reservations are not accepted, but outdoor seating is provided. The restaurant offers takeout services and is suitable for groups. Unfortunately, there is no WiFi available on the premises.\\n\\nCustomers have left positive reviews about Santa Barbara Chicken Ranch. One reviewer mentioned that the food is delicious, reasonably priced, and served in generous portions, with quick service. Another reviewer praised the fresh and amazing shredded chicken in the burritos, stating that the prices are affordable and the restaurant is worth visiting. A third reviewer highlighted the wood fire grill and homemade salsa, as well as the nostalgic pricing. They also mentioned that the restaurant delivers and opens promptly at 11 am, although there may be a wait if one doesn't call ahead.\\n\\nOverall, Santa Barbara Chicken Ranch has received an average rating of 4.0 stars from customers, who appreciate the tasty food, reasonable prices, and friendly service.\",\n",
       " 'negative': 'Please judge the following statement as true or false based on the references above: Santa Barbara Chicken Ranch is a restaurant located in the heart of Santa Barbara, California. The business offers a variety of cuisines, including health and medical services, Mexican food, barbecue, and cannabis clinics. The hours of operation are Monday through Sunday, from 11:00 am to 11:00 pm. The restaurant has outdoor seating and provides takeout options.\\n\\nThe restaurant has received positive reviews from its customers. One customer highly recommended the place, stating that it is a must-visit for anyone who is not from town. Another customer praised the quality of the food, which they described as delicious, low-cost, and served in generous portions. A third customer also gave high praise for the food, specifically mentioning the mesquite BBQ chicken and the tri-tip burrito. Overall, customers seem to appreciate the affordable prices and friendly service at Santa Barbara Chicken Ranch.',\n",
       " 'labels': [0, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 20:37:01.314695: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-22 20:37:01.772531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-22 20:37:02.011261: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-22 20:37:02.011701: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-22 20:37:02.400546: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-22 20:37:04.040556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d06234b4e6471586de88eeb1f47af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0939d20da83f47bd83b7f79cd4ecc7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49aef3814ee34f6bb7e1e5edde76a18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/633 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def tri_tokenize_function(examples):\n",
    "    anchor = tri_tokenizer(examples[\"anchor\"], truncation=True,max_length=512)\n",
    "    positive = tri_tokenizer(examples[\"positive\"], truncation=True,max_length=512)\n",
    "    negative = tri_tokenizer(examples[\"negative\"], truncation=True,max_length=512)\n",
    "\n",
    "    return {\n",
    "        \"anchor_input_ids\": anchor[\"input_ids\"],\n",
    "        \"anchor_attention_mask\": anchor[\"attention_mask\"],\n",
    "        \"positive_input_ids\": positive[\"input_ids\"],\n",
    "        \"positive_attention_mask\": positive[\"attention_mask\"],\n",
    "        \"negative_input_ids\": negative[\"input_ids\"],\n",
    "        \"negative_attention_mask\": negative[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "tri_tokenized_datasets = tri_raw_datasets.map(tri_tokenize_function, batched=True)\n",
    "tri_tokenized_datasets = tri_tokenized_datasets.remove_columns([\"anchor\", \"positive\", \"negative\"])\n",
    "#tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tri_tokenized_datasets.set_format(\"torch\")\n",
    "tri_data_collator = DataCollatorWithPadding(tokenizer=tri_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "class CustomDataCollator(DataCollatorWithPadding):\n",
    "    def __call__(self, features):\n",
    "        # features „ÅÆ‰æã: [{'anchor_input_ids': ..., 'anchor_attention_mask': ..., ...}, ...]\n",
    "        \n",
    "        # „Éá„Éº„Çø„ÅÆ„É™„Çπ„Éà„Çí‰ΩúÊàê\n",
    "        anchor_ids = [x['anchor_input_ids'].clone().detach() for x in features]\n",
    "        positive_ids = [x['positive_input_ids'].clone().detach() for x in features]\n",
    "        negative_ids = [x['negative_input_ids'].clone().detach() for x in features]\n",
    "        \n",
    "        anchor_mask = [x['anchor_attention_mask'].clone().detach() for x in features]\n",
    "        positive_mask = [x['positive_attention_mask'].clone().detach() for x in features]\n",
    "        negative_mask = [x['negative_attention_mask'].clone().detach() for x in features]\n",
    "        \n",
    "        # „Éë„Éá„Ç£„É≥„Ç∞Âá¶ÁêÜ\n",
    "        anchor_ids = pad_sequence(anchor_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        positive_ids = pad_sequence(positive_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        negative_ids = pad_sequence(negative_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        \n",
    "        anchor_mask = pad_sequence(anchor_mask, batch_first=True, padding_value=0)\n",
    "        positive_mask = pad_sequence(positive_mask, batch_first=True, padding_value=0)\n",
    "        negative_mask = pad_sequence(negative_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "        # „É©„Éô„É´„Çí‰ΩúÊàê\n",
    "        labels = [x['labels'] for x in features]\n",
    "        \n",
    "        \n",
    "        # „Éê„ÉÉ„ÉÅËæûÊõ∏„Çí‰ΩúÊàê\n",
    "        batch = {\n",
    "            \"input_ids\": [anchor_ids, positive_ids, negative_ids],\n",
    "            \"attention_mask\": [anchor_mask, positive_mask, negative_mask],\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "\n",
    "\n",
    "tri_data_collator = CustomDataCollator(tokenizer=tri_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4b8f9c24954ec6887b74af0b04e102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "#base_model = AutoModel.from_pretrained(\"FacebookAI/RoBERTa-base\")\n",
    "base_model = AutoModel.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "#base_model = AutoModel.from_pretrained(\"vinai/bertweet-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ‰Ωø„ÅÜË£ÖÁΩÆ\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "base_model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "import os\n",
    "\n",
    "\n",
    "class TripletModel(nn.Module):\n",
    "    def __init__(self, base_model, loss_function, question_encoder=None, generator=None):\n",
    "        super(TripletModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        #self.triplet_loss_fn = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(base_model.config.hidden_size * 2, 2)  # „É©„Éô„É´‰∫àÊ∏¨Áî®„ÅÆÂàÜÈ°ûÂ±§\n",
    "        # self.classification_loss_fn = nn.CrossEntropyLoss()  # „É©„Éô„É´‰∫àÊ∏¨„ÅÆÊêçÂ§±Èñ¢Êï∞\n",
    "        self.loss_function = loss_function\n",
    "        self.question_encoder = question_encoder\n",
    "        self.generator = generator\n",
    "        #self.alpha = alpha\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask = None,\n",
    "        labels = None,\n",
    "    ):\n",
    "        \n",
    "        anchor_input_ids = input_ids[0]\n",
    "        positive_input_ids = input_ids[1]\n",
    "        negative_input_ids = input_ids[2]\n",
    "        anchor_attention_mask = attention_mask[0]\n",
    "        positive_attention_mask = attention_mask[1]\n",
    "        negative_attention_mask = attention_mask[2]\n",
    "\n",
    "\n",
    "        anchor_output = self.base_model(input_ids=anchor_input_ids, attention_mask=anchor_attention_mask,return_dict=True)[0][:,-1,:]\n",
    "        positive_output = self.base_model(input_ids=positive_input_ids, attention_mask=positive_attention_mask,return_dict=True)[0][:,-1,:]\n",
    "        negative_output = self.base_model(input_ids=negative_input_ids, attention_mask=negative_attention_mask,return_dict=True)[0][:,-1,:]\n",
    "\n",
    "        #anchor_output = anchor_output.mean(dim=1)\n",
    "        #positive_output = positive_output.mean(dim=1)\n",
    "        #negative_output = negative_output.mean(dim=1)\n",
    "\n",
    "        anchor_output = self.dropout(anchor_output)\n",
    "        positive_output = self.dropout(positive_output)\n",
    "        negative_output = self.dropout(negative_output)\n",
    "\n",
    "        # „É©„Éô„É´‰∫àÊ∏¨„ÅÆÂá∫Âäõ\n",
    "        positive_logits = self.classifier(torch.cat([anchor_output, positive_output], dim=1))\n",
    "        negative_logits = self.classifier(torch.cat([anchor_output, negative_output], dim=1))\n",
    "\n",
    "        classification_loss, triplet_loss=self.loss_function(anchor_output, positive_output, negative_output, positive_logits, negative_logits)\n",
    "        loss = classification_loss + triplet_loss\n",
    "        \n",
    "        #return ModelOutput(logits=[positive_logits, negative_logits], loss=loss,classification_loss=classification_loss,triplet_loss=triplet_loss)\n",
    "        return ModelOutput(logits=[positive_logits, negative_logits],loss=loss)\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        \"\"\"\n",
    "        „É¢„Éá„É´ÂÖ®‰Ωì„Å®„Çµ„Éñ„É¢„Ç∏„É•„Éº„É´„Çí‰øùÂ≠ò„Åô„Çã„Ç´„Çπ„Çø„É†„É°„ÇΩ„ÉÉ„Éâ\n",
    "        \"\"\"\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        # „Çµ„Éñ„É¢„Ç∏„É•„Éº„É´„ÅÆ‰øùÂ≠ò\n",
    "        if self.question_encoder is not None:\n",
    "            self.question_encoder.save_pretrained(os.path.join(save_directory, \"question_encoder\"))\n",
    "        if self.generator is not None:\n",
    "            self.generator.save_pretrained(os.path.join(save_directory, \"generator\"))\n",
    "\n",
    "        # „É¢„Éá„É´ÂÖ®‰Ωì„ÅÆÈáç„Åø„Çí‰øùÂ≠ò\n",
    "        torch.save(self.state_dict(), os.path.join(save_directory, \"pytorch_model.bin\"))\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory):\n",
    "        \"\"\"\n",
    "        ‰øùÂ≠òÊ∏à„Åø„É¢„Éá„É´„Çí„É≠„Éº„Éâ„Åô„Çã„Ç´„Çπ„Çø„É†„É°„ÇΩ„ÉÉ„Éâ\n",
    "        \"\"\"\n",
    "        question_encoder = None\n",
    "        generator = None\n",
    "\n",
    "        # „Çµ„Éñ„É¢„Ç∏„É•„Éº„É´„ÅÆ„É≠„Éº„Éâ\n",
    "        if os.path.exists(os.path.join(save_directory, \"question_encoder\")):\n",
    "            question_encoder = AutoModel.from_pretrained(os.path.join(save_directory, \"question_encoder\"))\n",
    "        if os.path.exists(os.path.join(save_directory, \"generator\")):\n",
    "            generator = AutoModel.from_pretrained(os.path.join(save_directory, \"generator\"))\n",
    "\n",
    "        # „É¢„Éá„É´ÂÖ®‰Ωì„ÅÆÈáç„Åø„Çí„É≠„Éº„Éâ\n",
    "        model = cls(question_encoder=question_encoder, generator=generator)\n",
    "        model.load_state_dict(torch.load(os.path.join(save_directory, \"pytorch_model.bin\")))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "clas_list = []\n",
    "trip_list = []\n",
    "def triplet_loss(anchor_output, positive_output, negative_output, positive_logits, negative_logits):\n",
    "    # „É©„Éô„É´‰∫àÊ∏¨„ÅÆÊêçÂ§±Ë®àÁÆó\n",
    "    positive_targets = torch.zeros(positive_output.size(0), dtype=torch.long).to(device)  # „É©„Éô„É´1 (positive)\n",
    "    negative_targets = torch.ones(negative_output.size(0), dtype=torch.long).to(device)\n",
    "    positive_loss = nn.CrossEntropyLoss()(positive_logits, positive_targets)\n",
    "    negative_loss = nn.CrossEntropyLoss()(negative_logits, negative_targets)\n",
    "\n",
    "    # „É©„Éô„É´‰∫àÊ∏¨„ÅÆÊêçÂ§±„ÇíÂπ≥Âùá\n",
    "    classification_loss = (positive_loss + negative_loss) / 2.0\n",
    "\n",
    "    # „Éà„É™„Éó„É¨„ÉÉ„ÉàÊêçÂ§±„ÅÆË®àÁÆó\n",
    "    #triplet_loss = nn.TripletMarginLoss(margin=1, p=2)(anchor_output, positive_output, negative_output)\n",
    "    triplet_loss_fn = (nn.TripletMarginWithDistanceLoss(margin=1.5,distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)))\n",
    "    triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "    # ÊúÄÁµÇÊêçÂ§±\n",
    "    total_loss = classification_loss + triplet_loss \n",
    "    #return total_loss\n",
    "    trip_list.append(triplet_loss)\n",
    "    clas_list.append(classification_loss)\n",
    "    return classification_loss, triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# negative„Ååhallu„Åßlabel„Åå1\n",
    "def compute_tri_metrics(eval_pred):\n",
    "    #print(\"aa\")\n",
    "    # „É¢„Éá„É´„ÅÆÂá∫Âäõ„Åã„ÇâÊêçÂ§±„Å® logits „ÇíÂèñÂæó\n",
    "    logits,labels= eval_pred\n",
    "    \n",
    "    # „É≠„Ç∏„ÉÉ„Éà„ÅÆ„Çµ„Ç§„Ç∫„Å´Âêà„Çè„Åõ„Å¶Êé®Ë´ñ„ÇíË°å„ÅÜ\n",
    "    positive_logits = torch.tensor(logits[0])\n",
    "    negative_logits = torch.tensor(logits[1])\n",
    "    #positive_logits = torch.tensor(logits[0][0])\n",
    "    #negative_logits = torch.tensor(logits[0][1])\n",
    "\n",
    "\n",
    "\n",
    "    # „É©„Éô„É´‰∫àÊ∏¨\n",
    "    positive_preds = torch.argmax(positive_logits, dim=1)\n",
    "    negative_preds = torch.argmax(negative_logits, dim=1)\n",
    "\n",
    "    # Ê≠£Ëß£„Å®‰∫àÊ∏¨„É©„Éô„É´„ÅÆ‰∏ÄËá¥Êï∞\n",
    "    correct_positive = (positive_preds == 0).sum().item()\n",
    "    correct_negative = (negative_preds == 1).sum().item()\n",
    "    \n",
    "    # „É©„Éô„É´„ÅÆÊï∞\n",
    "    total_samples = positive_preds.size(0) + negative_preds.size(0)\n",
    "    \n",
    "    # Positive„Å®Negative„ÅÆ‰∫àÊ∏¨Êï∞\n",
    "    positive_preds_num = (positive_preds == 1).sum().item()\n",
    "    negative_preds_num = (negative_preds == 1).sum().item()\n",
    "    positive_num = positive_preds.size(0)\n",
    "    negative_num = negative_preds.size(0)\n",
    "\n",
    "    #print(negative_preds_num,positive_preds_num,negative_num)\n",
    "\n",
    "    # Precision, Recall, F1„ÅÆË®àÁÆó\n",
    "    precision = negative_preds_num / (positive_preds_num + negative_preds_num) if (positive_preds_num + negative_preds_num) > 0 else 0\n",
    "    recall = negative_preds_num / negative_num if negative_num > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Âπ≥ÂùáÊêçÂ§±„Å®Ê≠£Ëß£Áéá„ÅÆË®àÁÆó\n",
    "    #avg_loss = loss.item() if loss is not None else 0\n",
    "    accuracy = (correct_positive + correct_negative) / total_samples\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/22090.1.interactive/ipykernel_4194085/3761690756.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "from models import TripletModel\n",
    "\n",
    "\"\"\"\n",
    "base_model = AutoModel.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "# „É¢„Éá„É´„Å®„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÇíË™≠„ÅøËæº„ÇÄ\n",
    "name = \"./1224_triplet\"\n",
    "tri_model = TripletModel.from_pretrained(base_model, triplet_loss, name)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "# ‰Ωø„ÅÜË£ÖÁΩÆ\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "tri_model.to(device)\n",
    "\"\"\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",  \n",
    "    save_steps=10000,\n",
    "    learning_rate=1e-6,# 1e-6\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    fp16 = True,\n",
    "    gradient_accumulation_steps=12,\n",
    "    logging_dir=\"./logs\",\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    optim=\"adafactor\",\n",
    ")\n",
    "\n",
    "tri_model = TripletModel(base_model, triplet_loss)\n",
    "#triplet_loss_logger = TripletLossLogger()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=tri_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tri_tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tri_tokenized_datasets[\"dev\"],\n",
    "    data_collator=tri_data_collator,\n",
    "    tokenizer=tri_tokenizer,\n",
    "    compute_metrics=compute_tri_metrics,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/gs/fs/tga-arase-student/yamada/triplet-phi.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ood.t4.gsic.titech.ac.jp/gs/fs/tga-arase-student/yamada/triplet-phi.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mevaluate()\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/transformers/trainer.py:4076\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4073\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   4075\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4076\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   4077\u001b[0m     eval_dataloader,\n\u001b[1;32m   4078\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   4079\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4080\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4081\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   4082\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   4083\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   4084\u001b[0m )\n\u001b[1;32m   4086\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   4087\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/transformers/trainer.py:4270\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4267\u001b[0m         batch_size \u001b[39m=\u001b[39m observed_batch_size\n\u001b[1;32m   4269\u001b[0m \u001b[39m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 4270\u001b[0m losses, logits, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[39m=\u001b[39;49mignore_keys)\n\u001b[1;32m   4271\u001b[0m main_input_name \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39m\"\u001b[39m\u001b[39mmain_input_name\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   4272\u001b[0m inputs_decode \u001b[39m=\u001b[39m (\n\u001b[1;32m   4273\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m args\u001b[39m.\u001b[39minclude_for_metrics \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   4274\u001b[0m )\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/transformers/trainer.py:4486\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4484\u001b[0m \u001b[39mif\u001b[39;00m has_labels \u001b[39mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   4485\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4486\u001b[0m         loss, outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs, return_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   4487\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m   4489\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/transformers/trainer.py:3734\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3732\u001b[0m         loss_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_items_in_batch\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3733\u001b[0m     inputs \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3734\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   3735\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3736\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3737\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[39mreturn\u001b[39;00m model_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_to_fp32(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/models.py:173\u001b[0m, in \u001b[0;36mTripletModel.forward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m    169\u001b[0m positive_attention_mask \u001b[39m=\u001b[39m attention_mask[\u001b[39m1\u001b[39m]\n\u001b[1;32m    170\u001b[0m negative_attention_mask \u001b[39m=\u001b[39m attention_mask[\u001b[39m2\u001b[39m]\n\u001b[0;32m--> 173\u001b[0m anchor_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(input_ids\u001b[39m=\u001b[39;49manchor_input_ids, attention_mask\u001b[39m=\u001b[39;49manchor_attention_mask,return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m][:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:]\n\u001b[1;32m    174\u001b[0m positive_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model(input_ids\u001b[39m=\u001b[39mpositive_input_ids, attention_mask\u001b[39m=\u001b[39mpositive_attention_mask,return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m][:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:]\n\u001b[1;32m    175\u001b[0m negative_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model(input_ids\u001b[39m=\u001b[39mnegative_input_ids, attention_mask\u001b[39m=\u001b[39mnegative_attention_mask,return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m][:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:]\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/transformers/models/phi3/modeling_phi3.py:605\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds\n\u001b[1;32m    604\u001b[0m \u001b[39m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    607\u001b[0m \u001b[39m# decoder layers\u001b[39;00m\n\u001b[1;32m    608\u001b[0m all_hidden_states \u001b[39m=\u001b[39m () \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/gs/fs/tga-arase-student/yamada/nlp/lib64/python3.9/site-packages/transformers/models/phi3/modeling_phi3.py:368\u001b[0m, in \u001b[0;36mPhi3RotaryEmbedding.forward\u001b[0;34m(self, x, position_ids)\u001b[0m\n\u001b[1;32m    366\u001b[0m device_type \u001b[39m=\u001b[39m device_type \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device_type, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m device_type \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautocast(device_type\u001b[39m=\u001b[39mdevice_type, enabled\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 368\u001b[0m     freqs \u001b[39m=\u001b[39m (inv_freq_expanded\u001b[39m.\u001b[39;49mfloat() \u001b[39m@\u001b[39;49m position_ids_expanded\u001b[39m.\u001b[39;49mfloat())\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    369\u001b[0m     emb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((freqs, freqs), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    370\u001b[0m     cos \u001b[39m=\u001b[39m emb\u001b[39m.\u001b[39mcos()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_bmm)"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/780 16:03 < 1:02:59, 0.16 it/s, Epoch 2.03/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.988000</td>\n",
       "      <td>1.565921</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.692878</td>\n",
       "      <td>0.676558</td>\n",
       "      <td>0.699387</td>\n",
       "      <td>0.687783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.488800</td>\n",
       "      <td>1.380101</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.725519</td>\n",
       "      <td>0.816024</td>\n",
       "      <td>0.690955</td>\n",
       "      <td>0.748299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=tri_tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dev_task(name):\n",
    "    dev_data2 = [d for d in test_data if d[\"task_type\"] == name]\n",
    "    dev_id2 = [d[\"source_id\"] for d in dev_data2]\n",
    "    dev_id2 = list(set(dev_id2))\n",
    "    dev_trip2 = create_trip(dev_data2, dev_id2)\n",
    "    dev_df2 = pd.DataFrame(dev_trip2)\n",
    "    dev_ds2 = Dataset.from_pandas(dev_df2)\n",
    "    tri_tokenized_datasets_task = dev_ds2.map(tri_tokenize_function, batched=True)\n",
    "    tri_tokenized_datasets_task = tri_tokenized_datasets_task.remove_columns([\"anchor\", \"positive\", \"negative\"])\n",
    "    tri_tokenized_datasets_task.set_format(\"torch\")\n",
    "    return tri_tokenized_datasets_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883c376394664bc491502950313e8487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_qa = create_dev_task(\"QA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5213183760643005,\n",
       " 'eval_model_preparation_time': 0.0039,\n",
       " 'eval_accuracy': 0.8966666666666666,\n",
       " 'eval_recall': 0.92,\n",
       " 'eval_precision': 0.8789808917197452,\n",
       " 'eval_f1': 0.8990228013029316,\n",
       " 'eval_runtime': 9.4958,\n",
       " 'eval_samples_per_second': 15.796,\n",
       " 'eval_steps_per_second': 4.002}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=dev_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9811b97fd341318fbab03e3eaf2a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/291 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_d2t = create_dev_task(\"Data2txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9073538780212402,\n",
       " 'eval_model_preparation_time': 0.0039,\n",
       " 'eval_accuracy': 0.7852233676975945,\n",
       " 'eval_recall': 0.7491408934707904,\n",
       " 'eval_precision': 0.8074074074074075,\n",
       " 'eval_f1': 0.7771836007130126,\n",
       " 'eval_runtime': 19.65,\n",
       " 'eval_samples_per_second': 14.809,\n",
       " 'eval_steps_per_second': 3.715}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=dev_d2t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1443d5c65b456d98f25c4a2d4e311d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/192 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_sum = create_dev_task(\"Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1023415327072144,\n",
       " 'eval_model_preparation_time': 0.0039,\n",
       " 'eval_accuracy': 0.71875,\n",
       " 'eval_recall': 0.65625,\n",
       " 'eval_precision': 0.75,\n",
       " 'eval_f1': 0.7,\n",
       " 'eval_runtime': 12.0214,\n",
       " 'eval_samples_per_second': 15.971,\n",
       " 'eval_steps_per_second': 3.993}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=dev_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Éà„É¨„Éº„Éã„É≥„Ç∞Âæå„Å´„É¢„Éá„É´„Çí‰øùÂ≠ò\n",
    "# „Éà„É¨„Éº„Éã„É≥„Ç∞Âæå„Å´„É¢„Éá„É´„Çí‰øùÂ≠ò\n",
    "name = \"./1228_triplet\"\n",
    "trainer.save_model(name)\n",
    "trainer.save_state()\n",
    "tri_model.save_pretrained(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 26 00:10:58 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100                    On  | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             151W / 699W |  15370MiB / 95830MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA H100                    On  | 00000000:E4:00.0 Off |                    0 |\n",
      "| N/A   26C    P0             152W / 699W |  15422MiB / 95830MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   3486749      C   /home/3/uw03923/nlp/bin/python3           15356MiB |\n",
      "|    1   N/A  N/A   3486749      C   /home/3/uw03923/nlp/bin/python3           15408MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache() \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 22:07:52,130] A new study created in memory with name: no-name-3c0841e6-4883-4e79-a10b-1410de3e8fc1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fd30edfadf444e8d622b92ce9092ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/2105545.1.all.q/ipykernel_661295/3443566117.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [780/780 1:16:21, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.937700</td>\n",
       "      <td>0.666758</td>\n",
       "      <td>0.732036</td>\n",
       "      <td>0.688623</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.719875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.651100</td>\n",
       "      <td>0.606497</td>\n",
       "      <td>0.772455</td>\n",
       "      <td>0.745509</td>\n",
       "      <td>0.787975</td>\n",
       "      <td>0.766154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.558576</td>\n",
       "      <td>0.778443</td>\n",
       "      <td>0.730539</td>\n",
       "      <td>0.807947</td>\n",
       "      <td>0.767296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.521100</td>\n",
       "      <td>0.537769</td>\n",
       "      <td>0.799401</td>\n",
       "      <td>0.784431</td>\n",
       "      <td>0.808642</td>\n",
       "      <td>0.796353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.476900</td>\n",
       "      <td>0.541523</td>\n",
       "      <td>0.799401</td>\n",
       "      <td>0.835329</td>\n",
       "      <td>0.779330</td>\n",
       "      <td>0.806358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.427400</td>\n",
       "      <td>0.532387</td>\n",
       "      <td>0.797904</td>\n",
       "      <td>0.754491</td>\n",
       "      <td>0.826230</td>\n",
       "      <td>0.788732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.394300</td>\n",
       "      <td>0.531311</td>\n",
       "      <td>0.800898</td>\n",
       "      <td>0.763473</td>\n",
       "      <td>0.825243</td>\n",
       "      <td>0.793157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.356700</td>\n",
       "      <td>0.534783</td>\n",
       "      <td>0.794910</td>\n",
       "      <td>0.805389</td>\n",
       "      <td>0.788856</td>\n",
       "      <td>0.797037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.330600</td>\n",
       "      <td>0.537783</td>\n",
       "      <td>0.799401</td>\n",
       "      <td>0.802395</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.304900</td>\n",
       "      <td>0.543543</td>\n",
       "      <td>0.787425</td>\n",
       "      <td>0.778443</td>\n",
       "      <td>0.792683</td>\n",
       "      <td>0.785498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-21 23:25:06,386] Trial 0 finished with value: 0.785498489425982 and parameters: {'alpha': 0.3}. Best is trial 0 with value: 0.785498489425982.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38aefcb484b3429c9255a4056a10d00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/2105545.1.all.q/ipykernel_661295/3443566117.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [780/780 1:17:02, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.969700</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.694611</td>\n",
       "      <td>0.595808</td>\n",
       "      <td>0.742537</td>\n",
       "      <td>0.661130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.682100</td>\n",
       "      <td>0.656266</td>\n",
       "      <td>0.758982</td>\n",
       "      <td>0.676647</td>\n",
       "      <td>0.810036</td>\n",
       "      <td>0.737357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.613800</td>\n",
       "      <td>0.571792</td>\n",
       "      <td>0.763473</td>\n",
       "      <td>0.751497</td>\n",
       "      <td>0.769939</td>\n",
       "      <td>0.760606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.554700</td>\n",
       "      <td>0.551639</td>\n",
       "      <td>0.779940</td>\n",
       "      <td>0.724551</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.767036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.511200</td>\n",
       "      <td>0.558430</td>\n",
       "      <td>0.790419</td>\n",
       "      <td>0.730539</td>\n",
       "      <td>0.829932</td>\n",
       "      <td>0.777070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.456200</td>\n",
       "      <td>0.559521</td>\n",
       "      <td>0.785928</td>\n",
       "      <td>0.712575</td>\n",
       "      <td>0.835088</td>\n",
       "      <td>0.768982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.421900</td>\n",
       "      <td>0.555264</td>\n",
       "      <td>0.782934</td>\n",
       "      <td>0.775449</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.781297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.375700</td>\n",
       "      <td>0.543227</td>\n",
       "      <td>0.790419</td>\n",
       "      <td>0.772455</td>\n",
       "      <td>0.801242</td>\n",
       "      <td>0.786585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.347300</td>\n",
       "      <td>0.542052</td>\n",
       "      <td>0.797904</td>\n",
       "      <td>0.760479</td>\n",
       "      <td>0.822006</td>\n",
       "      <td>0.790047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>0.543016</td>\n",
       "      <td>0.797904</td>\n",
       "      <td>0.772455</td>\n",
       "      <td>0.813880</td>\n",
       "      <td>0.792627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-22 00:42:59,919] Trial 1 finished with value: 0.7926267281105991 and parameters: {'alpha': 0.3}. Best is trial 1 with value: 0.7926267281105991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.3\n"
     ]
    }
   ],
   "source": [
    "# alphaÊé¢Á¥¢\n",
    "import optuna\n",
    "from transformers import AutoModel\n",
    "import transformers\n",
    "import torch\n",
    "import peft\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# ÁõÆÁöÑÈñ¢Êï∞„ÇíÂÆöÁæ©\n",
    "def objective(trial):\n",
    "    torch.cuda.empty_cache() \n",
    "    # alpha „Çí 0.1 Âàª„Åø„ÅßÊé¢Á¥¢\n",
    "    alpha = trial.suggest_categorical('alpha', [0.3])\n",
    "    \n",
    "    # „É¢„Éá„É´„ÅÆÊêçÂ§±Èñ¢Êï∞„Å´ alpha „ÇíË®≠ÂÆö\n",
    "    base_model = AutoModel.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "    base_model.to(device)\n",
    "\n",
    "    tri_model = TripletModel(base_model,triplet_loss, alpha=alpha)\n",
    "\n",
    "    # „É¢„Éá„É´„ÅÆÂ≠¶Áøí\n",
    "    trainer = Trainer(\n",
    "        model=tri_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tri_tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tri_tokenized_datasets[\"dev\"],\n",
    "        data_collator=tri_data_collator,\n",
    "        tokenizer=tri_tokenizer,\n",
    "        compute_metrics=compute_tri_metrics,\n",
    "    \n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # Ë©ï‰æ° („Åì„Åì„Åß„ÅØ eval_accuracy „ÇíÊúÄÂ§ßÂåñ„Åô„Çã‰æã)\n",
    "    metrics = trainer.evaluate()\n",
    "    # Ëøî„Åô„Çπ„Ç≥„Ç¢„ÇíÂ§âÊõ¥ÂèØËÉΩ (‰æã: F1 „Çπ„Ç≥„Ç¢, Á≤æÂ∫¶„Å™„Å©)\n",
    "    return metrics['eval_f1']\n",
    "\n",
    "# Optuna „ÅÆ„Çπ„Çø„Éá„Ç£„Çí‰ΩúÊàê\n",
    "study = optuna.create_study(direction='maximize')  # Á≤æÂ∫¶„ÇíÊúÄÂ§ßÂåñ„Åô„ÇãË®≠ÂÆö\n",
    "study.optimize(objective, n_trials=2)  # Ë©¶Ë°åÂõûÊï∞ (0.1 Âàª„Åø„Å™„Çâ11Ë©¶Ë°å)\n",
    "\n",
    "# ÊúÄÈÅ©„Å™ alpha „ÅÆÁ¢∫Ë™ç\n",
    "best_alpha = study.best_params['alpha']\n",
    "print(f\"Best alpha: {best_alpha}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5019dd1aee824d81b06b8bdfac3b912d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qkv_proj', 'gate_up_proj', 'o_proj', 'down_proj']\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "base_model = AutoModel.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\",\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=transformers.BitsAndBytesConfig(  # ÈáèÂ≠êÂåñ„Åô„ÇãÊåáÂÆö\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "    )\n",
    "\n",
    "import re\n",
    "# „Å©„Åì„Å´LoRA„ÅÆ„Éë„É©„É°„Éº„Çø„ÇíËøΩÂä†„Åß„Åç„Çã„Åã\n",
    "linear_layers = list(set(re.findall(\n",
    "    r'\\((\\w+)\\): Linear', str(base_model.modules)\n",
    ")))\n",
    "print(linear_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Åô„Åπ„Å¶„ÅÆË©¶Ë°åÁµêÊûú„ÇíÂèñÂæó\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alphas = [trial.params['alpha'] for trial in study.trials]\n",
    "scores = [trial.value for trial in study.trials]\n",
    "\n",
    "plt.plot(alphas, scores, marker='o')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Evaluation Score (e.g., Accuracy)')\n",
    "plt.title('Alpha vs Evaluation Score')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Ëâ≤„ÄÖÊ§úË®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ‰∫àÊ∏¨ÁµêÊûú„ÇíDataFrame„Å´Â§âÊèõ\n",
    "predictions_df = pd.DataFrame(predictions.predictions[1])\n",
    "\n",
    "predictions_df['label_ids'] = 1\n",
    "\n",
    "# CSV„Éï„Ç°„Ç§„É´„Å®„Åó„Å¶‰øùÂ≠ò\n",
    "predictions_df.to_csv(\"predictions_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„ÇÄ\n",
    "df1 = pd.read_csv(\"predictions.csv\")\n",
    "df2 = pd.read_csv(\"predictions_1.csv\")\n",
    "\n",
    "# Á∏¶„Å´ÁµêÂêà\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# ÁµêÂêà„Åó„Åü„Éá„Éº„Çø„ÇíCSV„Å®„Åó„Å¶‰øùÂ≠ò\n",
    "combined_df.to_csv(\"predictions_2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_list = [x.item() for x in trip_list]\n",
    "clas_list = [x.item() for x in clas_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0,len(clas_list),len(clas_list)//1000)\n",
    "clas_list = np.array(clas_list)\n",
    "trip_list = np.array(trip_list)\n",
    "plt.plot(x,clas_list[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,trip_list[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,trip_list[x]+clas_list[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2509\n",
      "1255\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions.predictions[0]))  # Âá∫Âäõ„ÅÆÂΩ¢Áä∂„ÇíÁ¢∫Ë™ç\n",
    "print(len(predictions.label_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6853\n",
      "Precision: 0.6286\n",
      "Recall: 0.9059\n",
      "F1 Score: 0.7422\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# CSV„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„ÇÄ\n",
    "df = pd.read_csv(\"predictions_2.csv\")\n",
    "\n",
    "# „ÇØ„É©„Çπ„É©„Éô„É´„Çí‰∫àÊ∏¨\n",
    "df['predicted_label'] = np.where(df[\"0\"] > df[\"1\"], 0, 1)\n",
    "\n",
    "# Ê≠£Ëß£„É©„Éô„É´„Å®‰∫àÊ∏¨„É©„Éô„É´„ÇíÂèñÂæó\n",
    "true_labels = df['label_ids']  # Ê≠£Ëß£„É©„Éô„É´„ÅÆÂàóÂêç„ÇíÊåáÂÆö\n",
    "predicted_labels = df['predicted_label']\n",
    "\n",
    "# ÂêÑË©ï‰æ°ÊåáÊ®ô„ÇíË®àÁÆó\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# ÁµêÊûú„ÇíË°®Á§∫\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
